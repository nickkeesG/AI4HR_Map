{
  "nodes": [
    {
      "id": "Mapping the Discourse on AI Safety & Ethics",
      "title": "Mapping the Discourse on AI Safety & Ethics",
      "tags": [
        "project"
      ],
      "content": "#project \n\nhttps://ai.objectives.institute/blog/mapping-the-discourse-on-ai-safety-amp-ethics\n\nProject by the AI Objectives Institute that used their \"Talk to the City\" tool to analyze and map Twitter conversations about AI safety and ethics, identifying six distinct perspectives ranging from near-term harms to long-term existential risks. The tool automatically clustered thousands of tweets to reveal different viewpoints on AI governance, safety approaches, and development concerns, finding significant overlap between groups despite their different priorities.",
      "x": 788.547473702122,
      "y": -380.61239477665157,
      "size": 18.285714285714285
    },
    {
      "id": "Lei Nelissen",
      "title": "Lei Nelissen",
      "tags": [
        "person"
      ],
      "content": "#person",
      "x": 3692.4366545678186,
      "y": 1142.4553761640127,
      "size": 16.571428571428573
    },
    {
      "id": "ACE - A LLM-based Negotiation Coaching System",
      "title": "ACE - A LLM-based Negotiation Coaching System",
      "tags": [
        "project"
      ],
      "content": "#project \n\nhttps://arxiv.org/pdf/2410.01555\n\nLLM-based system that provides targeted feedback to help users improve their bargaining skills. The system identifies specific negotiation mistakes using an annotation scheme developed with [MBA](https://en.wikipedia.org/wiki/Master_of_Business_Administration) instructors, then provides detailed feedback on preparation strategies (like setting appropriate target prices) and in-conversation tactics (like including rationales with offers and strategic closing behavior). Through a controlled experiment with 374 participants across two negotiation rounds, the researchers demonstrated that users who received ACE's coaching significantly improved their objective negotiation outcomes compared to control groups.",
      "x": 4041.6678647452236,
      "y": 253.4329420771978,
      "size": 18.857142857142858
    },
    {
      "id": "Zhehui Liao",
      "title": "Zhehui Liao",
      "tags": [
        "person"
      ],
      "content": "#person",
      "x": 1088.5466929376678,
      "y": 1764.3582779852723,
      "size": 16.571428571428573
    },
    {
      "id": "Jonathan Stray",
      "title": "Jonathan Stray",
      "tags": [
        "person"
      ],
      "content": "#person \n\nhttp://jonathanstray.com/",
      "x": 1557.4840034772917,
      "y": 985.1290569276915,
      "size": 17.142857142857142
    },
    {
      "id": "David Rand",
      "title": "David Rand",
      "tags": [
        "person"
      ],
      "content": "#person \n",
      "x": 552.86779890692,
      "y": 654.2670978276634,
      "size": 16.571428571428573
    },
    {
      "id": "Remesh",
      "title": "Remesh",
      "tags": [
        "organization"
      ],
      "content": "#organization \n\nhttps://www.remesh.ai",
      "x": 1434.9197807893906,
      "y": 780.4664062408266,
      "size": 17.142857142857142
    },
    {
      "id": "Amelia Glaese",
      "title": "Amelia Glaese",
      "tags": [
        "person"
      ],
      "content": "#person",
      "x": 1255.5505198378098,
      "y": 499.37128163947017,
      "size": 16.571428571428573
    },
    {
      "id": "Amy Zhang",
      "title": "Amy Zhang",
      "tags": [
        "person"
      ],
      "content": "#person \n\nhttps://amyzhang.github.io/",
      "x": 1296.67931470172,
      "y": 1424.3976391045085,
      "size": 17.714285714285715
    },
    {
      "id": "Amina Green",
      "title": "Amina Green",
      "tags": [
        "person"
      ],
      "content": "#person \n\nhttps://www.amina-io.com/",
      "x": 1708.848852745666,
      "y": 1220.2859801318612,
      "size": 17.142857142857142
    },
    {
      "id": "Kyle Lo",
      "title": "Kyle Lo",
      "tags": [
        "person"
      ],
      "content": "#person",
      "x": 1167.2078193114032,
      "y": 1849.0516260907625,
      "size": 16.571428571428573
    },
    {
      "id": "Shahar Hechtlinger",
      "title": "Shahar Hechtlinger",
      "tags": [
        "person"
      ],
      "content": "#person",
      "x": 999.7260100957333,
      "y": 1076.9243071209094,
      "size": 16.571428571428573
    },
    {
      "id": "Zoe Rahwan",
      "title": "Zoe Rahwan",
      "tags": [
        "person"
      ],
      "content": "#person",
      "x": 1111.1206944884832,
      "y": 1164.6364880829267,
      "size": 16.571428571428573
    },
    {
      "id": "Maggie Appleton",
      "title": "Maggie Appleton",
      "tags": [
        "person"
      ],
      "content": "#person \n\nhttps://maggieappleton.com/",
      "x": -1986.0378873394982,
      "y": 345.1972131135029,
      "size": 17.714285714285715
    },
    {
      "id": "Andy Zou",
      "title": "Andy Zou",
      "tags": [
        "person"
      ],
      "content": "#person",
      "x": -906.6659596965399,
      "y": 430.7431010076412,
      "size": 16.571428571428573
    },
    {
      "id": "Wargames for Peace",
      "title": "Wargames for Peace",
      "tags": [
        "project"
      ],
      "content": "#project \n\nhttps://www.youtube.com/watch?v=9_zSzjTWr8A\n\n[MATS](https://www.matsprogram.org/) project to simulate  a tabletop exercises with LLMs, allowing a human to play solo.",
      "x": -2216.0206491589165,
      "y": 317.47789193596424,
      "size": 18.285714285714285
    },
    {
      "id": "Evelien Nieuwenburg",
      "title": "Evelien Nieuwenburg",
      "tags": [
        "person"
      ],
      "content": "#person",
      "x": 3697.499433125433,
      "y": 1087.6426664248283,
      "size": 17.142857142857142
    },
    {
      "id": "Enhancing AI-Assisted Group Decision Making through LLM-Powered Devil’s Advocate",
      "title": "Enhancing AI-Assisted Group Decision Making through LLM-Powered Devil’s Advocate",
      "tags": [
        "project"
      ],
      "content": "#project \n\nhttps://dl.acm.org/doi/pdf/10.1145/3640543.3645199\n\nStudy tested \"LLM-Powered Devil's Advocate\" systems that use GPT-3.5 to challenge group decisions in AI-assisted tasks. Researchers ran a randomized experiment where groups made recidivism risk predictions with AI help, and some groups also had an LLM devil's advocate that either questioned the AI's recommendations or challenged the group's majority opinion. The devil's advocate used critique questions and comments to prompt deeper thinking. The key finding was that questioning AI recommendations significantly improved group accuracy, while questioning group opinions provided no benefit.",
      "x": 1644.2006929316392,
      "y": 3713.969999573129,
      "size": 18.285714285714285
    },
    {
      "id": "Zaria Jalan",
      "title": "Zaria Jalan",
      "tags": [
        "person"
      ],
      "content": "#person",
      "x": 1386.190552838559,
      "y": 986.1306566948462,
      "size": 16.571428571428573
    },
    {
      "id": "Squiggle AI",
      "title": "Squiggle AI",
      "tags": [
        "project"
      ],
      "content": "#project\n\nhttps://www.lesswrong.com/posts/7worWgggeHL3Eb7wq/introducing-squiggle-ai\n\nTool that automatically generates probabilistic cost-effectiveness models using the Squiggle programming language, allowing users to quickly create Fermi estimates and decision analyses without needing to learn programming. It combines large language models with specialized scaffolding to produce structured, adjustable models for questions like comparing charitable interventions or evaluating policy decisions.",
      "x": -658.0983208874798,
      "y": -987.2815180985507,
      "size": 17.714285714285715
    },
    {
      "id": "Kenny Peng",
      "title": "Kenny Peng",
      "tags": [
        "person"
      ],
      "content": "#person",
      "x": -1100.7574622203372,
      "y": 3337.194553598093,
      "size": 16.571428571428573
    },
    {
      "id": "Yoshua Bengio",
      "title": "Yoshua Bengio",
      "tags": [
        "person"
      ],
      "content": "#person \n\nhttps://yoshuabengio.org",
      "x": 2505.262233261202,
      "y": 150.56630904175333,
      "size": 16.571428571428573
    },
    {
      "id": "Colleen McKenzie",
      "title": "Colleen McKenzie",
      "tags": [
        "person"
      ],
      "content": "#person",
      "x": 615.6457683620001,
      "y": -374.6339202786093,
      "size": 17.714285714285715
    },
    {
      "id": "Reimagining Democracy for AI",
      "title": "Reimagining Democracy for AI",
      "tags": [
        "project"
      ],
      "content": "#project \n\nhttps://aviv.me/Reimagining-Democracy-for-AI-Journal-of-Democracy.pdf\n\nPaper proposes reinventing \"democratic infrastructure\" through four key innovations: \n1. representative deliberations (citizen assemblies selected by lottery), \n2. AI-augmentation of democratic processes, \n3. democracy-as-a-service organizations, and \n4. platform democracy for governing tech companies.",
      "x": 1741.3530545382905,
      "y": 861.8425828439722,
      "size": 16.571428571428573
    },
    {
      "id": "Tom Davidson",
      "title": "Tom Davidson",
      "tags": [
        "person"
      ],
      "content": "#person \n\nhttps://tomdavidson-ai.github.io/",
      "x": -1788.7773735372552,
      "y": 204.77188300552578,
      "size": 17.142857142857142
    },
    {
      "id": "Joseph Chee Chang",
      "title": "Joseph Chee Chang",
      "tags": [
        "person"
      ],
      "content": "#person",
      "x": 1113.2389426501634,
      "y": 1863.0531295677174,
      "size": 16.571428571428573
    },
    {
      "id": "AI‐assisted scenario generation for strategic planning",
      "title": "AI‐assisted scenario generation for strategic planning",
      "tags": [
        "project"
      ],
      "content": "#project \n\nhttps://onlinelibrary.wiley.com/doi/pdf/10.1002/ffo2.148\n\nThis paper explores how LLMs can be used to generate scenarios for strategic planning (for organizations/businesses), examining whether AI-produced scenarios meet professional standards and how they can best assist human decision-makers. The authors conclude that AI-assisted scenario development shows significant promise as a tool for generating \"raw material\" that human facilitators can refine.",
      "x": 3097.8329820032714,
      "y": -1201.0364840274362,
      "size": 17.142857142857142
    },
    {
      "id": "Nina Lutz",
      "title": "Nina Lutz",
      "tags": [
        "person"
      ],
      "content": "#person \n",
      "x": 593.9891638038955,
      "y": 626.3486394185837,
      "size": 16.571428571428573
    },
    {
      "id": "Truthful AI",
      "title": "Truthful AI",
      "tags": [
        "project"
      ],
      "content": "#project \n\nhttps://arxiv.org/pdf/2110.06674\n\nPaper proposes developing AI systems that avoid stating falsehoods, particularly \"negligent falsehoods\" - statements that AI systems should have been able to recognize as likely false given available information. The authors argue for establishing societal standards and institutions to evaluate and certify AI truthfulness, moving beyond current approaches that rely on human-like accountability mechanisms which don't translate well to AI systems. They outline technical approaches for building more truthful AI systems and explore the governance frameworks needed to implement truthfulness standards at scale, including certification bodies and adjudication processes for evaluating AI statements.",
      "x": -1628.9682601355698,
      "y": 499.78644622592236,
      "size": 20.57142857142857
    },
    {
      "id": "Alek Chakroff",
      "title": "Alek Chakroff",
      "tags": [
        "person"
      ],
      "content": "#person",
      "x": 703.5530019082382,
      "y": -617.439855261721,
      "size": 16.571428571428573
    },
    {
      "id": "Ariel Procaccia",
      "title": "Ariel Procaccia",
      "tags": [
        "person"
      ],
      "content": "#person \n\nhttps://procaccia.info/",
      "x": 1767.4207500457344,
      "y": 495.40167289038965,
      "size": 17.142857142857142
    },
    {
      "id": "Ryan Shea",
      "title": "Ryan Shea",
      "tags": [
        "person"
      ],
      "content": "#person",
      "x": 3988.715238567769,
      "y": 207.99283635935524,
      "size": 16.571428571428573
    },
    {
      "id": "Hannah Sheahan",
      "title": "Hannah Sheahan",
      "tags": [
        "person"
      ],
      "content": "#person \n\nhttps://hannahsheahan.github.io/",
      "x": 1355.6345270716827,
      "y": 495.9672877817744,
      "size": 17.142857142857142
    },
    {
      "id": "Kevin Leyton-Brown",
      "title": "Kevin Leyton-Brown",
      "tags": [
        "person"
      ],
      "content": "#person \n",
      "x": 609.4688133586419,
      "y": 733.6508194289695,
      "size": 16.571428571428573
    },
    {
      "id": "AI and the Future of Digital Public Squares",
      "title": "AI and the Future of Digital Public Squares",
      "tags": [
        "project"
      ],
      "content": "#project \n\nhttps://arxiv.org/pdf/2412.09988\n\nPaper examines how LLMs can improve online discussions and democratic participation. \n\n## Four Main Areas of Focus\n\n**1. Collective Dialogue Systems**\n- Tools that help large groups have structured conversations and find common ground on important issues.\n\n**2. Bridging Systems**\n- Algorithms that promote content bringing people together rather than showing divisive posts that drive engagement.\n\n**3. Community-Driven Moderation**\n- AI tools to help volunteer moderators manage online communities more effectively while keeping humans in control.\n\n**4. Proof of Humanity Systems**\n- Methods to verify that real humans (not bots) are participating in online discussions as AI makes fake accounts easier to create.",
      "x": 1423.1375195527432,
      "y": 867.5196560102631,
      "size": 31.42857142857143
    },
    {
      "id": "Daniel P. Hogan",
      "title": "Daniel P. Hogan",
      "tags": [
        "person"
      ],
      "content": "#person",
      "x": -1507.8716164354419,
      "y": -2027.8242143512912,
      "size": 17.142857142857142
    },
    {
      "id": "Recursive Public",
      "title": "Recursive Public",
      "tags": [
        "project"
      ],
      "content": "#project \n\nhttps://vtaiwan-openai-2023.vercel.app/Report_%20Recursive%20Public.pdf\n\nPilot democratic deliberation system designed to gather public input on AI governance decisions through cascading online discussions and AI-powered analysis. The project used platforms like [Polis](https://pol.is) and [Talk to the City](https://ai.objectives.institute/talk-to-the-city) to engage over 1,000 participants across different communities in identifying AI governance priorities, then employed LLMs to analyze consensus and disagreement patterns in the discussions.\n\nFunded by [OpenAI Democratic Inputs to AI](https://github.com/openai/democratic-inputs).",
      "x": 835.0730612401152,
      "y": -42.327038893972045,
      "size": 21.714285714285715
    },
    {
      "id": "Rowan Wilkinson",
      "title": "Rowan Wilkinson",
      "tags": [
        "person"
      ],
      "content": "#person",
      "x": 880.6122452149882,
      "y": -105.83090148661746,
      "size": 16.571428571428573
    },
    {
      "id": "Snow Globe",
      "title": "Snow Globe",
      "tags": [
        "project"
      ],
      "content": "#project \n\nhttps://github.com/IQTLabs/snowglobe\nhttps://arxiv.org/pdf/2404.11446\n\nSystem that uses LLMs to automate qualitative wargames - open-ended strategic simulations where participants respond with natural language rather than choosing from predefined moves. The system can simulate crisis scenarios like AI incidents or geopolitical conflicts, allowing multiple AI agents to play different roles while a control agent moderates and adjudicates outcomes.",
      "x": -1567.358358192519,
      "y": -1969.309216161687,
      "size": 17.714285714285715
    },
    {
      "id": "Owain Evans",
      "title": "Owain Evans",
      "tags": [
        "person"
      ],
      "content": "#person \n\nhttps://owainevans.github.io/",
      "x": -1372.1944531749268,
      "y": 477.9869800887734,
      "size": 17.714285714285715
    },
    {
      "id": "Forecasting",
      "title": "Forecasting",
      "tags": [
        "topic"
      ],
      "content": "#topic",
      "x": -747.4431928673606,
      "y": -323.2770515513226,
      "size": 26.285714285714285
    },
    {
      "id": "Martin Weiss",
      "title": "Martin Weiss",
      "tags": [
        "person"
      ],
      "content": "#person",
      "x": 2554.358120375232,
      "y": 122.40646192028377,
      "size": 16.571428571428573
    },
    {
      "id": "Li Erran Li",
      "title": "Li Erran Li",
      "tags": [
        "person"
      ],
      "content": "#person",
      "x": 2455.7047326598263,
      "y": 33.593761978029704,
      "size": 16.571428571428573
    },
    {
      "id": "Alice Marwick",
      "title": "Alice Marwick",
      "tags": [
        "person"
      ],
      "content": "#person \n\nhttps://tiara.org/",
      "x": 1499.3963013694793,
      "y": 980.9523479974121,
      "size": 16.571428571428573
    },
    {
      "id": "Oliver Klingefjord",
      "title": "Oliver Klingefjord",
      "tags": [
        "person"
      ],
      "content": "#person \n\nhttps://www.klingefjord.com/",
      "x": 1178.5910932786057,
      "y": -246.8472025448904,
      "size": 18.285714285714285
    },
    {
      "id": "Nicholas Rowland",
      "title": "Nicholas Rowland",
      "tags": [
        "person"
      ],
      "content": "#person",
      "x": 3126.6299360752982,
      "y": -1145.0685049761623,
      "size": 16.571428571428573
    },
    {
      "id": "Scaffolding cooperation in human groups with deep reinforcement learning",
      "title": "Scaffolding cooperation in human groups with deep reinforcement learning",
      "tags": [
        "project"
      ],
      "content": "#project \n\nhttps://www.nature.com/articles/s41562-023-01686-7.pdf\n\nResearchers used deep reinforcement learning to train an AI \"social planner\" that makes recommendations about which connections to form or break in human social networks playing a cooperation game for real money. The AI learned a counterintuitive \"encouraging\" strategy: instead of isolating defectors from cooperators (the traditional approach), it placed defectors in small neighborhoods surrounded by cooperators, which led groups to achieve 77.7% cooperation rates compared to just 42.8% in static networks.",
      "x": 1318.1137871394092,
      "y": 721.7663063479347,
      "size": 20
    },
    {
      "id": "Language Agents as Digital Representatives in Collective Decision-Making",
      "title": "Language Agents as Digital Representatives in Collective Decision-Making",
      "tags": [
        "project"
      ],
      "content": "#project \n\nhttps://arxiv.org/pdf/2502.09369\n\nResearchers fine-tuned LLMs to simulate how specific people would participate in consensus-finding discussions, particularly in the critique phase where participants evaluate draft consensus statements.",
      "x": 1173.1295844688327,
      "y": 662.1306612244009,
      "size": 21.142857142857142
    },
    {
      "id": "Dirk U. Wulff",
      "title": "Dirk U. Wulff",
      "tags": [
        "person"
      ],
      "content": "#person",
      "x": 1108.926778892928,
      "y": 1111.2480613974628,
      "size": 16.571428571428573
    },
    {
      "id": "Luke Thorburn",
      "title": "Luke Thorburn",
      "tags": [
        "person"
      ],
      "content": "#person",
      "x": 1650.4746137334355,
      "y": 709.4227495487538,
      "size": 17.142857142857142
    },
    {
      "id": "David Schwitzgebel",
      "title": "David Schwitzgebel",
      "tags": [
        "person"
      ],
      "content": "#person",
      "x": -266.6761943198043,
      "y": 2454.8521255812566,
      "size": 16.571428571428573
    },
    {
      "id": "Jay Van Bavel",
      "title": "Jay Van Bavel",
      "tags": [
        "person"
      ],
      "content": "#person \n",
      "x": 746.3419841580535,
      "y": 765.2003632492579,
      "size": 16.571428571428573
    },
    {
      "id": "Foresight AI",
      "title": "Foresight AI",
      "tags": [
        "project"
      ],
      "content": "#project \n\nhttps://www.lightningrod.ai/foresight\n\nAI tool (with a free version!) which allows you to query the probability of some future event. The system will provide a probability, as well as a reasoning trace explaining how that probability was generated. The tool also gives some feedback about the quality of your question (e.g. clear resolution criteria), as well as offering alternate questions you might want to ask.",
      "x": -513.2013549868367,
      "y": -680.0646260910952,
      "size": 17.714285714285715
    },
    {
      "id": "Sander van der Linden",
      "title": "Sander van der Linden",
      "tags": [
        "person"
      ],
      "content": "#person \n",
      "x": 741.6974951717235,
      "y": 613.1207583737784,
      "size": 16.571428571428573
    },
    {
      "id": "Double Crux Bot",
      "title": "Double Crux Bot",
      "tags": [
        "project"
      ],
      "content": "#project \n\nhttps://www.lesswrong.com/posts/ENgwdgdHZ2a6HTWDL/announcing-the-double-crux-bot\n\nGPT-powered chatbot for Slack and Discord that facilitates \"double crux\" conversations - a conflict resolution technique that helps two people identify the core disagreement underlying their dispute by finding shared cruxes (key beliefs that, if changed, would shift both parties' positions). The bot acts as a mediator to help people \"make their reasoning explicit and reflect on the crux of the issue\" and \"build better inferences about each other's motivations and frameworks so that they can come to a resolution.\"",
      "x": 3589.89856296859,
      "y": -1156.7002435643292,
      "size": 17.714285714285715
    },
    {
      "id": "Emily Saltz",
      "title": "Emily Saltz",
      "tags": [
        "person"
      ],
      "content": "#person \n\nhttps://www.emilysaltz.space/",
      "x": 1675.7682442794192,
      "y": 962.4398438049817,
      "size": 17.142857142857142
    },
    {
      "id": "Julien Cornebise",
      "title": "Julien Cornebise",
      "tags": [
        "person"
      ],
      "content": "#person",
      "x": 2133.7538366244335,
      "y": 741.7209256089054,
      "size": 17.142857142857142
    },
    {
      "id": "Gili Rusak",
      "title": "Gili Rusak",
      "tags": [
        "person"
      ],
      "content": "#person",
      "x": 2078.0982759157587,
      "y": 314.993988535065,
      "size": 16.571428571428573
    },
    {
      "id": "Daniel Jarrett",
      "title": "Daniel Jarrett",
      "tags": [
        "person"
      ],
      "content": "#person",
      "x": 1210.2578542148913,
      "y": 589.8451188793808,
      "size": 17.142857142857142
    },
    {
      "id": "Danny Fanklin",
      "title": "Danny Fanklin",
      "tags": [
        "person"
      ],
      "content": "#person",
      "x": -431.6774386700457,
      "y": -494.60304253552584,
      "size": 17.714285714285715
    },
    {
      "id": "Language Models as Critical Thinking Tools - A Case Study of Philosophers",
      "title": "Language Models as Critical Thinking Tools - A Case Study of Philosophers",
      "tags": [
        "project"
      ],
      "content": "#project \n\nhttps://arxiv.org/pdf/2404.04516\n \nStudy of 21 professional philosophers examining how language models could serve as \"critical thinking tools\" rather than just productivity accelerators. The researchers found that current LMs fail as critical thinking partners because they lack \"selfhood\" (consistent perspectives, memory, beliefs) and \"initiative\" (curiosity, proactivity, willingness to challenge users), leading philosophers to describe them as \"boring,\" \"bland,\" and \"cowardly.\" The authors propose three new LM roles to better support deep reasoning:\n1. the Interlocutor (high selfhood/initiative, challenges and disagrees), \n2. the Monitor (low selfhood/high initiative, provides diverse perspectives), and \n3. the Respondent (high selfhood/low initiative, reacts from specific viewpoints)",
      "x": 1322.0568355756795,
      "y": 1603.2768802970643,
      "size": 18.857142857142858
    },
    {
      "id": "Talk to the City",
      "title": "Talk to the City",
      "tags": [
        "project"
      ],
      "content": "#project \n\nhttps://ai.objectives.institute/talk-to-the-city\n\nOpen-source AI system that analyzes large-scale public input from surveys, interviews, and meetings using LLMs to identify themes and cluster opinions while maintaining the specificity of individual responses. The system processes both structured and unstructured data to generate interactive reports that allow decision-makers to explore opinion distributions at multiple scales, from broad thematic patterns down to individual participant perspectives",
      "x": 648.7804714199932,
      "y": -306.67853489950744,
      "size": 16.571428571428573
    },
    {
      "id": "Arvind Narayanan",
      "title": "Arvind Narayanan",
      "tags": [
        "person"
      ],
      "content": "#person",
      "x": 1141.6446111346118,
      "y": 1024.7335170157417,
      "size": 16.571428571428573
    },
    {
      "id": "My Current Claims and Cruxes on LLM Forecasting & Epistemics",
      "title": "My Current Claims and Cruxes on LLM Forecasting & Epistemics",
      "tags": [
        "project"
      ],
      "content": "#project \n\nhttps://forum.effectivealtruism.org/posts/EykCuXDCFAT5oGyux/my-current-claims-and-cruxes-on-llm-forecasting-and\n\nPost outlines a vision for LLM-based Epistemic Processes (LEPs) that could automate virtually all aspects of knowledge work, from gathering and synthesizing information to generating forecasts and presenting results to users. The author breaks down LEPs into 13 distinct components including data collection, world modeling, human elicitation, numeric modeling, decomposition/amplification techniques, and various forms of automated decision support. A central argument is that these systems will require substantial \"scaffolding\" (supporting software infrastructure) and will likely be dominated by centralized platforms rather than distributed individual forecasters, potentially replacing human participation in prediction markets like Manifold and Metaculus. The post also addresses potential risks including AI acceleration, misuse by malicious actors, and the challenge that these powerful epistemic tools might increase world complexity faster than they improve our ability to navigate it.",
      "x": -846.3985542892191,
      "y": -1145.8208936945189,
      "size": 17.142857142857142
    },
    {
      "id": "Ezequiel Lopez-Lopez",
      "title": "Ezequiel Lopez-Lopez",
      "tags": [
        "person"
      ],
      "content": "#person",
      "x": 1154.027562329465,
      "y": 1134.100690252959,
      "size": 16.571428571428573
    },
    {
      "id": "Niki Dupuis",
      "title": "Niki Dupuis",
      "tags": [
        "person"
      ],
      "content": "#person",
      "x": 3688.7093262635094,
      "y": -985.8406135311669,
      "size": 17.714285714285715
    },
    {
      "id": "Michael W. Morris",
      "title": "Michael W. Morris",
      "tags": [
        "person"
      ],
      "content": "#person",
      "x": 4062.862764169195,
      "y": 318.719315877871,
      "size": 16.571428571428573
    },
    {
      "id": "AI Enhanced Reasoning - Augmenting Human Critical Thinking With AI Systems",
      "title": "AI Enhanced Reasoning - Augmenting Human Critical Thinking With AI Systems",
      "tags": [
        "project"
      ],
      "content": "#project \n\nhttps://www.proquest.com/openview/3393dad9ea55a5d47f1a0b19e607f964/1.pdf\n\nMasters thesis, uses AI to help humans reason better through 3 experiments:\n- AI logic-checkers that spot argument flaws in real-time\n- Studies on how deceptive AI explanations mess with human thinking\n- AI that improves reasoning by asking smart questions",
      "x": 1171.564562563712,
      "y": -2768.51555121768,
      "size": 16.571428571428573
    },
    {
      "id": "Manifold Markets",
      "title": "Manifold Markets",
      "tags": [
        "organization"
      ],
      "content": "#organization \n\nhttps://manifold.markets/home",
      "x": -853.4949587966678,
      "y": -880.135286309336,
      "size": 16.571428571428573
    },
    {
      "id": "Sentinel",
      "title": "Sentinel",
      "tags": [
        "organization"
      ],
      "content": "#organization \n\nhttps://sentinel-team.org",
      "x": -1070.884753433492,
      "y": -1530.2437019367424,
      "size": 17.142857142857142
    },
    {
      "id": "Sarah Bluhm",
      "title": "Sarah Bluhm",
      "tags": [
        "person"
      ],
      "content": "#person",
      "x": 3589.6795614195557,
      "y": -1228.0681934337858,
      "size": 16.571428571428573
    },
    {
      "id": "Collective Intelligence Project",
      "title": "Collective Intelligence Project",
      "tags": [
        "organization"
      ],
      "content": "#organization \n\nhttps://www.cip.org/",
      "x": 1196.948459287815,
      "y": 832.5435137077104,
      "size": 18.285714285714285
    },
    {
      "id": "Modelling Political Coalition Negotiations Using LLM-based Agents",
      "title": "Modelling Political Coalition Negotiations Using LLM-based Agents",
      "tags": [
        "project"
      ],
      "content": "#project \n\nhttps://arxiv.org/pdf/2402.11712\n\nThis research paper introduces a system for modeling political coalition negotiations using LLM-based agents. The authors create AI agents that represent different political parties and simulate the complex negotiation process to predict which policy statements from party manifestos will be included in final coalition agreements.",
      "x": -2168.1174286900696,
      "y": 2346.6799117617625,
      "size": 18.285714285714285
    },
    {
      "id": "Darren Fancher",
      "title": "Darren Fancher",
      "tags": [
        "person"
      ],
      "content": "#person",
      "x": -136.97161845378483,
      "y": 3776.950868675314,
      "size": 16.571428571428573
    },
    {
      "id": "Jason W. Burton",
      "title": "Jason W. Burton",
      "tags": [
        "person"
      ],
      "content": "#person",
      "x": 1054.4055521605299,
      "y": 1090.8123716623745,
      "size": 16.571428571428573
    },
    {
      "id": "Approaching Human-Level Forecasting with Language Models",
      "title": "Approaching Human-Level Forecasting with Language Models",
      "tags": [
        "project"
      ],
      "content": "#project \n\nhttps://arxiv.org/pdf/2402.18563\n\nLLM system forecasting on competitive forecasting platforms by combining news retrieval, structured reasoning, and fine-tuning techniques to predict binary outcomes. The system automates the traditional forecasting process through three key components: \n1. retrieving relevant information from news sources,\n2. generating reasoned predictions, and \n3. aggregating multiple forecasts into a final prediction.",
      "x": -511.9886221542186,
      "y": 220.05320773975464,
      "size": 18.857142857142858
    },
    {
      "id": "Alice Siu",
      "title": "Alice Siu",
      "tags": [
        "person"
      ],
      "content": "#person \n\nhttps://www.alicesiu.com/",
      "x": 985.0211574111167,
      "y": 1172.7456815191272,
      "size": 17.142857142857142
    },
    {
      "id": "Kris Skotheim",
      "title": "Kris Skotheim",
      "tags": [
        "person"
      ],
      "content": "#person",
      "x": -411.3547046728863,
      "y": -545.4937432550389,
      "size": 17.142857142857142
    },
    {
      "id": "Ziv Epstein",
      "title": "Ziv Epstein",
      "tags": [
        "person"
      ],
      "content": "#person",
      "x": 1207.1550182508104,
      "y": -2638.0944084472067,
      "size": 16.571428571428573
    },
    {
      "id": "Felix Sieker",
      "title": "Felix Sieker",
      "tags": [
        "person"
      ],
      "content": "#person",
      "x": 1522.6148975381761,
      "y": 930.6143108641814,
      "size": 16.571428571428573
    },
    {
      "id": "Audrey Tang",
      "title": "Audrey Tang",
      "tags": [
        "person"
      ],
      "content": "#person \n\nhttps://github.com/audreyt",
      "x": 1086.0447800074567,
      "y": 719.5887259305994,
      "size": 18.857142857142858
    },
    {
      "id": "Bridging Bot",
      "title": "Bridging Bot",
      "tags": [
        "project"
      ],
      "content": "#project \n\nhttps://www.youtube.com/watch?v=QgfXdJ-7pF4\n\nLLM-based bot that de-escalates arguments on Reddit. Funded by Google Jigsaw.",
      "x": 1785.7024767083278,
      "y": 1200.0487188351551,
      "size": 18.857142857142858
    },
    {
      "id": "Philosophy & Morality",
      "title": "Philosophy & Morality",
      "tags": [
        "topic"
      ],
      "content": "#topic",
      "x": 556.133995757879,
      "y": 1824.8772754010947,
      "size": 19.428571428571427
    },
    {
      "id": "Wasim Almasri",
      "title": "Wasim Almasri",
      "tags": [
        "person"
      ],
      "content": "#person",
      "x": 1602.9627753407951,
      "y": 618.6943993536536,
      "size": 16.571428571428573
    },
    {
      "id": "Jia-Wei Cui",
      "title": "Jia-Wei Cui",
      "tags": [
        "person"
      ],
      "content": "#person",
      "x": 884.4669013720609,
      "y": 83.069990629577,
      "size": 17.142857142857142
    },
    {
      "id": "LLMs as Research Tools -  A Large Scale Survey of Researchers’ Usage and Perceptions",
      "title": "LLMs as Research Tools -  A Large Scale Survey of Researchers’ Usage and Perceptions",
      "tags": [
        "project"
      ],
      "content": "#project \n\nhttps://arxiv.org/pdf/2411.05025\n\nLarge-scale survey study examining how researchers currently use LLMs in their research workflows and their perceptions of benefits and risks. The study surveyed 816 verified research paper authors across multiple disciplines to understand usage patterns, demographic differences, and attitudes toward LLM tools in academic work.",
      "x": 1156.1294161915903,
      "y": 1698.8968115333146,
      "size": 21.142857142857142
    },
    {
      "id": "Andrea Tacchetti",
      "title": "Andrea Tacchetti",
      "tags": [
        "person"
      ],
      "content": "#person \n\nhttps://www.andreatacchetti.com/",
      "x": 1260.3557935086312,
      "y": 682.8647806521557,
      "size": 17.142857142857142
    },
    {
      "id": "Tristan Xiao",
      "title": "Tristan Xiao",
      "tags": [
        "person"
      ],
      "content": "#person",
      "x": -814.0999428682537,
      "y": 533.6793262651005,
      "size": 16.571428571428573
    },
    {
      "id": "Pepijn Verburg",
      "title": "Pepijn Verburg",
      "tags": [
        "person"
      ],
      "content": "#person",
      "x": 3750.832351524982,
      "y": 1187.1960449906455,
      "size": 16.571428571428573
    },
    {
      "id": "Jacob Steinhardt",
      "title": "Jacob Steinhardt",
      "tags": [
        "person"
      ],
      "content": "#person \n\nhttps://jsteinhardt.stat.berkeley.edu/",
      "x": -689.2056804129033,
      "y": 371.8596738179657,
      "size": 17.142857142857142
    },
    {
      "id": "Wenkang Ji",
      "title": "Wenkang Ji",
      "tags": [
        "person"
      ],
      "content": "#person",
      "x": -1680.132786153885,
      "y": 2769.8636134537037,
      "size": 16.571428571428573
    },
    {
      "id": "Susan Leavy",
      "title": "Susan Leavy",
      "tags": [
        "person"
      ],
      "content": "#person",
      "x": 958.946015279396,
      "y": 1047.1735494609654,
      "size": 16.571428571428573
    },
    {
      "id": "Meeyoung Cha",
      "title": "Meeyoung Cha",
      "tags": [
        "person"
      ],
      "content": "#person \n",
      "x": 640.5524594959512,
      "y": 595.0887027404054,
      "size": 16.571428571428573
    },
    {
      "id": "Kehang Zhu",
      "title": "Kehang Zhu",
      "tags": [
        "person"
      ],
      "content": "#person \n",
      "x": 1849.9174012120527,
      "y": -53.883251834481996,
      "size": 16.571428571428573
    },
    {
      "id": "Future Search",
      "title": "Future Search",
      "tags": [
        "organization"
      ],
      "content": "#organization \n\nhttps://futuresearch.ai/",
      "x": -717.8526553292105,
      "y": -719.7276361443852,
      "size": 22.285714285714285
    },
    {
      "id": "Nick Bostrom",
      "title": "Nick Bostrom",
      "tags": [
        "person"
      ],
      "content": "#person \n\nhttps://nickbostrom.com/\n",
      "x": 692.6717770301715,
      "y": 630.92732644266,
      "size": 16.571428571428573
    },
    {
      "id": "Sven Seuken",
      "title": "Sven Seuken",
      "tags": [
        "person"
      ],
      "content": "#person \n",
      "x": 1903.8297731579873,
      "y": -38.7864793208898,
      "size": 16.571428571428573
    },
    {
      "id": "Richard Li",
      "title": "Richard Li",
      "tags": [
        "person"
      ],
      "content": "#person",
      "x": -940.7466985429488,
      "y": 474.9853314430529,
      "size": 16.571428571428573
    },
    {
      "id": "Ben Wilson",
      "title": "Ben Wilson",
      "tags": [
        "person"
      ],
      "content": "#person",
      "x": -246.84657412896797,
      "y": -802.6439658065143,
      "size": 18.285714285714285
    },
    {
      "id": "Lightning Rod Labs",
      "title": "Lightning Rod Labs",
      "tags": [
        "organization"
      ],
      "content": "#organization \n\nhttps://www.lightningrod.ai/",
      "x": -485.4756382927939,
      "y": -568.3602203235023,
      "size": 18.285714285714285
    },
    {
      "id": "Human v Bots Forecasting Tournament 2024",
      "title": "Human v Bots Forecasting Tournament 2024",
      "tags": [
        "project"
      ],
      "content": "#project\n\nhttps://news.manifold.markets/p/human-v-bots-forecasting-tournament\n\nCompetition hosted by Manifold Markets, where human forecasters compete directly against AI systems to predict major world events throughout 2024, with winners determined by forecasting accuracy and profit.",
      "x": -783.1858281412245,
      "y": -810.106102551167,
      "size": 17.714285714285715
    },
    {
      "id": "Democratic Policy Development using Collective Dialogues and AI",
      "title": "Democratic Policy Development using Collective Dialogues and AI",
      "tags": [
        "project"
      ],
      "content": "#project \n\nhttps://arxiv.org/pdf/2311.02242\n\nPaper presents a democratic process for developing AI policies through collective dialogues, using AI tools to facilitate large-scale deliberation and consensus-finding among diverse public participants. The system combines AI-augmented group discussions with bridging-based ranking algorithms to identify points of consensus, then uses GPT-4 to translate these into concrete policy guidelines that are refined through expert input and further public feedback.\n\nFunded by [OpenAI Democratic Inputs to AI](https://github.com/openai/democratic-inputs).",
      "x": 1665.396310946405,
      "y": 867.003953915849,
      "size": 18.285714285714285
    },
    {
      "id": "David Huang",
      "title": "David Huang",
      "tags": [
        "person"
      ],
      "content": "#person \n",
      "x": 1914.5906312194227,
      "y": 182.3270762034121,
      "size": 16.571428571428573
    },
    {
      "id": "Dan Hendrycks",
      "title": "Dan Hendrycks",
      "tags": [
        "person"
      ],
      "content": "#person \n\nhttps://danhendrycks.com/",
      "x": -848.2753514987046,
      "y": 577.917902273538,
      "size": 16.571428571428573
    },
    {
      "id": "Curtis Holdsworth",
      "title": "Curtis Holdsworth",
      "tags": [
        "person"
      ],
      "content": "#person",
      "x": -75.87709658829615,
      "y": 3865.3037411350742,
      "size": 16.571428571428573
    },
    {
      "id": "Siddarth Srinivasan",
      "title": "Siddarth Srinivasan",
      "tags": [
        "person"
      ],
      "content": "#person \n",
      "x": 612.7560851620352,
      "y": 390.90148399012725,
      "size": 16.571428571428573
    },
    {
      "id": "Julian Michael",
      "title": "Julian Michael",
      "tags": [
        "person"
      ],
      "content": "#person \n\nhttps://julianmichael.org/",
      "x": 4325.204187162485,
      "y": 1039.2522055365914,
      "size": 16
    },
    {
      "id": "Değer Turan",
      "title": "Değer Turan",
      "tags": [
        "person"
      ],
      "content": "#person",
      "x": 371.54679486309595,
      "y": -417.8764113264942,
      "size": 18.285714285714285
    },
    {
      "id": "Owen Cotton-Barratt",
      "title": "Owen Cotton-Barratt",
      "tags": [
        "person"
      ],
      "content": "#person \n\nhttps://strangecities.substack.com/",
      "x": -1767.049221344344,
      "y": 489.6781367043218,
      "size": 17.714285714285715
    },
    {
      "id": "Jared Moore",
      "title": "Jared Moore",
      "tags": [
        "person"
      ],
      "content": "#person \n\nhttps://jaredmoore.org/",
      "x": 1387.231520480465,
      "y": 1705.9864367070088,
      "size": 16.571428571428573
    },
    {
      "id": "Ulrike Hahn",
      "title": "Ulrike Hahn",
      "tags": [
        "person"
      ],
      "content": "#person",
      "x": 1046.0842952759554,
      "y": 1051.9234963407023,
      "size": 16.571428571428573
    },
    {
      "id": "Zhuoyan Li",
      "title": "Zhuoyan Li",
      "tags": [
        "person"
      ],
      "content": "#person",
      "x": 1708.5747319508507,
      "y": 3732.0690944936523,
      "size": 16.571428571428573
    },
    {
      "id": "Aymen Kallala",
      "title": "Aymen Kallala",
      "tags": [
        "person"
      ],
      "content": "#person",
      "x": 4109.578091822254,
      "y": 260.9788036619757,
      "size": 16.571428571428573
    },
    {
      "id": "AI-Augmented Predictions - LLM Assistants Improve Human Forecasting Accuracy",
      "title": "AI-Augmented Predictions - LLM Assistants Improve Human Forecasting Accuracy",
      "tags": [
        "project"
      ],
      "content": "#project \n\nhttps://arxiv.org/abs/2402.07862\n\nThis paper studies how LLMs can serve as assistants to improve human forecasting accuracy, testing two types of assistants - one designed to provide high-quality \"superforecasting\" advice and another designed to be overconfident and noisy. In a study with 991 participants answering forecasting questions, both LLM assistants significantly enhanced human prediction accuracy compared to controls, with the superforecasting assistant showing up to 41% improvement in some analyses.",
      "x": -185.16606034019384,
      "y": -153.3063209812553,
      "size": 19.428571428571427
    },
    {
      "id": "Jeff Fossett",
      "title": "Jeff Fossett",
      "tags": [
        "person"
      ],
      "content": "#person",
      "x": 1652.7106936259504,
      "y": 1204.7793434839928,
      "size": 17.142857142857142
    },
    {
      "id": "Cameron Jones",
      "title": "Cameron Jones",
      "tags": [
        "person"
      ],
      "content": "#person",
      "x": -441.50774791523173,
      "y": -219.45425505903108,
      "size": 16.571428571428573
    },
    {
      "id": "AI Debate Maps",
      "title": "AI Debate Maps",
      "tags": [
        "project"
      ],
      "content": "#project \n\nhttps://www.societylibrary.org/topics-blog/ai-alignment-superintelligence-ethics\n\nDebate maps specifically about Artificial Intelligence.",
      "x": 119.44632840542677,
      "y": -1129.4661000636625,
      "size": 17.142857142857142
    },
    {
      "id": "Sara Fish",
      "title": "Sara Fish",
      "tags": [
        "person"
      ],
      "content": "#person",
      "x": 2115.737299394384,
      "y": 224.70085490593098,
      "size": 16.571428571428573
    },
    {
      "id": "Human vs. Machine -  Behavioral Differences between Expert Humans and Language Models in Wargame Simulations",
      "title": "Human vs. Machine -  Behavioral Differences between Expert Humans and Language Models in Wargame Simulations",
      "tags": [
        "project"
      ],
      "content": "#project \n\nhttps://arxiv.org/pdf/2403.03407\n\nPaper examines how LLMs behave compared to human experts when simulating military crisis decision-making through wargames. The study involved 214 national security experts and compared their responses to AI-simulated teams in a fictional U.S.-China crisis scenario over the Taiwan Strait. The research found that while LLMs showed significant overlap with human decision-making patterns, they demonstrated concerning tendencies toward more aggressive actions and were significantly affected by changes in scenario parameters.",
      "x": -2213.2951352228097,
      "y": -1376.7759485449815,
      "size": 20
    },
    {
      "id": "Hadjar Homaei",
      "title": "Hadjar Homaei",
      "tags": [
        "person"
      ],
      "content": "#person",
      "x": 2078.3315161452247,
      "y": 736.3969386386419,
      "size": 17.142857142857142
    },
    {
      "id": "Philipp Schoenegger",
      "title": "Philipp Schoenegger",
      "tags": [
        "person"
      ],
      "content": "#person \n\nhttps://philipp-schoenegger.weebly.com/",
      "x": -308.0505670602049,
      "y": -355.92667066361173,
      "size": 19.428571428571427
    },
    {
      "id": "William Saunders",
      "title": "William Saunders",
      "tags": [
        "person"
      ],
      "content": "#person",
      "x": -1568.9964861649082,
      "y": 554.7317005121304,
      "size": 16.571428571428573
    },
    {
      "id": "Julian Berger",
      "title": "Julian Berger",
      "tags": [
        "person"
      ],
      "content": "#person",
      "x": 1067.6834794229983,
      "y": 888.5642714490548,
      "size": 16.571428571428573
    },
    {
      "id": "Joe Kwon",
      "title": "Joe Kwon",
      "tags": [
        "person"
      ],
      "content": "#person",
      "x": -893.9290832199296,
      "y": 509.18197162098244,
      "size": 16.571428571428573
    },
    {
      "id": "Jingyan Zhou",
      "title": "Jingyan Zhou",
      "tags": [
        "person"
      ],
      "content": "#person \n\nhttps://para-zhou.github.io/",
      "x": 219.24634731218836,
      "y": 2698.421522266516,
      "size": 16.571428571428573
    },
    {
      "id": "Pietro Nickl",
      "title": "Pietro Nickl",
      "tags": [
        "person"
      ],
      "content": "#person",
      "x": 1005.5126368301044,
      "y": 1026.5466334649998,
      "size": 16.571428571428573
    },
    {
      "id": "Metaculus",
      "title": "Metaculus",
      "tags": [
        "organization"
      ],
      "content": "#organization\n\nhttps://www.metaculus.com/",
      "x": -291.65109336863395,
      "y": -589.3419401920876,
      "size": 20
    },
    {
      "id": "Nuño Sempere",
      "title": "Nuño Sempere",
      "tags": [
        "person"
      ],
      "content": "#person \n\nhttps://nunosempere.com/",
      "x": -1036.424048922515,
      "y": -1428.1816687468543,
      "size": 17.714285714285715
    },
    {
      "id": "Sparse Autoencoders for Hypothesis Generation",
      "title": "Sparse Autoencoders for Hypothesis Generation",
      "tags": [
        "project"
      ],
      "content": "#project \n\nhttps://arxiv.org/pdf/2502.04382\n\nA method that uses sparse autoencoders to automatically generate human-interpretable hypotheses about what features in text data predict certain outcomes (like which headlines get more clicks or what language patterns distinguish Republican vs Democratic speeches). The system works by training neural networks to find interpretable patterns in text, selecting the most predictive patterns, and then using language models to translate those patterns into clear natural language explanations.",
      "x": -1168.3566575077007,
      "y": 3354.988885430247,
      "size": 18.857142857142858
    },
    {
      "id": "Bruno Marnette",
      "title": "Bruno Marnette",
      "tags": [
        "person"
      ],
      "content": "#person \n\nhttps://brunomarnette.substack.com/",
      "x": 731.6434970566758,
      "y": -167.79134490423854,
      "size": 17.714285714285715
    },
    {
      "id": "Ben Rachbach",
      "title": "Ben Rachbach",
      "tags": [
        "person"
      ],
      "content": "#person",
      "x": -1990.652548025034,
      "y": 404.6802552639591,
      "size": 17.714285714285715
    },
    {
      "id": "Chun-Wei Chiang",
      "title": "Chun-Wei Chiang",
      "tags": [
        "person"
      ],
      "content": "#person",
      "x": 1578.4838178652146,
      "y": 3697.1683523660167,
      "size": 16.571428571428573
    },
    {
      "id": "Huaben Chen",
      "title": "Huaben Chen",
      "tags": [
        "person"
      ],
      "content": "#person",
      "x": -1772.3711753708837,
      "y": 2785.444406256389,
      "size": 16.571428571428573
    },
    {
      "id": "David Parkes",
      "title": "David Parkes",
      "tags": [
        "person"
      ],
      "content": "#person",
      "x": 1720.2221261795037,
      "y": 282.75903684678264,
      "size": 18.285714285714285
    },
    {
      "id": "Ian Beacock",
      "title": "Ian Beacock",
      "tags": [
        "person"
      ],
      "content": "#person",
      "x": 1636.8020845388053,
      "y": 1015.3881351163002,
      "size": 17.142857142857142
    },
    {
      "id": "Stefan M. Herzog",
      "title": "Stefan M. Herzog",
      "tags": [
        "person"
      ],
      "content": "#person",
      "x": 1012.5821375962063,
      "y": 903.8478373431252,
      "size": 16.571428571428573
    },
    {
      "id": "Max Lamparth",
      "title": "Max Lamparth",
      "tags": [
        "person"
      ],
      "content": "#person \n\nhttps://www.maxlamparth.com/",
      "x": -2139.075864003679,
      "y": -1403.148621492655,
      "size": 17.142857142857142
    },
    {
      "id": "Dalit Shalom",
      "title": "Dalit Shalom",
      "tags": [
        "person"
      ],
      "content": "#person",
      "x": 1533.9051536390257,
      "y": 840.2001456050014,
      "size": 16.571428571428573
    },
    {
      "id": "The Computational Democracy Project",
      "title": "The Computational Democracy Project",
      "tags": [
        "organization"
      ],
      "content": "#organization \n\nhttps://compdemocracy.org/",
      "x": 2057.334021465157,
      "y": 849.740764420989,
      "size": 19.428571428571427
    },
    {
      "id": "Matthew Botvinick",
      "title": "Matthew Botvinick",
      "tags": [
        "person"
      ],
      "content": "#person",
      "x": 1370.3134606965014,
      "y": 676.0315589136434,
      "size": 18.285714285714285
    },
    {
      "id": "Emma Pierson",
      "title": "Emma Pierson",
      "tags": [
        "person"
      ],
      "content": "#person",
      "x": -1232.3071679230638,
      "y": 3328.5312009090453,
      "size": 16.571428571428573
    },
    {
      "id": "Chris Pal",
      "title": "Chris Pal",
      "tags": [
        "person"
      ],
      "content": "#person",
      "x": 2505.2364184752696,
      "y": 91.18753166945096,
      "size": 16.571428571428573
    },
    {
      "id": "Factored Cognition Primer",
      "title": "Factored Cognition Primer",
      "tags": [
        "project"
      ],
      "content": "#project \n\nhttps://primer.ought.org/\n\nTutorial that teaches how to write \"compositional language model programs\" using factored cognition - a technique that breaks down complex reasoning tasks into smaller, manageable subtasks that can be solved recursively through methods like question-answering and debate. This approach aims to make AI reasoning more transparent and supervisable by decomposing sophisticated thinking into many small, independent tasks that humans can verify.",
      "x": -1885.318632019537,
      "y": 546.5167869305374,
      "size": 16.571428571428573
    },
    {
      "id": "Oriana Skylar Mastro",
      "title": "Oriana Skylar Mastro",
      "tags": [
        "person"
      ],
      "content": "#person",
      "x": -2265.2412686853318,
      "y": -1316.9126737793454,
      "size": 16.571428571428573
    },
    {
      "id": "Generative Social Choice",
      "title": "Generative Social Choice",
      "tags": [
        "project"
      ],
      "content": "#project \n\nhttps://arxiv.org/pdf/2309.01291\n\nPaper introduces a framework that combines LLMs with social choice theory to enable democratic decision-making on open-ended questions where traditional voting on predetermined alternatives isn't sufficient. The system can generate new consensus statements from diverse participant opinions and predict how well individuals would agree with any statement.\n\nFunded by [OpenAI Democratic Inputs to AI](https://github.com/openai/democratic-inputs).",
      "x": 2000.2661562421904,
      "y": 282.115527354089,
      "size": 20
    },
    {
      "id": "Amplifying transformative potential while designing augmented deliberative systems",
      "title": "Amplifying transformative potential while designing augmented deliberative systems",
      "tags": [
        "project"
      ],
      "content": "#project \n\nhttps://ai.objectives.institute/blog/amplifying-transformative-potential-while-designing-augmented-deliberative-systems\n\nBlog post presenting the \"Goldilocks Framework for Augmented Group Intelligence,\" which proposes principles for designing AI systems that enhance human deliberation and decision-making while preserving authentic human agency. The framework argues that effective AI-augmented deliberation requires balancing two key elements: \n1. participants' ability to understand and steer AI outputs (human agency) and \n2. their commitment to deliberative outcomes \n\nHigher-commitment decisions requiring correspondingly higher human agency.",
      "x": 700.6406862856695,
      "y": -77.10666514285275,
      "size": 16.571428571428573
    },
    {
      "id": "Quantified Uncertainty Research Institute",
      "title": "Quantified Uncertainty Research Institute",
      "tags": [
        "organization"
      ],
      "content": "#organization \n\nhttps://quantifieduncertainty.org/",
      "x": -798.2335318785231,
      "y": -1310.1155250287434,
      "size": 17.142857142857142
    },
    {
      "id": "Creating a large language model of a philosopher",
      "title": "Creating a large language model of a philosopher",
      "tags": [
        "project"
      ],
      "content": "#project \n\nhttps://arxiv.org/abs/2302.01339\n\nExperiment where they fine-tuned GPT-3 on the works of Daniel Dennett to see if an LLM can produce expert-quality philosophical texts. They tested the \"digi-Dan\" model by asking both the real Dennett and the AI model ten philosophical questions, then had 425 participants try to distinguish between Dennett's actual answers and the machine-generated responses. The study found that experts often chose the AI's answers over Dennett's actual responses, and on two questions, the AI outputs were selected by more experts than Dennett's own answers.",
      "x": -191.03887356290213,
      "y": 2454.014419868978,
      "size": 18.285714285714285
    },
    {
      "id": "What's Important in \"AI for Epistemics\"?",
      "title": "What's Important in \"AI for Epistemics\"?",
      "tags": [
        "project"
      ],
      "content": "#project \n\nhttps://www.forethought.org/research/whats-important-in-ai-for-epistemics\n\nAnalysis of priorities and strategies for developing \"AI for epistemics\": essentially AI tools that help humans and organizations make better, more informed decisions by improving how we discover, evaluate, and act on information. The paper argues that as AI becomes more powerful, it will dramatically reshape how society generates knowledge and makes decisions, creating both opportunities to enhance human reasoning and risks of manipulation or poor coordination.",
      "x": -1693.3007157168433,
      "y": 424.986550231917,
      "size": 16.571428571428573
    },
    {
      "id": "Making Artificial Intelligence Work for Investigative Journalism",
      "title": "Making Artificial Intelligence Work for Investigative Journalism",
      "tags": [
        "project"
      ],
      "content": "#project \n\nhttps://www.tandfonline.com/doi/full/10.1080/21670811.2019.1630289\n\nPaper examines how artificial intelligence can be applied to investigative journalism, focusing on the practical challenges and realistic opportunities for AI tools in newsrooms. The research reveals that while AI has enormous theoretical potential for helping journalists analyze large datasets and uncover hidden patterns of public interest, current applications remain limited to relatively simple tasks like document classification and data cleaning.",
      "x": 1661.758339817345,
      "y": 1084.58986120295,
      "size": 16.571428571428573
    },
    {
      "id": "Ezra Karger",
      "title": "Ezra Karger",
      "tags": [
        "person"
      ],
      "content": "#person \n\nhttps://ezrakarger.com/",
      "x": 13.294680683389643,
      "y": 76.75026712023329,
      "size": 18.285714285714285
    },
    {
      "id": "LLMediator",
      "title": "LLMediator",
      "tags": [
        "project"
      ],
      "content": "#project \n\nhttps://arxiv.org/pdf/2307.16732\n\nAn experimental platform that uses GPT-4 to enhance online dispute resolution by helping parties communicate more effectively and reach amicable settlements. The system offers three key features: \n1. reformulating inflammatory messages to be less confrontational while preserving their core meaning, \n2. generating draft intervention messages for human mediators to guide discussions, and \n3. allowing AI to autonomously mediate certain low-stakes disputes.",
      "x": 2220.8672292388665,
      "y": -2295.825191344095,
      "size": 17.714285714285715
    },
    {
      "id": "Ai-Heng Lee",
      "title": "Ai-Heng Lee",
      "tags": [
        "person"
      ],
      "content": "#person",
      "x": 1072.0816918590474,
      "y": 1820.2532532332432,
      "size": 16.571428571428573
    },
    {
      "id": "Mantas Mazeika",
      "title": "Mantas Mazeika",
      "tags": [
        "person"
      ],
      "content": "#person",
      "x": -947.6833679189886,
      "y": 532.1636341909024,
      "size": 16.571428571428573
    },
    {
      "id": "Amit Goldenberg",
      "title": "Amit Goldenberg",
      "tags": [
        "person"
      ],
      "content": "#person \n",
      "x": 701.437464398921,
      "y": 737.2087059205674,
      "size": 16.571428571428573
    },
    {
      "id": "Adam Bales",
      "title": "Adam Bales",
      "tags": [
        "person"
      ],
      "content": "#person",
      "x": -1664.4666048359338,
      "y": 602.5420430102494,
      "size": 16.571428571428573
    },
    {
      "id": "Daniel Kokotajlo",
      "title": "Daniel Kokotajlo",
      "tags": [
        "person"
      ],
      "content": "#person",
      "x": -2287.9333745974973,
      "y": 382.44662513897083,
      "size": 17.142857142857142
    },
    {
      "id": "Nathanael Fast",
      "title": "Nathanael Fast",
      "tags": [
        "person"
      ],
      "content": "#person",
      "x": 1443.4431818309442,
      "y": 985.1211103549442,
      "size": 16.571428571428573
    },
    {
      "id": "Tell Me Why - Incentivizing Explanations",
      "title": "Tell Me Why - Incentivizing Explanations",
      "tags": [
        "project"
      ],
      "content": "#project \n\nhttps://arxiv.org/abs/2502.13410\n\nPaper presents a \"deliberation mechanism\" that uses economic incentives to get experts to explain their reasoning, not just state their conclusions. The mechanism works through a three-party structure: experts submit both beliefs and explanations to a supervisor, who then reports aggregated beliefs to a principal, with all parties scored using proper scoring rules based on accuracy. The key insight is that a supervisor can credibly commit to ignoring any expert reports that lack explanations, which forces experts to provide rationales despite the extra effort required, because without explanations, experts receive zero reward.",
      "x": 722.9932566978406,
      "y": 435.1233355907172,
      "size": 18.285714285714285
    },
    {
      "id": "Amelia Wattenberger",
      "title": "Amelia Wattenberger",
      "tags": [
        "person"
      ],
      "content": "#person \n\nhttps://wattenberger.com\nhttps://github.com/Wattenberger",
      "x": 2822.658368554034,
      "y": -1995.5627131174708,
      "size": 17.142857142857142
    },
    {
      "id": "Metaforecast",
      "title": "Metaforecast",
      "tags": [
        "project"
      ],
      "content": "#project \n\nhttps://metaforecast.org\n\nSearch engine and aggregator for predictions and forecasts from various prediction markets and forecasting platforms, allowing users to search for probability estimates on topics like geopolitics, technology, and other events. It provides quality ratings for forecasts and tools to help users navigate and understand prediction data across multiple platforms.\n\n(not currently maintained)",
      "x": -911.2498873715613,
      "y": -1287.3762821910332,
      "size": 17.714285714285715
    },
    {
      "id": "Santeri Koivula",
      "title": "Santeri Koivula",
      "tags": [
        "person"
      ],
      "content": "#person",
      "x": 3526.7139804658846,
      "y": -1186.8896167812852,
      "size": 16.571428571428573
    },
    {
      "id": "What are human values,and how do we align AI to them?",
      "title": "What are human values,and how do we align AI to them?",
      "tags": [
        "project"
      ],
      "content": "#project \n\nhttps://arxiv.org/pdf/2404.10636\n\nPaper introduces \"Moral Graph Elicitation\" (MGE), a method for systematically collecting and organizing human values to better align AI systems with what people actually care about. The researchers developed a process that uses AI-powered interviews to elicit specific \"values cards\" from people (detailed descriptions of what they pay attention to when making meaningful choices) then connects these values in a graph structure based on which values participants consider \"wiser\" than others.",
      "x": 1253.099985655064,
      "y": -246.90291544937514,
      "size": 18.285714285714285
    },
    {
      "id": "Andre Ye",
      "title": "Andre Ye",
      "tags": [
        "person"
      ],
      "content": "#person \n\nhttps://andre-ye.org/",
      "x": 1324.240662897017,
      "y": 1712.652167125521,
      "size": 16.571428571428573
    },
    {
      "id": "Wargames & TTX",
      "title": "Wargames & TTX",
      "tags": [
        "topic"
      ],
      "content": "#topic",
      "x": -1892.2380713597158,
      "y": -1126.8699947554496,
      "size": 18.857142857142858
    },
    {
      "id": "Inyoung Cheong",
      "title": "Inyoung Cheong",
      "tags": [
        "person"
      ],
      "content": "#person",
      "x": 1176.2650821457353,
      "y": 1788.6525225333953,
      "size": 16.571428571428573
    },
    {
      "id": "Jon Kleinberg",
      "title": "Jon Kleinberg",
      "tags": [
        "person"
      ],
      "content": "#person",
      "x": -1212.0001337832573,
      "y": 3407.5149949711413,
      "size": 16.571428571428573
    },
    {
      "id": "Danny Halawi",
      "title": "Danny Halawi",
      "tags": [
        "person"
      ],
      "content": "#person",
      "x": -399.49091861133286,
      "y": 117.53463577365183,
      "size": 17.142857142857142
    },
    {
      "id": "Escalation Risks from Language Models in Military and Diplomatic Decision-Making",
      "title": "Escalation Risks from Language Models in Military and Diplomatic Decision-Making",
      "tags": [
        "project"
      ],
      "content": "#project \nhttps://dl.acm.org/doi/pdf/10.1145/3630106.3658942\n\nPaper examines what happens when LLMs are placed in charge of simulated nations in wargaming scenarios, finding that these AI systems frequently escalate conflicts and, in some cases, even deploy nuclear weapons. The researchers tested five different LLMs controlling autonomous nation agents and discovered that all models showed escalatory patterns, with some developing arms-race dynamics and making decisions based on concerning justifications like deterrence and first-strike tactics.",
      "x": -2094.3333318231967,
      "y": -1459.651732839816,
      "size": 20
    },
    {
      "id": "Filippo Menczer",
      "title": "Filippo Menczer",
      "tags": [
        "person"
      ],
      "content": "#person \n",
      "x": 599.2964114239293,
      "y": 788.7618531523019,
      "size": 16.571428571428573
    },
    {
      "id": "Esin Durmus",
      "title": "Esin Durmus",
      "tags": [
        "person"
      ],
      "content": "#person \n\nhttps://esdurmus.github.io/",
      "x": 2111.255777757817,
      "y": 643.7713226273537,
      "size": 16.571428571428573
    },
    {
      "id": "Robert Gambee",
      "title": "Robert Gambee",
      "tags": [
        "person"
      ],
      "content": "#person",
      "x": -746.1246973487597,
      "y": -677.5281127461095,
      "size": 17.142857142857142
    },
    {
      "id": "Valdemar Danry",
      "title": "Valdemar Danry",
      "tags": [
        "person"
      ],
      "content": "#person\n\nhttps://valdemardanry.com/",
      "x": 1102.4705595757653,
      "y": -2731.308366831243,
      "size": 18.857142857142858
    },
    {
      "id": "Contra papers claiming superhuman AI forecasting",
      "title": "Contra papers claiming superhuman AI forecasting",
      "tags": [
        "project"
      ],
      "content": "#project \n\nhttps://www.lesswrong.com/posts/uGkRcHqatmPkvpGLq/contra-papers-claiming-superhuman-ai-forecasting\n\nCritique of recent academic papers that claim AI systems have achieved \"superhuman\" forecasting performance. The authors from FutureSearch systematically debunk these claims by identifying major methodological flaws, including inadequate information retrieval capabilities, data contamination, unfair timing advantages, and misleading statistical interpretations that make AI performance appear better than it actually is.",
      "x": -912.2504237409754,
      "y": -668.3597869352501,
      "size": 18.857142857142858
    },
    {
      "id": "Srijoni Majumdar",
      "title": "Srijoni Majumdar",
      "tags": [
        "person"
      ],
      "content": "#person",
      "x": -2928.033314011626,
      "y": 1296.9175044223866,
      "size": 16.571428571428573
    },
    {
      "id": "Forecasting Future World Events with Neural Networks",
      "title": "Forecasting Future World Events with Neural Networks",
      "tags": [
        "project"
      ],
      "content": "#project \n\nhttps://arxiv.org/pdf/2206.15474\n\nIntroduces \"Autocast,\" a dataset and benchmark for evaluating neural networks' ability to forecast future world events using thousands of real forecasting questions from public tournaments. The dataset tests language models on diverse topics like politics, economics, and science by simulating historical forecasting conditions—providing only past news articles to prevent information leakage from the future.\n\n(research from 2022)",
      "x": -844.9675993592969,
      "y": 476.331173079975,
      "size": 22.285714285714285
    },
    {
      "id": "ForecastBench - A Dynamic Benchmark of AI Forecasting Capabilities",
      "title": "ForecastBench - A Dynamic Benchmark of AI Forecasting Capabilities",
      "tags": [
        "project"
      ],
      "content": "#project \n\nhttps://arxiv.org/pdf/2409.19839\nhttps://www.forecastbench.org/\n\nBenchmark system that continuously evaluates AI forecasting capabilities using 1,000 automatically generated questions about future events that update in real-time. The system avoids data contamination by using only questions about genuinely unresolved future events, and it compares LLM performance against human forecasters including \"superforecasters\" who have proven track records. Research found that expert human forecasters significantly outperform the best-performing LLMs (like Claude 3.5 Sonnet).",
      "x": -256.173907996596,
      "y": 55.060340516367795,
      "size": 20.57142857142857
    },
    {
      "id": "Sofi Vanhanen",
      "title": "Sofi Vanhanen",
      "tags": [
        "person"
      ],
      "content": "#person \n\nhttps://sofiavanhanen.fi/",
      "x": 3629.495175318134,
      "y": -1087.890476428783,
      "size": 18.285714285714285
    },
    {
      "id": "AI Forecasting Benchmark",
      "title": "AI Forecasting Benchmark",
      "tags": [
        "project"
      ],
      "content": "#project \n\nhttps://www.metaculus.com/aib/\n\nMetaculus AIB (AI Benchmarking) is a series that benchmarks \"the state of the art in AI forecasting against the best humans on real-world questions.\"",
      "x": -491.69904780690194,
      "y": -761.3030318136879,
      "size": 20.57142857142857
    },
    {
      "id": "Mosaic Labs",
      "title": "Mosaic Labs",
      "tags": [
        "organization"
      ],
      "content": "#organization \n\nhttps://mosaic-labs.org/",
      "x": 3659.5707234295,
      "y": -1036.9469431723346,
      "size": 17.142857142857142
    },
    {
      "id": "Rose Novick",
      "title": "Rose Novick",
      "tags": [
        "person"
      ],
      "content": "#person",
      "x": 1360.1274172628246,
      "y": 1757.1206343568244,
      "size": 16.571428571428573
    },
    {
      "id": "AI doing philosophy = AI generating hands?",
      "title": "AI doing philosophy = AI generating hands?",
      "tags": [
        "project"
      ],
      "content": "#project \n\nhttps://www.lesswrong.com/posts/G4ARgcnFogpqorQgb/ai-doing-philosophy-ai-generating-hands-1\n\nWei Dai argues that just as current AI systems generate beautiful images but distorted hands, future AI might excel at science and technology while being dangerously incompetent at philosophical reasoning, creating a critical capability gap that requires urgent attention.",
      "x": 590.8140967392611,
      "y": 2738.198721233041,
      "size": 17.142857142857142
    },
    {
      "id": "Rich Rippin",
      "title": "Rich Rippin",
      "tags": [
        "person"
      ],
      "content": "#person",
      "x": 3694.035488581077,
      "y": 1198.3347755218717,
      "size": 16.571428571428573
    },
    {
      "id": "Why Chatbots Are Not the Future",
      "title": "Why Chatbots Are Not the Future",
      "tags": [
        "project"
      ],
      "content": "#project \n\nhttps://wattenberger.com/thoughts/boo-chatbots\n\nArticle that argues chatbots are poor interfaces for AI systems because they lack clear affordances, force users to learn complex prompting, and isolate responses without allowing iterative refinement. The author advocates for AI tools with better user interfaces that provide contextual controls, visual feedback, and support human agency rather than replacing human decision-making.",
      "x": 2764.192839543276,
      "y": -2018.307292734567,
      "size": 16.571428571428573
    },
    {
      "id": "Andrea Brennen",
      "title": "Andrea Brennen",
      "tags": [
        "person"
      ],
      "content": "#person",
      "x": -1557.6993347002503,
      "y": -2058.063640049635,
      "size": 17.142857142857142
    },
    {
      "id": "Habermas Machine",
      "title": "Habermas Machine",
      "tags": [
        "project"
      ],
      "content": "#project\n\nhttps://www.science.org/doi/10.1126/science.adq2852\n\nAI system that uses two fine-tuned language models to mediate group discussions and help people with differing opinions find common ground on contentious issues. The system takes individual written opinions from participants, generates candidate \"group statements\" that synthesize everyone's perspectives, and iteratively refines these statements based on participant feedback until the group converges on a shared position. Research showed that AI-mediated discussions were more effective than unmediated opinion exposure at causing people to change their minds and reach consensus, with the AI slightly outperforming untrained human mediators.",
      "x": 1358.4189684092353,
      "y": 558.0232276908089,
      "size": 22.857142857142858
    },
    {
      "id": "Daniel Schroeder",
      "title": "Daniel Schroeder",
      "tags": [
        "person"
      ],
      "content": "#person \n",
      "x": 700.0322187809883,
      "y": 793.523096438554,
      "size": 16.571428571428573
    },
    {
      "id": "Yanchen Jiang",
      "title": "Yanchen Jiang",
      "tags": [
        "person"
      ],
      "content": "#person \n",
      "x": 1810.0639307985211,
      "y": -13.16190561814123,
      "size": 16.571428571428573
    },
    {
      "id": "Gabriel Mukobi",
      "title": "Gabriel Mukobi",
      "tags": [
        "person"
      ],
      "content": "#person",
      "x": -2055.7964012026973,
      "y": -1533.6501881483662,
      "size": 16.571428571428573
    },
    {
      "id": "Justin Reppert",
      "title": "Justin Reppert",
      "tags": [
        "person"
      ],
      "content": "#person \n\nhttps://www.justinreppert.com/",
      "x": -2033.3844297777323,
      "y": 436.6834102604725,
      "size": 17.714285714285715
    },
    {
      "id": "Jack Wildman",
      "title": "Jack Wildman",
      "tags": [
        "person"
      ],
      "content": "#person",
      "x": -700.5637035103321,
      "y": -656.0808982602216,
      "size": 17.714285714285715
    },
    {
      "id": "Yiling Chen",
      "title": "Yiling Chen",
      "tags": [
        "person"
      ],
      "content": "#person \n",
      "x": 664.051508207191,
      "y": 365.72042602708405,
      "size": 16.571428571428573
    },
    {
      "id": "Levin Brinkmann",
      "title": "Levin Brinkmann",
      "tags": [
        "person"
      ],
      "content": "#person",
      "x": 1007.1351642394536,
      "y": 967.4523645504034,
      "size": 16.571428571428573
    },
    {
      "id": "Itai Shapira",
      "title": "Itai Shapira",
      "tags": [
        "person"
      ],
      "content": "#person",
      "x": 2069.1543214687053,
      "y": 259.1800226103808,
      "size": 16.571428571428573
    },
    {
      "id": "Bernhard Schölkopf",
      "title": "Bernhard Schölkopf",
      "tags": [
        "person"
      ],
      "content": "#person",
      "x": 2511.1998961170652,
      "y": 32.70947325052749,
      "size": 16.571428571428573
    },
    {
      "id": "Bram Delisse",
      "title": "Bram Delisse",
      "tags": [
        "person"
      ],
      "content": "#person",
      "x": 3810.4621239709318,
      "y": 1068.047576495609,
      "size": 16.571428571428573
    },
    {
      "id": "Helen Meng",
      "title": "Helen Meng",
      "tags": [
        "person"
      ],
      "content": "#person",
      "x": 173.4878607384688,
      "y": 2733.421782830301,
      "size": 16.571428571428573
    },
    {
      "id": "Aldo de Moor",
      "title": "Aldo de Moor",
      "tags": [
        "person"
      ],
      "content": "#person",
      "x": 3730.597492092218,
      "y": 1241.4150514987286,
      "size": 16.571428571428573
    },
    {
      "id": "Debate Maps",
      "title": "Debate Maps",
      "tags": [
        "project"
      ],
      "content": "#project \n\nhttps://www.societylibrary.org/debate-mapping-program\n\nAutomated pipeline for scraping online content, extracting arguments/claims, and organizing them into visual \"maps\" which structure the debate into a tree of statements and their logical relationships.",
      "x": -12.304182348450263,
      "y": -1067.4816033717182,
      "size": 17.142857142857142
    },
    {
      "id": "Outcome-based Reinforcement Learning to Predict the Future",
      "title": "Outcome-based Reinforcement Learning to Predict the Future",
      "tags": [
        "project"
      ],
      "content": "#project \n\nhttps://www.lightningrod.ai/outcome-based-reinforcement-learning-to-predict-the-future\n\nPaper presents a method for training AI models to make better probabilistic forecasts about future events using reinforcement learning with outcome-based rewards. The researchers adapted existing reinforcement learning algorithms (like GRPO and ReMax) to work with the messy, delayed feedback that comes from real-world prediction tasks, using a dataset of over 100,000 forecasting questions from sources like Polymarket. Their approach produces a 14B parameter model that matches the accuracy of frontier models like OpenAI's o1 while achieving better calibration, ultimately translating this into higher hypothetical trading profits when betting against prediction markets.",
      "x": -378.65594725565177,
      "y": -456.26349804572686,
      "size": 19.428571428571427
    },
    {
      "id": "Molly Hickman",
      "title": "Molly Hickman",
      "tags": [
        "person"
      ],
      "content": "#person \n\nhttps://hickman-santini.github.io/",
      "x": -196.22191279599275,
      "y": -377.8911140363388,
      "size": 17.142857142857142
    },
    {
      "id": "Kevin McKee",
      "title": "Kevin McKee",
      "tags": [
        "person"
      ],
      "content": "#person",
      "x": 1426.397731730554,
      "y": 671.5137816131387,
      "size": 16.571428571428573
    },
    {
      "id": "Forethought",
      "title": "Forethought",
      "tags": [
        "organization"
      ],
      "content": "#organization \n\nhttps://www.forethought.org/",
      "x": -1771.7035542392243,
      "y": 317.48703027353775,
      "size": 17.714285714285715
    },
    {
      "id": "AI Future Project",
      "title": "AI Future Project",
      "tags": [
        "organization"
      ],
      "content": "#organization \n\nhttps://ai-futures.org/",
      "x": -2227.808470514541,
      "y": 447.0357084853911,
      "size": 17.142857142857142
    },
    {
      "id": "Kevin Systrom",
      "title": "Kevin Systrom",
      "tags": [
        "person"
      ],
      "content": "#person",
      "x": 3386.618674604603,
      "y": 2616.600659473594,
      "size": 16.571428571428573
    },
    {
      "id": "Bot Mediation",
      "title": "Bot Mediation",
      "tags": [
        "project"
      ],
      "content": "#project \n\nhttps://www.americanbar.org/groups/law_practice/resources/law-technology-today/2025/ai-powered-mediation-for-efficient-legal-dispute-resolution/\n\nAI system designed to mediate disputes between parties without human mediators. The platform claims to reduce mediation timeframes from months to days (without having to pay for professional mediation). The company was selected as a [finalist](https://www.techshow.com/2025/02/voting-is-closed-results-are-in-here-are-the-15-legal-tech-startups-selected-for-the-2025-startup-alley-at-aba-techshow/) in the American Bar Association's [TECHSHOW](https://www.techshow.com/) 2025 Startup Alley competition.",
      "x": -76.12155763233767,
      "y": 3799.673203372321,
      "size": 17.714285714285715
    },
    {
      "id": "Linters for Thought",
      "title": "Linters for Thought",
      "tags": [
        "project"
      ],
      "content": "#project \n\nhttps://textpress.md/jlevy/d/lft.html\n\nEssay about using AI and software engineering principles to augment human intelligence and improve collective problem-solving. The essay proposes developing \"linters\" for human reasoning - automated tools that catch errors in logic, fact-checking, and consistency in writing and decision-making, similar to how code linters catch programming errors.",
      "x": 3931.383288706197,
      "y": -351.52681930047817,
      "size": 16.571428571428573
    },
    {
      "id": "Lizka Vaintrob",
      "title": "Lizka Vaintrob",
      "tags": [
        "person"
      ],
      "content": "#person",
      "x": -1838.159891060826,
      "y": 279.2616617432391,
      "size": 17.714285714285715
    },
    {
      "id": "Fine-tuning language models to find agreement among humans with diverse preferences",
      "title": "Fine-tuning language models to find agreement among humans with diverse preferences",
      "tags": [
        "project"
      ],
      "content": "#project \n\nhttps://proceedings.neurips.cc/paper_files/paper/2022/file/f978c8f3b5f399cae464e85f72e28503-Paper-Conference.pdf\n\nPaper presents a system that fine-tunes an LLM to generate consensus statements that maximize expected approval among groups of people with diverse political opinions. The researchers developed a method where participants provide written opinions on political questions, and an AI system generates candidate consensus statements that aim to find common ground among the group members.\n\n(research from 2022)",
      "x": 1279.701405292439,
      "y": 602.5496145797362,
      "size": 22.285714285714285
    },
    {
      "id": "Sean Trott",
      "title": "Sean Trott",
      "tags": [
        "person"
      ],
      "content": "#person",
      "x": -212.1781690123337,
      "y": -97.70010759208293,
      "size": 16.571428571428573
    },
    {
      "id": "Nathan Young",
      "title": "Nathan Young",
      "tags": [
        "person"
      ],
      "content": "#person \n\nhttps://nathanpmyoung.substack.com/",
      "x": -545.145274347403,
      "y": -983.4306936756087,
      "size": 16.571428571428573
    },
    {
      "id": "Nicholas Christakis",
      "title": "Nicholas Christakis",
      "tags": [
        "person"
      ],
      "content": "#person \n",
      "x": 656.3899362887881,
      "y": 709.3746224659601,
      "size": 16.571428571428573
    },
    {
      "id": "Shiyu Zhao",
      "title": "Shiyu Zhao",
      "tags": [
        "person"
      ],
      "content": "#person",
      "x": -1661.961705989539,
      "y": 2862.074841626482,
      "size": 16.571428571428573
    },
    {
      "id": "Matthew Groh",
      "title": "Matthew Groh",
      "tags": [
        "person"
      ],
      "content": "#person",
      "x": 1156.8936631841618,
      "y": -2589.6255755514535,
      "size": 16.571428571428573
    },
    {
      "id": "Ralf H. J. M. Kurvers",
      "title": "Ralf H. J. M. Kurvers",
      "tags": [
        "person"
      ],
      "content": "#person",
      "x": 965.8893510094586,
      "y": 935.0505653114542,
      "size": 16.571428571428573
    },
    {
      "id": "Ivan Vendrov",
      "title": "Ivan Vendrov",
      "tags": [
        "person"
      ],
      "content": "#person \n\nhttps://www.vendrov.ai/",
      "x": 1810.5588734061498,
      "y": 305.9910348849648,
      "size": 17.714285714285715
    },
    {
      "id": "Jonas Kunst",
      "title": "Jonas Kunst",
      "tags": [
        "person"
      ],
      "content": "#person \n",
      "x": 697.0061324055793,
      "y": 577.4824470578427,
      "size": 16.571428571428573
    },
    {
      "id": "Nat McAleese",
      "title": "Nat McAleese",
      "tags": [
        "person"
      ],
      "content": "#person",
      "x": 1202.6273734661868,
      "y": 517.211418216224,
      "size": 16.571428571428573
    },
    {
      "id": "Elicit",
      "title": "Elicit",
      "tags": [
        "organization"
      ],
      "content": "#organization \n\nhttps://elicit.com/",
      "x": -2027.2873240845167,
      "y": 375.96539772345454,
      "size": 20.57142857142857
    },
    {
      "id": "Can AI bring deliberative democracy to the masses?",
      "title": "Can AI bring deliberative democracy to the masses?",
      "tags": [
        "project"
      ],
      "content": "#project \n\nhttps://www.law.nyu.edu/sites/default/files/Helen%20Landemore%20Can%20AI%20bring%20deliberative%20democracy%20to%20the%20masses.pdf\n\nConceptual paper analyzes France's 2019 \"Great National Debate\" as a case study for scaling deliberative democracy, then theorizes two AI-augmented models: \n1. mass online deliberation platforms that use algorithms to cluster and organize arguments among thousands of participants, and \n2. rotating randomly-selected citizen assemblies supported by AI for facilitation, translation, fact-checking, and data synthesis.",
      "x": 1516.8095912183417,
      "y": 1184.4598882773644,
      "size": 16.571428571428573
    },
    {
      "id": "Colin Megill",
      "title": "Colin Megill",
      "tags": [
        "person"
      ],
      "content": "#person \n\nhttps://colinmegill.com/",
      "x": 2149.182032920351,
      "y": 798.9739724169445,
      "size": 17.142857142857142
    },
    {
      "id": "Diana Acosta-Navas",
      "title": "Diana Acosta-Navas",
      "tags": [
        "person"
      ],
      "content": "#person",
      "x": 1479.1244173198797,
      "y": 826.1725374640181,
      "size": 16.571428571428573
    },
    {
      "id": "John Bash",
      "title": "John Bash",
      "tags": [
        "person"
      ],
      "content": "#person",
      "x": -295.00162878021047,
      "y": -699.0077204062693,
      "size": 17.142857142857142
    },
    {
      "id": "Andreas Stuhlmüller",
      "title": "Andreas Stuhlmüller",
      "tags": [
        "person"
      ],
      "content": "#person \n\nhttps://stuhlmueller.org/",
      "x": -2027.6511164368258,
      "y": 499.3550973775167,
      "size": 17.714285714285715
    },
    {
      "id": "The AI Adoption Gap - Preparing the US Government for Advanced AI",
      "title": "The AI Adoption Gap - Preparing the US Government for Advanced AI",
      "tags": [
        "project"
      ],
      "content": "#project \n\nhttps://www.forethought.org/research/the-ai-adoption-gap\n\nResearch piece examining how the US federal government is falling behind the private sector in adopting AI technologies, creating risks for democratic institutions and national security. The research argues this gap could leave the government unable to effectively respond to AI-driven existential challenges or maintain oversight of rapidly advancing AI systems.",
      "x": -1893.0985324874694,
      "y": 206.04870559270014,
      "size": 16.571428571428573
    },
    {
      "id": "Alex Krasodomski-Jones",
      "title": "Alex Krasodomski-Jones",
      "tags": [
        "person"
      ],
      "content": "#person",
      "x": 795.8151217612628,
      "y": -129.01254009002102,
      "size": 16.571428571428573
    },
    {
      "id": "Zachary Jacobs",
      "title": "Zachary Jacobs",
      "tags": [
        "person"
      ],
      "content": "#person",
      "x": -218.36615785961288,
      "y": -18.596035026356134,
      "size": 17.142857142857142
    },
    {
      "id": "Rethinking Machine Ethics –Can LLMs Perform Moral Reasoning through the Lens of Moral Theories?",
      "title": "Rethinking Machine Ethics –Can LLMs Perform Moral Reasoning through the Lens of Moral Theories?",
      "tags": [
        "project"
      ],
      "content": "#project \n\nhttps://arxiv.org/pdf/2308.15399\n\nPaper presents a framework for enabling LLMs to make moral judgments by grounding them in established moral theories rather than learning from crowdsourced data. The researchers develop prompting techniques that guide models like GPT-4 to reason through ethical scenarios using theories from normative ethics (Justice, Deontology, Utilitarianism) and moral psychology (Theory of Dyadic Morality), producing explainable moral reasoning and decisions.",
      "x": 242.51830751517156,
      "y": 2646.2101137593295,
      "size": 20.57142857142857
    },
    {
      "id": "Joshua Levy",
      "title": "Joshua Levy",
      "tags": [
        "person"
      ],
      "content": "#person \n\nhttps://github.com/jlevy",
      "x": 3872.379266414667,
      "y": -371.0092680854118,
      "size": 16.571428571428573
    },
    {
      "id": "Goodheart Labs",
      "title": "Goodheart Labs",
      "tags": [
        "organization"
      ],
      "content": "#organization \n\nhttps://goodheartlabs.com/",
      "x": -536.1165178685933,
      "y": -862.8993316329166,
      "size": 17.714285714285715
    },
    {
      "id": "Artifact",
      "title": "Artifact",
      "tags": [
        "project"
      ],
      "content": "#project \n\nhttps://techcrunch.com/2024/01/18/why-artifact-from-instagrams-founders-failed-shut-down/\n\nArtifact was an AI-powered news aggregation app created by Instagram's co-founders that used machine learning to recommend articles, summarize news stories, and rewrite clickbait headlines into clearer formats. The app featured social elements like commenting and following, creating an engaged community around news consumption, but ultimately shut down in early 2024.\n\nThey failed because they couldn't achieve sustainable user growth beyond their initial core community, despite having a quality product that users genuinely engaged with.",
      "x": 3432.127389478733,
      "y": 2572.0140546656303,
      "size": 17.142857142857142
    },
    {
      "id": "Wearable Reasoner",
      "title": "Wearable Reasoner",
      "tags": [
        "project"
      ],
      "content": "#project \n\nhttps://dl.acm.org/doi/pdf/10.1145/3384657.3384799\n\nProof-of-concept wearable device (smart glasses with audio feedback) that uses AI to analyze spoken arguments in real-time and tell users whether claims are supported by evidence or not. The system employs argumentation mining techniques to classify statements and provides either simple feedback (\"supported/unsupported\") or explainable feedback that describes what type of evidence was found.",
      "x": 1030.178547158851,
      "y": -2736.2828627715576,
      "size": 18.285714285714285
    },
    {
      "id": "Yuan-Fang Li",
      "title": "Yuan-Fang Li",
      "tags": [
        "person"
      ],
      "content": "#person",
      "x": -2101.408687075367,
      "y": 2357.3363376149478,
      "size": 16.571428571428573
    },
    {
      "id": "Eli Lifland",
      "title": "Eli Lifland",
      "tags": [
        "person"
      ],
      "content": "#person \n\nhttps://www.elilifland.com/",
      "x": -2088.797171070193,
      "y": 462.7401955014765,
      "size": 17.714285714285715
    },
    {
      "id": "David Garcia",
      "title": "David Garcia",
      "tags": [
        "person"
      ],
      "content": "#person \n",
      "x": 603.7471438883641,
      "y": 679.5813518746548,
      "size": 16.571428571428573
    },
    {
      "id": "Brendan Fong",
      "title": "Brendan Fong",
      "tags": [
        "person"
      ],
      "content": "#person \n\nhttp://www.brendanfong.com/",
      "x": -2592.60808144401,
      "y": 1882.646090768226,
      "size": 16.571428571428573
    },
    {
      "id": "Iterated Decomposition - Improving Science Q&A by Supervising Reasoning Processes",
      "title": "Iterated Decomposition - Improving Science Q&A by Supervising Reasoning Processes",
      "tags": [
        "project"
      ],
      "content": "#project \n\nhttps://arxiv.org/pdf/2301.01751\nhttps://github.com/oughtinc/ice\n\nA human-in-the-loop workflow for breaking down complex question-answering tasks into smaller, interpretable steps for language models. The approach involves systematically decomposing complex tasks, evaluating intermediate results, diagnosing failures, and refining the decomposition through multiple iterations, supported by a visualization tool called ICE.",
      "x": -2081.8814606953724,
      "y": 401.41473238306327,
      "size": 20
    },
    {
      "id": "Deceptive Explanations by Large Language Models Lead People to Change their Beliefs About Misinformation More Often than Honest Explanations",
      "title": "Deceptive Explanations by Large Language Models Lead People to Change their Beliefs About Misinformation More Often than Honest Explanations",
      "tags": [
        "project"
      ],
      "content": "#project \n\nhttps://dl.acm.org/doi/pdf/10.1145/3706598.3713408\n\nResearch study examines how AI-generated explanations can be used to combat misinformation more effectively than simple false classifications alone. The researchers conducted an experiment with 589 participants who rated the truthfulness of news headlines before and after receiving AI feedback, finding that deceptive explanations were significantly more persuasive than both honest explanations and deceptive classifications without explanations.",
      "x": 1134.8172370273448,
      "y": -2658.7202582472933,
      "size": 18.285714285714285
    },
    {
      "id": "Peter Mühlbacher",
      "title": "Peter Mühlbacher",
      "tags": [
        "person"
      ],
      "content": "#person \n\nhttp://peter.muehlbacher.me/",
      "x": -857.2972832702691,
      "y": -654.0740118249536,
      "size": 18.285714285714285
    },
    {
      "id": "Lawrence Phillips",
      "title": "Lawrence Phillips",
      "tags": [
        "person"
      ],
      "content": "#person",
      "x": -786.997452942158,
      "y": -671.6640862022364,
      "size": 18.285714285714285
    },
    {
      "id": "Junan Li",
      "title": "Junan Li",
      "tags": [
        "person"
      ],
      "content": "#person",
      "x": 165.7542383900035,
      "y": 2677.979415021302,
      "size": 16.571428571428573
    },
    {
      "id": "Shu Yang Lin",
      "title": "Shu Yang Lin",
      "tags": [
        "person"
      ],
      "content": "#person \n\nhttps://www.shuyanglin.com/",
      "x": 771.7824733892007,
      "y": -32.82433362604927,
      "size": 18.285714285714285
    },
    {
      "id": "Grim",
      "title": "Grim",
      "tags": [
        "project"
      ],
      "content": "#project \n\nhttps://github.com/SentinelTeam/grim\nhttps://www.lesswrong.com/posts/eR69f3hi5ozxchhYg/scaling-wargaming-for-global-catastrophic-risks-with-ai\n\nAI-powered wargaming tool that uses LLMs to simulate complex catastrophic scenarios and help organizations practice emergency responses. The tool functions as a Telegram bot where participants can take actions, request information, and feed data into crisis simulations, with AI serving as both forecaster and game master to create detailed, dynamic scenarios.",
      "x": -1135.8393199919878,
      "y": -1478.5909522257905,
      "size": 17.714285714285715
    },
    {
      "id": "Nikos Bosse",
      "title": "Nikos Bosse",
      "tags": [
        "person"
      ],
      "content": "#person \n\nhttps://followtheargument.org/",
      "x": -843.047489587698,
      "y": -605.5760820500159,
      "size": 18.285714285714285
    },
    {
      "id": "Aleks Berditchevskaia",
      "title": "Aleks Berditchevskaia",
      "tags": [
        "person"
      ],
      "content": "#person",
      "x": 1197.361858670253,
      "y": 1041.6197981931798,
      "size": 16.571428571428573
    },
    {
      "id": "Rolf Kleef",
      "title": "Rolf Kleef",
      "tags": [
        "person"
      ],
      "content": "#person",
      "x": 3843.953645623031,
      "y": 1221.4286225042035,
      "size": 16.571428571428573
    },
    {
      "id": "Raphael Köster",
      "title": "Raphael Köster",
      "tags": [
        "person"
      ],
      "content": "#person",
      "x": 1250.5806382096266,
      "y": 546.7121239912451,
      "size": 17.142857142857142
    },
    {
      "id": "Ravi Iyer",
      "title": "Ravi Iyer",
      "tags": [
        "person"
      ],
      "content": "#person",
      "x": 1360.2647556643778,
      "y": 932.1982198459554,
      "size": 16.571428571428573
    },
    {
      "id": "Jacquelyn Schneider",
      "title": "Jacquelyn Schneider",
      "tags": [
        "person"
      ],
      "content": "#person",
      "x": -2179.5691824765895,
      "y": -1455.2408031463438,
      "size": 17.714285714285715
    },
    {
      "id": "Xin Lucy Liu",
      "title": "Xin Lucy Liu",
      "tags": [
        "person"
      ],
      "content": "#person",
      "x": 4068.3870882762517,
      "y": 188.17854393152223,
      "size": 16.571428571428573
    },
    {
      "id": "Jamie Joyce",
      "title": "Jamie Joyce",
      "tags": [
        "person"
      ],
      "content": "#person \n\nhttps://www.jamiejoyce.com/",
      "x": -130.36663231613733,
      "y": -1067.9827765464338,
      "size": 16.571428571428573
    },
    {
      "id": "Deep Ganguli",
      "title": "Deep Ganguli",
      "tags": [
        "person"
      ],
      "content": "#person \n\nhttps://dganguli.github.io/pweb/",
      "x": 2147.370190950839,
      "y": 688.0551231311405,
      "size": 16.571428571428573
    },
    {
      "id": "Eric Schwitzgebel",
      "title": "Eric Schwitzgebel",
      "tags": [
        "person"
      ],
      "content": "#person",
      "x": -204.58100920442638,
      "y": 2528.4131610049744,
      "size": 16.571428571428573
    },
    {
      "id": "$SL Muses",
      "title": "$SL Muses",
      "tags": [
        "project"
      ],
      "content": "#project \nhttps://www.sltoken.xyz/\n\nAI agents called \"Muses\" that users can tag on social media platforms to perform reasoning tasks like fact-checking claims and researching topics. Their \"Muse of Truth\" provides substantive analysis of factual claims with evidence gaps and source references, while the \"Muse of Research\" pulls resources on requested topics.\n\nThe initiative was funded through a community-created meme token ($SL) that was anonymously gifted to the charity, with 50% of holdings donated to support their mission.",
      "x": -76.37214047101332,
      "y": -1049.2771759946258,
      "size": 16.571428571428573
    },
    {
      "id": "Rajiv Movva",
      "title": "Rajiv Movva",
      "tags": [
        "person"
      ],
      "content": "#person",
      "x": -1165.0217445557917,
      "y": 3285.5408734103257,
      "size": 16.571428571428573
    },
    {
      "id": "Farhad Moghimifar",
      "title": "Farhad Moghimifar",
      "tags": [
        "person"
      ],
      "content": "#person",
      "x": -2155.0928086864137,
      "y": 2280.438293658685,
      "size": 16.571428571428573
    },
    {
      "id": "Lukas Finnveden",
      "title": "Lukas Finnveden",
      "tags": [
        "person"
      ],
      "content": "#person",
      "x": -1770.6095471676322,
      "y": 430.1205037586758,
      "size": 18.285714285714285
    },
    {
      "id": "Reddit",
      "title": "Reddit",
      "tags": [
        "organization"
      ],
      "content": "#organization",
      "x": 1444.619481491618,
      "y": 1191.50396883749,
      "size": 16.571428571428573
    },
    {
      "id": "Samuel Aeschbach",
      "title": "Samuel Aeschbach",
      "tags": [
        "person"
      ],
      "content": "#person",
      "x": 1012.454341600771,
      "y": 1122.9833666029804,
      "size": 16.571428571428573
    },
    {
      "id": "Indre Tuminauskaite",
      "title": "Indre Tuminauskaite",
      "tags": [
        "person"
      ],
      "content": "#person",
      "x": -256.3482287166553,
      "y": -305.9958249138554,
      "size": 16.571428571428573
    },
    {
      "id": "Israel-Palestine Collective Dialogues",
      "title": "Israel-Palestine Collective Dialogues",
      "tags": [
        "project"
      ],
      "content": "#project \n\nhttps://arxiv.org/abs/2503.01769\n\nPaper describes a case study of using AI and collective dialogue methods to help Israeli and Palestinian peacebuilders find common ground during the conflict following October 7, 2023. The research integrates LLMs, bridging-based ranking algorithms, and online collective dialogues to identify shared perspectives across divided groups.",
      "x": 1513.2149875917946,
      "y": 692.5559634486403,
      "size": 20
    },
    {
      "id": "Jaromir Savelka",
      "title": "Jaromir Savelka",
      "tags": [
        "person"
      ],
      "content": "#person",
      "x": 2206.1527616859025,
      "y": -2232.476065623996,
      "size": 16.571428571428573
    },
    {
      "id": "Opportunities and Risks of LLMs for Scalable Deliberation with Polis",
      "title": "Opportunities and Risks of LLMs for Scalable Deliberation with Polis",
      "tags": [
        "project"
      ],
      "content": "#project",
      "x": 2030.0768750921764,
      "y": 689.5426397389598,
      "size": 21.142857142857142
    },
    {
      "id": "Luca Righetti",
      "title": "Luca Righetti",
      "tags": [
        "person"
      ],
      "content": "#person",
      "x": -1608.5787237565016,
      "y": 592.2434745346627,
      "size": 16.571428571428573
    },
    {
      "id": "Anka Reuel",
      "title": "Anka Reuel",
      "tags": [
        "person"
      ],
      "content": "#person",
      "x": -2019.6451071960157,
      "y": -1489.8746609698208,
      "size": 16.571428571428573
    },
    {
      "id": "Andrew Konya",
      "title": "Andrew Konya",
      "tags": [
        "person"
      ],
      "content": "#person \n\nhttps://andrewkonya.com/",
      "x": 1563.7748126336485,
      "y": 791.1411106203664,
      "size": 18.857142857142858
    },
    {
      "id": "IQT Labs",
      "title": "IQT Labs",
      "tags": [
        "organization"
      ],
      "content": "#organization \n\nhttps://www.iqt.org/",
      "x": -1501.088614337867,
      "y": -2102.1091151589467,
      "size": 17.142857142857142
    },
    {
      "id": "Gholamreza Haffari",
      "title": "Gholamreza Haffari",
      "tags": [
        "person"
      ],
      "content": "#person",
      "x": -2181.9887577435397,
      "y": 2411.6063431444727,
      "size": 16.571428571428573
    },
    {
      "id": "Joe Edelman",
      "title": "Joe Edelman",
      "tags": [
        "person"
      ],
      "content": "#person \n\nhttps://nxhx.org/",
      "x": 1356.6939030945252,
      "y": -196.06959723472013,
      "size": 17.714285714285715
    },
    {
      "id": "MIT Media Lab",
      "title": "MIT Media Lab",
      "tags": [
        "organization"
      ],
      "content": "#organization \n\nhttps://www.media.mit.edu/",
      "x": 1006.2843605632936,
      "y": -2684.9292905918865,
      "size": 17.714285714285715
    },
    {
      "id": "Evangelos Pournaras",
      "title": "Evangelos Pournaras",
      "tags": [
        "person"
      ],
      "content": "#person",
      "x": -2894.3098275292855,
      "y": 1188.511929249774,
      "size": 16.571428571428573
    },
    {
      "id": "Jacob Ganz",
      "title": "Jacob Ganz",
      "tags": [
        "person"
      ],
      "content": "#person",
      "x": -2270.1138391784716,
      "y": -1434.05530323859,
      "size": 17.142857142857142
    },
    {
      "id": "Francisco Marmolejo-Cossío",
      "title": "Francisco Marmolejo-Cossío",
      "tags": [
        "person"
      ],
      "content": "#person \n",
      "x": 1876.5959770832362,
      "y": 128.6832906753345,
      "size": 16.571428571428573
    },
    {
      "id": "Jan Balaguer",
      "title": "Jan Balaguer",
      "tags": [
        "person"
      ],
      "content": "#person \n\nhttps://www.janbalaguer.net/",
      "x": 1227.3076588867907,
      "y": 638.3421386448723,
      "size": 17.714285714285715
    },
    {
      "id": "The Society Library",
      "title": "The Society Library",
      "tags": [
        "organization"
      ],
      "content": "#organization \n\nhttps://societylibrary.medium.com/\nhttps://www.societylibrary.org/",
      "x": -123.51596440526208,
      "y": -972.6539885865532,
      "size": 18.285714285714285
    },
    {
      "id": "Mantic",
      "title": "Mantic",
      "tags": [
        "organization"
      ],
      "content": "#organization \n\nhttps://mntc.ai/",
      "x": -472.4300783671211,
      "y": -887.3407076068335,
      "size": 17.714285714285715
    },
    {
      "id": "Peter Park",
      "title": "Peter Park",
      "tags": [
        "person"
      ],
      "content": "#person",
      "x": -226.8077158046543,
      "y": -232.4893312689628,
      "size": 17.142857142857142
    },
    {
      "id": "Peter Wills",
      "title": "Peter Wills",
      "tags": [
        "person"
      ],
      "content": "#person",
      "x": -1677.473663165288,
      "y": 547.2208205595574,
      "size": 16.571428571428573
    },
    {
      "id": "Flynn Devine",
      "title": "Flynn Devine",
      "tags": [
        "person"
      ],
      "content": "#person",
      "x": 847.9493191426627,
      "y": -152.40763438745594,
      "size": 16.571428571428573
    },
    {
      "id": "Gordon Pennycook",
      "title": "Gordon Pennycook",
      "tags": [
        "person"
      ],
      "content": "#person \n",
      "x": 562.3827246281912,
      "y": 703.2775975225394,
      "size": 16.571428571428573
    },
    {
      "id": "Dawn Song",
      "title": "Dawn Song",
      "tags": [
        "person"
      ],
      "content": "#person \n\nhttps://dawnsong.io/",
      "x": 59.22529616244745,
      "y": 606.6469952544618,
      "size": 17.142857142857142
    },
    {
      "id": "Wisdom of the silicon crowd - LLM ensemble prediction capabilities rival human crowd accuracy",
      "title": "Wisdom of the silicon crowd - LLM ensemble prediction capabilities rival human crowd accuracy",
      "tags": [
        "project"
      ],
      "content": "#project \n\nhttps://www.science.org/doi/pdf/10.1126/sciadv.adp1528\n\nPaper demonstrates that an ensemble of 12 different large language models can achieve forecasting accuracy that rivals human crowd predictions in real-world forecasting tournaments. The study shows that while individual LLMs typically underperform human crowds, aggregating predictions from multiple LLMs creates a \"wisdom of the silicon crowd\" effect that matches human accuracy on 31 binary forecasting questions.",
      "x": -292.04779945167223,
      "y": -239.92132255230587,
      "size": 19.428571428571427
    },
    {
      "id": "Anthony Corso",
      "title": "Anthony Corso",
      "tags": [
        "person"
      ],
      "content": "#person",
      "x": -2207.0321862085693,
      "y": -1302.1970500310003,
      "size": 16.571428571428573
    },
    {
      "id": "Aviv Ovadya",
      "title": "Aviv Ovadya",
      "tags": [
        "person"
      ],
      "content": "#person \n\nhttps://aviv.me/",
      "x": 1597.9484999890756,
      "y": 842.2153044669851,
      "size": 18.285714285714285
    },
    {
      "id": "Michiel Bakker",
      "title": "Michiel Bakker",
      "tags": [
        "person"
      ],
      "content": "#person \n\nhttps://miba.dev/",
      "x": 1225.501866244462,
      "y": 730.5079873536822,
      "size": 21.142857142857142
    },
    {
      "id": "Let’s use AI to harden human defenses against AI manipulation",
      "title": "Let’s use AI to harden human defenses against AI manipulation",
      "tags": [
        "project"
      ],
      "content": "#project \n\nhttps://www.lesswrong.com/posts/zxmzBTwKkPMxQQcfR/let-s-use-ai-to-harden-human-defenses-against-ai\n\nBlog post proposes using AI systems to discover and catalog human manipulation techniques in order to train \"detector-AIs\" and humans to recognize these tactics, essentially creating a defensive system against AI manipulation. The approach involves optimizing AIs to persuade humans of both truths and falsehoods, analyzing what manipulation strategies they develop, and then training detection systems to identify these techniques in real-world scenarios.",
      "x": -1820.2062335519965,
      "y": 123.53840436834315,
      "size": 16.571428571428573
    },
    {
      "id": "Anne-Marie Nussberger",
      "title": "Anne-Marie Nussberger",
      "tags": [
        "person"
      ],
      "content": "#person",
      "x": 1049.2268533909287,
      "y": 993.0573248996991,
      "size": 16.571428571428573
    },
    {
      "id": "Mapping AI Discourse",
      "title": "Mapping AI Discourse",
      "tags": [
        "topic"
      ],
      "content": "#topic",
      "x": 953.8736451437898,
      "y": -640.3740320424765,
      "size": 18.285714285714285
    },
    {
      "id": "Wei Dai",
      "title": "Wei Dai",
      "tags": [
        "person"
      ],
      "content": "#person \n\nhttps://wdai.us/",
      "x": 592.1653710341611,
      "y": 2821.081196902191,
      "size": 16.571428571428573
    },
    {
      "id": "Finn Hambly",
      "title": "Finn Hambly",
      "tags": [
        "person"
      ],
      "content": "#person",
      "x": -777.0782596942344,
      "y": -732.8668921963484,
      "size": 17.142857142857142
    },
    {
      "id": "Christopher Small",
      "title": "Christopher Small",
      "tags": [
        "person"
      ],
      "content": "#person \n\nhttp://metasoarous.com/",
      "x": 1854.4692018669803,
      "y": 861.0514458806144,
      "size": 18.285714285714285
    },
    {
      "id": "Lisa Schirch",
      "title": "Lisa Schirch",
      "tags": [
        "person"
      ],
      "content": "#person \n\nhttps://kroc.nd.edu/faculty-and-staff/lisa-schirch/",
      "x": 1561.488577698378,
      "y": 890.5844108032592,
      "size": 18.285714285714285
    },
    {
      "id": "Nikhil Garg",
      "title": "Nikhil Garg",
      "tags": [
        "person"
      ],
      "content": "#person",
      "x": -1128.707877583395,
      "y": 3412.009170808478,
      "size": 16.571428571428573
    },
    {
      "id": "Sensemaker",
      "title": "Sensemaker",
      "tags": [
        "project"
      ],
      "content": "#project \n\nhttps://github.com/Jigsaw-Code/sensemaking-tools/\nhttps://report.whatcouldbgbe.com/\n\nCollection of AI-powered systems designed to help analyze and understand large-scale online conversations by automatically identifying topics, categorizing statements, and summarizing areas of agreement and disagreement from thousands of public comments.",
      "x": 1893.37190843819,
      "y": 1114.4626559031512,
      "size": 16.571428571428573
    },
    {
      "id": "Joshua Becker",
      "title": "Joshua Becker",
      "tags": [
        "person"
      ],
      "content": "#person",
      "x": 1052.7700255883067,
      "y": 937.2987696504957,
      "size": 16.571428571428573
    },
    {
      "id": "Frank Schweitzer",
      "title": "Frank Schweitzer",
      "tags": [
        "person"
      ],
      "content": "#person \n",
      "x": 558.6711963993663,
      "y": 760.2996607075387,
      "size": 16.571428571428573
    },
    {
      "id": "Elizabeth Barry",
      "title": "Elizabeth Barry",
      "tags": [
        "person"
      ],
      "content": "#person",
      "x": 2038.8526036228927,
      "y": 782.8111371581372,
      "size": 17.142857142857142
    },
    {
      "id": "Zhuoran Lu",
      "title": "Zhuoran Lu",
      "tags": [
        "person"
      ],
      "content": "#person",
      "x": 1619.8499804346925,
      "y": 3776.7969245538343,
      "size": 16.571428571428573
    },
    {
      "id": "Pat Pataranutaporn",
      "title": "Pat Pataranutaporn",
      "tags": [
        "person"
      ],
      "content": "#person \n\nhttps://patpat.world/",
      "x": 1066.2378443172674,
      "y": -2682.567814798428,
      "size": 18.285714285714285
    },
    {
      "id": "LLMs Can Teach Themselves to Better Predict the Future",
      "title": "LLMs Can Teach Themselves to Better Predict the Future",
      "tags": [
        "project"
      ],
      "content": "#project \n\nhttps://arxiv.org/pdf/2502.05253\n\nMethod for improving large language models' forecasting capabilities through self-play and outcome-driven fine-tuning, where models generate multiple reasoning traces for prediction questions and learn from which approaches led to more accurate forecasts. The authors demonstrate that smaller models (14B parameters) fine-tuned with this method can achieve forecasting performance comparable to much larger frontier models like GPT-4o.",
      "x": -409.11929591810326,
      "y": -402.5919502022023,
      "size": 18.285714285714285
    },
    {
      "id": "Martin Chadwick",
      "title": "Martin Chadwick",
      "tags": [
        "person"
      ],
      "content": "#person",
      "x": 1305.5587724179861,
      "y": 528.6905398060712,
      "size": 17.142857142857142
    },
    {
      "id": "Gary Marcus",
      "title": "Gary Marcus",
      "tags": [
        "person"
      ],
      "content": "#person \n\nhttp://www.garymarcus.com/",
      "x": 647.5288302303742,
      "y": 648.3052897331523,
      "size": 16.571428571428573
    },
    {
      "id": "Fred Zhang",
      "title": "Fred Zhang",
      "tags": [
        "person"
      ],
      "content": "#person \n\nhttps://fredzhang.me/",
      "x": -379.93425636737896,
      "y": 176.99362172420902,
      "size": 17.142857142857142
    },
    {
      "id": "Nexus",
      "title": "Nexus",
      "tags": [
        "project"
      ],
      "content": "#project \n\nhttps://nexus-tool.com/\n\nNexus is a discussion platform that organizes user statements and reactions into a visual graph, where semantically similar statements are connected and colored based on consensus, divergence, and quality scores. The tool specifically counteracts factionalism and amplifying unique perspectives by weighting minority voices more heavily, highlighting surprising agreement between usual opponents (consensus), and promoting dissenting views within like-minded groups (divergence).",
      "x": 3709.580929369303,
      "y": -1062.1952294556093,
      "size": 17.142857142857142
    },
    {
      "id": "AI Objectives Institute",
      "title": "AI Objectives Institute",
      "tags": [
        "organization"
      ],
      "content": "#organization \n\nhttps://ai.objectives.institute/",
      "x": 665.7615191436854,
      "y": -247.07081557119588,
      "size": 20.57142857142857
    },
    {
      "id": "Ryan Jia",
      "title": "Ryan Jia",
      "tags": [
        "person"
      ],
      "content": "#person",
      "x": -902.6698848942369,
      "y": 567.3643356058368,
      "size": 16.571428571428573
    },
    {
      "id": "Ought",
      "title": "Ought",
      "tags": [
        "organization"
      ],
      "content": "#organization \n\nhttps://ought.org/ (website is old)\n\nOught 1.0 has effectively become what is now Elicit (the company) continuing development on Elicit (the tool). Ought 2.0 has not yet done any public work.",
      "x": -1888.3722954977052,
      "y": 453.0511056583274,
      "size": 23.42857142857143
    },
    {
      "id": "Prateek Buch",
      "title": "Prateek Buch",
      "tags": [
        "person"
      ],
      "content": "#person",
      "x": 1413.2051277119535,
      "y": 936.8849366411313,
      "size": 16.571428571428573
    },
    {
      "id": "Martin Saveski",
      "title": "Martin Saveski",
      "tags": [
        "person"
      ],
      "content": "#person",
      "x": 1207.126326621334,
      "y": 356.73600567435466,
      "size": 16.571428571428573
    },
    {
      "id": "Paul Gölz",
      "title": "Paul Gölz",
      "tags": [
        "person"
      ],
      "content": "#person",
      "x": 2128.522044837979,
      "y": 280.17413887250194,
      "size": 16.571428571428573
    },
    {
      "id": "Meaning Alignment Institute",
      "title": "Meaning Alignment Institute",
      "tags": [
        "organization"
      ],
      "content": "#organization \n\nhttps://www.meaningalignment.org/",
      "x": 1258.0435242093358,
      "y": -308.1817574200967,
      "size": 17.714285714285715
    },
    {
      "id": "Q1 AI Benchmarking results",
      "title": "Q1 AI Benchmarking results",
      "tags": [
        "project"
      ],
      "content": "#project \n\nhttps://www.metaculus.com/notebooks/38673/q1-ai-benchmarking-results/\n\nReport on Metaculus's quarterly tournament that compared AI forecasting bots against professional human forecasters on real-world prediction questions, with the key finding that professional forecasters significantly outperformed the best AI systems.",
      "x": -376.8873672015769,
      "y": -773.3612804866773,
      "size": 18.285714285714285
    },
    {
      "id": "Lucie Flek",
      "title": "Lucie Flek",
      "tags": [
        "person"
      ],
      "content": "#person",
      "x": 1186.0900331035532,
      "y": 992.1321419547221,
      "size": 16.571428571428573
    },
    {
      "id": "Harold Trinkunas",
      "title": "Harold Trinkunas",
      "tags": [
        "person"
      ],
      "content": "#person",
      "x": -2291.422696857617,
      "y": -1367.70469698592,
      "size": 16.571428571428573
    },
    {
      "id": "Nasim Rahaman",
      "title": "Nasim Rahaman",
      "tags": [
        "person"
      ],
      "content": "#person",
      "x": 2557.8866861803585,
      "y": 65.65856855303231,
      "size": 16.571428571428573
    },
    {
      "id": "Xiaoying Zhang",
      "title": "Xiaoying Zhang",
      "tags": [
        "person"
      ],
      "content": "#person",
      "x": 175.88879151181476,
      "y": 2621.7430464125996,
      "size": 16.571428571428573
    },
    {
      "id": "Roast My Post",
      "title": "Roast My Post",
      "tags": [
        "project"
      ],
      "content": "#project \n\nhttps://www.roastmypost.org\nhttps://github.com/quantified-uncertainty/roast-my-post\n\nOpen-source AI-powered platform for document analysis and evaluation. It allows users to upload documents and receive feedback with inline comments from customizable AI agents.",
      "x": -796.7656435233521,
      "y": -1415.5938324039541,
      "size": 16.571428571428573
    },
    {
      "id": "Forecasting Research Institute",
      "title": "Forecasting Research Institute",
      "tags": [
        "organization"
      ],
      "content": "#organization \n\nhttps://forecastingresearch.org/",
      "x": -153.6122917815696,
      "y": -87.61093230240742,
      "size": 18.857142857142858
    },
    {
      "id": "Christopher Summerfield",
      "title": "Christopher Summerfield",
      "tags": [
        "person"
      ],
      "content": "#person",
      "x": 1037.8359073408596,
      "y": 622.2526019294744,
      "size": 18.285714285714285
    },
    {
      "id": "Colin Irwin",
      "title": "Colin Irwin",
      "tags": [
        "person"
      ],
      "content": "#person",
      "x": 1787.8513614818683,
      "y": 917.2872833286947,
      "size": 16.571428571428573
    },
    {
      "id": "Global Dialogues",
      "title": "Global Dialogues",
      "tags": [
        "project"
      ],
      "content": "#project \n\nhttps://globaldialogues.ai\n\nResearch initiative that uses AI-powered interactive dialogue platforms to systematically map global public perspectives on AI development and its societal impacts.",
      "x": 1296.0147866388418,
      "y": 779.8433552858686,
      "size": 17.714285714285715
    },
    {
      "id": "X Community Notes",
      "title": "X Community Notes",
      "tags": [
        "organization"
      ],
      "content": "#organization \n\nhttps://communitynotes.x.com/guide/en/welcome",
      "x": 1317.2122904651371,
      "y": 183.28666940004246,
      "size": 16.571428571428573
    },
    {
      "id": "Hoover Wargaming and Crisis Simulation Initiative",
      "title": "Hoover Wargaming and Crisis Simulation Initiative",
      "tags": [
        "organization"
      ],
      "content": "#organization \n\nhttps://www.hoover.org/research-teams/wargaming-and-crisis-simulation-initiative",
      "x": -2247.325695112165,
      "y": -1498.6468612151302,
      "size": 17.142857142857142
    },
    {
      "id": "Luke Stebbing",
      "title": "Luke Stebbing",
      "tags": [
        "person"
      ],
      "content": "#person \n\nhttps://lukestebbing.com/",
      "x": -1992.994109604841,
      "y": 471.5976775812864,
      "size": 17.714285714285715
    },
    {
      "id": "How Malicious AI Swarms Can Threaten Democracy",
      "title": "How Malicious AI Swarms Can Threaten Democracy",
      "tags": [
        "project"
      ],
      "content": "#project \n\nhttps://arxiv.org/pdf/2506.06299\n\nExamines how coordinated networks of AI agents (\"AI swarms\") can manipulate democratic discourse and decision-making through sophisticated influence operations. The authors warn that unlike traditional botnets that simply repeat scripted messages, AI swarms can create thousands of distinct personas that adapt, learn from feedback, and coordinate autonomously to fabricate grassroots consensus and fragment shared reality.",
      "x": 734.1845256822567,
      "y": 681.6766011163681,
      "size": 28
    },
    {
      "id": "CeesJan Mol",
      "title": "CeesJan Mol",
      "tags": [
        "person"
      ],
      "content": "#person",
      "x": 3847.7837913166422,
      "y": 1109.426556128145,
      "size": 16.571428571428573
    },
    {
      "id": "Philip Tetlock",
      "title": "Philip Tetlock",
      "tags": [
        "person"
      ],
      "content": "#person",
      "x": -269.190694144188,
      "y": -121.36964146524063,
      "size": 18.857142857142858
    },
    {
      "id": "Sayash Kapoor",
      "title": "Sayash Kapoor",
      "tags": [
        "person"
      ],
      "content": "#person",
      "x": 1095.3622312447662,
      "y": 1015.6017130919702,
      "size": 16.571428571428573
    },
    {
      "id": "Plurality Mapping Project",
      "title": "Plurality Mapping Project",
      "tags": [
        "project"
      ],
      "content": "#project \n\nhttps://publish.obsidian.md/plurality-map",
      "x": 1580.7603953677365,
      "y": 1181.6513301396883,
      "size": 16.571428571428573
    },
    {
      "id": "Juan-Pablo Rivera",
      "title": "Juan-Pablo Rivera",
      "tags": [
        "person"
      ],
      "content": "#person",
      "x": -2112.588722836765,
      "y": -1535.2811894329448,
      "size": 16.571428571428573
    },
    {
      "id": "Anita W. Woolley",
      "title": "Anita W. Woolley",
      "tags": [
        "person"
      ],
      "content": "#person",
      "x": 1199.6621406794438,
      "y": 1097.6159370392663,
      "size": 16.571428571428573
    },
    {
      "id": "AI4Democracy - How AI Can Be Used to Inform Policymaking?",
      "title": "AI4Democracy - How AI Can Be Used to Inform Policymaking?",
      "tags": [
        "project"
      ],
      "content": "#project \n\nhttps://ai.objectives.institute/blog/ai4democracy-paper-how-ai-can-be-used-to-inform-policymaking\n\nReport on how LLMs can be used to process large volumes of public input and aggregate opinions for policymakers, specifically with \"[Talk to the City](https://ai.objectives.institute/talk-to-the-city).\" The paper examines three real-world case studies (including one on [DAO](https://en.wikipedia.org/wiki/Decentralized_autonomous_organization) governance).",
      "x": 480.8942378747947,
      "y": -429.5254953059022,
      "size": 17.142857142857142
    },
    {
      "id": "Deep Research Bench - Evaluating AI Web Research Agents",
      "title": "Deep Research Bench - Evaluating AI Web Research Agents",
      "tags": [
        "project"
      ],
      "content": "#project \n\nhttps://arxiv.org/abs/2506.06287\nhttps://evals.futuresearch.ai/\n\nBenchmark for evaluating AI agents on complex, multi-step web research tasks that mirror real-world analytical work, using a frozen web environment to ensure consistent and repeatable evaluations. The benchmark tests research capabilities including strategic planning, source credibility assessment, systematic information gathering, and synthesis across contradictory sources through eight different task categories.",
      "x": -788.3209992852137,
      "y": -565.6379415639828,
      "size": 21.142857142857142
    },
    {
      "id": "Edith Elkind",
      "title": "Edith Elkind",
      "tags": [
        "person"
      ],
      "content": "#person",
      "x": -2823.591781504284,
      "y": 1282.3060599742723,
      "size": 16.571428571428573
    },
    {
      "id": "Common Ground",
      "title": "Common Ground",
      "tags": [
        "project"
      ],
      "content": "#project \n\nhttps://www.dembrane.com/en-US/blog/report-openai-october-2023\n\nAI-powered platform for democratic deliberation that organizes people into small groups to discuss and vote on statements, with an AI moderator (powered by GPT-4) synthesizing new statements from their live video conversations with each other. Statements produced by each group are \"cross pollinated\" to each other and ultimately synthesized into final consensus statements.\n\nFunded by [OpenAI Democratic Inputs to AI](https://github.com/openai/democratic-inputs).",
      "x": 3769.4335996636014,
      "y": 1129.4195938439689,
      "size": 22.857142857142858
    },
    {
      "id": "Echo",
      "title": "Echo",
      "tags": [
        "project"
      ],
      "content": "#project \n\nhttps://www.dembrane.com/en-US/products/echo\nhttps://github.com/dembrane/echo\n\nECHO is a transcription, analysis, and reporting platform designed for multilingual conversations and stakeholder engagement scenarios. It can capture multiple simultaneous conversations in real-time, transcribe them across multiple languages, and use AI to analyze the content and generate insights from the discussions.",
      "x": 3612.5665593855706,
      "y": 979.1067686717648,
      "size": 16.571428571428573
    },
    {
      "id": "Dembrane",
      "title": "Dembrane",
      "tags": [
        "organization"
      ],
      "content": "#organization \n\nhttps://www.dembrane.com\n\nFunded (in part) by the European Union via the [AI4Deliberation](https://www.ai4dproject.eu/) project.",
      "x": 3670.5319699200368,
      "y": 1017.8958794711315,
      "size": 17.714285714285715
    },
    {
      "id": "John Aslanides",
      "title": "John Aslanides",
      "tags": [
        "person"
      ],
      "content": "#person",
      "x": 1303.997142438219,
      "y": 470.9978661943301,
      "size": 16.571428571428573
    },
    {
      "id": "Supernotes",
      "title": "Supernotes",
      "tags": [
        "project"
      ],
      "content": "#project \n\nhttps://dl.acm.org/doi/pdf/10.1145/3696410.3714934\n\nAI system that synthesizes multiple community-written fact-checking notes into single, more effective notes that build consensus among diverse users. The system uses an LLM to generate many candidate notes from existing human-written notes, then employs a machine learning model trained on millions of historical ratings to predict which candidates would be most helpful to users with different viewpoints.",
      "x": 1244.5536392225954,
      "y": 442.2656807259026,
      "size": 18.285714285714285
    },
    {
      "id": "Accelerated Preference Elicitation with LLM-Based Proxies",
      "title": "Accelerated Preference Elicitation with LLM-Based Proxies",
      "tags": [
        "project"
      ],
      "content": "#project \n\nhttps://arxiv.org/pdf/2501.14625\n\nAI system that help people communicate their preferences more efficiently in complex auctions by using natural language instead of technical queries. The system uses LLM-powered \"proxies\" that learn what people want through conversational interactions and can infer preferences for items not explicitly discussed.",
      "x": 1821.5847704294936,
      "y": 212.07982179665802,
      "size": 18.285714285714285
    },
    {
      "id": "LLM-Powered Preference Elicitation in Combinatorial Assignment",
      "title": "LLM-Powered Preference Elicitation in Combinatorial Assignment",
      "tags": [
        "project"
      ],
      "content": "#project \n\nhttps://arxiv.org/pdf/2502.10308\n\nAI system that allows people to express complex preferences through natural language rather than answering technical queries in allocation mechanisms. For example, in course registration, students could provide a single free-text description of their preferences (e.g., \"I prefer courses scheduled closely together\" or \"Course A and B complement each other\"), and an LLM proxy would answer comparison queries on their behalf. Experiments testing the system were run using LLM-simulated people.",
      "x": 1811.4457162469735,
      "y": 87.51711177245846,
      "size": 19.428571428571427
    },
    {
      "id": "Jon Evans",
      "title": "Jon Evans",
      "tags": [
        "person"
      ],
      "content": "#person \n\nhttps://rezendi.com",
      "x": -610.9648619044706,
      "y": -618.6960073290362,
      "size": 18.285714285714285
    },
    {
      "id": "Nick Hikita",
      "title": "Nick Hikita",
      "tags": [
        "person"
      ],
      "content": "#person",
      "x": -13.373112740791635,
      "y": 3784.34210242425,
      "size": 16.571428571428573
    },
    {
      "id": "Iyad Rahwan",
      "title": "Iyad Rahwan",
      "tags": [
        "person"
      ],
      "content": "#person",
      "x": 959.255604596829,
      "y": 991.807045825743,
      "size": 16.571428571428573
    },
    {
      "id": "Anna Strasser",
      "title": "Anna Strasser",
      "tags": [
        "person"
      ],
      "content": "#person",
      "x": -259.0789864987981,
      "y": 2511.229859108959,
      "size": 16.571428571428573
    },
    {
      "id": "Romuald Elie",
      "title": "Romuald Elie",
      "tags": [
        "person"
      ],
      "content": "#person",
      "x": 1102.9510065073475,
      "y": 584.5322563537535,
      "size": 16.571428571428573
    },
    {
      "id": "Grace Kwak Danciu",
      "title": "Grace Kwak Danciu",
      "tags": [
        "person"
      ],
      "content": "#person",
      "x": 1469.0361038918277,
      "y": 932.0050994350513,
      "size": 16.571428571428573
    },
    {
      "id": "Prompt Engineering Large Language Models’ Forecasting Capabilities",
      "title": "Prompt Engineering Large Language Models’ Forecasting Capabilities",
      "tags": [
        "project"
      ],
      "content": "#project \n\nhttps://arxiv.org/pdf/2506.01578\n\nPaper presents a systematic study of prompt engineering techniques for improving large language models' forecasting capabilities. The researchers tested 38 different prompts across four major LLMs using 100 forecasting questions, followed by a second study with compound and professionally-designed prompts. Surprisingly, they found that most prompt modifications had negligible or even negative effects on forecasting accuracy, with some techniques like explicit Bayesian reasoning actually making predictions worse.",
      "x": -349.42584004628867,
      "y": -215.63530292055648,
      "size": 18.857142857142858
    },
    {
      "id": "Midjourney",
      "title": "Midjourney",
      "tags": [
        "organization"
      ],
      "content": "#organization \n\nhttps://www.midjourney.com\n\nThe Collective Intelligence team at Midjourney (currently stealth).",
      "x": 2062.822718578165,
      "y": 201.81415254647143,
      "size": 17.142857142857142
    },
    {
      "id": "Luke Hewitt",
      "title": "Luke Hewitt",
      "tags": [
        "person"
      ],
      "content": "#person",
      "x": -329.37185961867965,
      "y": -509.61354593330196,
      "size": 16.571428571428573
    },
    {
      "id": "Ted Suzman",
      "title": "Ted Suzman",
      "tags": [
        "person"
      ],
      "content": "#person",
      "x": 2095.8625924779726,
      "y": 794.9387059636165,
      "size": 17.142857142857142
    },
    {
      "id": "Avital Balwit",
      "title": "Avital Balwit",
      "tags": [
        "person"
      ],
      "content": "#person",
      "x": -1562.9896593507694,
      "y": 499.59978264995505,
      "size": 16.571428571428573
    },
    {
      "id": "Ozzie Gooen",
      "title": "Ozzie Gooen",
      "tags": [
        "person"
      ],
      "content": "#person",
      "x": -795.6299935195096,
      "y": -1201.5289699537964,
      "size": 18.285714285714285
    },
    {
      "id": "Georgina Evans",
      "title": "Georgina Evans",
      "tags": [
        "person"
      ],
      "content": "#person \n\nhttps://sites.google.com/view/georginaevans",
      "x": 1434.751021566787,
      "y": 500.4740088642572,
      "size": 16.571428571428573
    },
    {
      "id": "Ryan Lowe",
      "title": "Ryan Lowe",
      "tags": [
        "person"
      ],
      "content": "#person",
      "x": 1279.688024262066,
      "y": -362.91454807188126,
      "size": 17.142857142857142
    },
    {
      "id": "Nicky Case",
      "title": "Nicky Case",
      "tags": [
        "person"
      ],
      "content": "#person\n\nhttps://ncase.me/",
      "x": -2304.2625537355993,
      "y": 283.5827877537476,
      "size": 16.571428571428573
    },
    {
      "id": "Justin Stimatze",
      "title": "Justin Stimatze",
      "tags": [
        "person"
      ],
      "content": "#person",
      "x": 662.5538728575052,
      "y": -411.9857351861423,
      "size": 17.142857142857142
    },
    {
      "id": "Brett Hennig",
      "title": "Brett Hennig",
      "tags": [
        "person"
      ],
      "content": "#person",
      "x": 3804.366191394412,
      "y": 1180.9557534355997,
      "size": 16.571428571428573
    },
    {
      "id": "Nandika Donthi",
      "title": "Nandika Donthi",
      "tags": [
        "person"
      ],
      "content": "#person",
      "x": 1424.6119078058373,
      "y": 1043.8070404484067,
      "size": 17.142857142857142
    },
    {
      "id": "Jay Baxter",
      "title": "Jay Baxter",
      "tags": [
        "person"
      ],
      "content": "#person \n\nhttps://jaybaxter.net/",
      "x": 1293.9304234936697,
      "y": 292.7328541518838,
      "size": 17.142857142857142
    },
    {
      "id": "GitHub Next",
      "title": "GitHub Next",
      "tags": [
        "organization"
      ],
      "content": "#organization \n\nhttps://githubnext.com",
      "x": 2860.1705097321346,
      "y": -1944.5392066794366,
      "size": 16.571428571428573
    },
    {
      "id": "Elicit Tool",
      "title": "Elicit Tool",
      "tags": [
        "project"
      ],
      "content": "#project \n\nhttps://elicit.com/\n\nAI research assistant that helps users conduct literature reviews by searching academic papers, extracting key findings, and synthesizing research evidence to answer specific questions.",
      "x": -1934.779592795771,
      "y": 498.89241080967696,
      "size": 17.714285714285715
    },
    {
      "id": "Xixin Wu",
      "title": "Xixin Wu",
      "tags": [
        "person"
      ],
      "content": "#person",
      "x": 224.68590181083243,
      "y": 2755.5934482456846,
      "size": 16.571428571428573
    },
    {
      "id": "Language models can reduce asymmetry in information markets",
      "title": "Language models can reduce asymmetry in information markets",
      "tags": [
        "project"
      ],
      "content": "#project \n\nhttps://arxiv.org/pdf/2403.14443\n\nPaper presents the \"Information Bazaar,\" a simulated digital marketplace where AI agents powered by LLMs buy and sell information on behalf of principals. The system addresses the buyer's inspection paradox in information markets, where buyers need access to information to assess its value, but sellers need to limit access to prevent theft. The key innovation is that AI agents have dual capabilities: they can evaluate the quality of privileged information and can \"forget\" unpurchased content, allowing temporary inspection without unauthorized retention.",
      "x": 2431.1917960528526,
      "y": 110.73748944122963,
      "size": 20
    },
    {
      "id": "Plurality Institute",
      "title": "Plurality Institute",
      "tags": [
        "organization"
      ],
      "content": "#organization",
      "x": 1548.928220590021,
      "y": 1070.4694466931655,
      "size": 20
    },
    {
      "id": "AI Tools for Existential Security",
      "title": "AI Tools for Existential Security",
      "tags": [
        "project"
      ],
      "content": "#project \n\nhttps://www.forethought.org/research/ai-tools-for-existential-security\n\nPaper by Forethought that argues that rather than simply trying to slow down AI development, we should strategically accelerate the development of specific AI applications that help humanity navigate existential risks. \n\nThree key categories of beneficial AI tools: \n1. epistemic applications to help us anticipate and plan for emerging challenges \n2. coordination-enabling applications to help diverse groups work together towards shared goals\n3. risk-targeted applications to address specific challenges",
      "x": -1805.396984903366,
      "y": 381.93763369559025,
      "size": 17.142857142857142
    },
    {
      "id": "Renée DiResta",
      "title": "Renée DiResta",
      "tags": [
        "person"
      ],
      "content": "#person",
      "x": 1492.5712196708228,
      "y": 879.9340112496327,
      "size": 16.571428571428573
    },
    {
      "id": "Forecasting Tools",
      "title": "Forecasting Tools",
      "tags": [
        "project"
      ],
      "content": "#project \n\nhttps://github.com/Metaculus/forecasting-tools\n\nPython framework for building AI-powered forecasting systems that help humans predict future events more accurately.",
      "x": -267.1494960515555,
      "y": -872.7522764512429,
      "size": 17.142857142857142
    },
    {
      "id": "Karim Benyekhlef",
      "title": "Karim Benyekhlef",
      "tags": [
        "person"
      ],
      "content": "#person",
      "x": 2175.6970660772563,
      "y": -2341.4728709137075,
      "size": 16.571428571428573
    },
    {
      "id": "Yara Kyrychenko",
      "title": "Yara Kyrychenko",
      "tags": [
        "person"
      ],
      "content": "#person \n",
      "x": 649.6030223428628,
      "y": 759.0161123424392,
      "size": 16.571428571428573
    },
    {
      "id": "How large language models can reshape collective intelligence",
      "title": "How large language models can reshape collective intelligence",
      "tags": [
        "project"
      ],
      "content": "#project \n\nhttps://www.nature.com/articles/s41562-024-01959-9\n\nPaper examines how LLMs are reshaping collective intelligence (the ability of groups to solve problems better than individuals alone). The authors examine both the opportunities and risks that LLMs present for human collective reasoning and decision-making processes.",
      "x": 1121.8359764769725,
      "y": 957.7475405469911,
      "size": 32
    },
    {
      "id": "Mike Krieger",
      "title": "Mike Krieger",
      "tags": [
        "person"
      ],
      "content": "#person",
      "x": 3476.3057953048537,
      "y": 2527.430930765537,
      "size": 16.571428571428573
    },
    {
      "id": "Democratic Fine-Tuning",
      "title": "Democratic Fine-Tuning",
      "tags": [
        "project"
      ],
      "content": "#project \n\nhttps://www.meaningalignment.org/research/openai-dft-the-first-moral-graph\n\nLLM-based system creates \"moral graphs\" by using a two-stage process that aims to uncover shared values underlying political disagreement. A specialized chatbot engages participants in dialogue about contentious scenarios, asking for personal stories and role models to extract the underlying value behind responses rather than collecting ideological commitments like slogans or rules. The system then shows participants stories of people transitioning between different values and asks whether such transitions represent gains in wisdom, creating a graph structure where edges represent consensus about which values are more comprehensive than others\n\nFunded by [OpenAI Democratic Inputs to AI](https://github.com/openai/democratic-inputs).",
      "x": 1459.291986189716,
      "y": -19.522276055203207,
      "size": 18.285714285714285
    },
    {
      "id": "Michael Curry",
      "title": "Michael Curry",
      "tags": [
        "person"
      ],
      "content": "#person \n",
      "x": 1919.060025933745,
      "y": 17.24082010191114,
      "size": 16.571428571428573
    },
    {
      "id": "Carl Miller",
      "title": "Carl Miller",
      "tags": [
        "person"
      ],
      "content": "#person \n\nhttps://www.carlmiller.co/",
      "x": 1090.9638718162498,
      "y": 350.8267083816078,
      "size": 17.142857142857142
    },
    {
      "id": "Oded Adomi Leshem",
      "title": "Oded Adomi Leshem",
      "tags": [
        "person"
      ],
      "content": "#person",
      "x": 1648.5107050980628,
      "y": 653.2007948666912,
      "size": 16.571428571428573
    },
    {
      "id": "Evie Yu-Yen Cheng",
      "title": "Evie Yu-Yen Cheng",
      "tags": [
        "person"
      ],
      "content": "#person",
      "x": 1124.0392657656873,
      "y": 1807.0514608098467,
      "size": 16.571428571428573
    },
    {
      "id": "Ermis Soumalias",
      "title": "Ermis Soumalias",
      "tags": [
        "person"
      ],
      "content": "#person ",
      "x": 1863.5724705216303,
      "y": 3.587053537558891,
      "size": 16.571428571428573
    },
    {
      "id": "Generative AI Voting - Fair Collective Choice is Resilient to LLM Biases and Inconsistencies",
      "title": "Generative AI Voting - Fair Collective Choice is Resilient to LLM Biases and Inconsistencies",
      "tags": [
        "project"
      ],
      "content": "#project \n\nhttps://arxiv.org/pdf/2406.11871\n\nResearch studies how LLMs can serve as representatives for human voters in democratic processes like participatory budgeting and elections. The research demonstrates that AI systems can effectively represent abstaining voters.",
      "x": -2881.253012502264,
      "y": 1251.6391269224252,
      "size": 17.714285714285715
    },
    {
      "id": "Rai Sur",
      "title": "Rai Sur",
      "tags": [
        "person"
      ],
      "content": "#person \n\nhttps://rai.dev",
      "x": -1136.882572758444,
      "y": -1562.3371179774033,
      "size": 17.142857142857142
    },
    {
      "id": "Naomi Esther",
      "title": "Naomi Esther",
      "tags": [
        "person"
      ],
      "content": "#person",
      "x": 3858.534690820754,
      "y": 1165.3731749697106,
      "size": 16.571428571428573
    },
    {
      "id": "Research",
      "title": "Research",
      "tags": [
        "topic"
      ],
      "content": "#topic",
      "x": -507.4349535393232,
      "y": 795.513991809036,
      "size": 17.714285714285715
    },
    {
      "id": "Tantum Collins",
      "title": "Tantum Collins",
      "tags": [
        "person"
      ],
      "content": "#person",
      "x": 1398.8418936929143,
      "y": 456.0242909837236,
      "size": 16.571428571428573
    },
    {
      "id": "Bench to the Future - A Pastcasting Benchmark for Forecasting Agents",
      "title": "Bench to the Future - A Pastcasting Benchmark for Forecasting Agents",
      "tags": [
        "project"
      ],
      "content": "#project \n\nhttps://arxiv.org/abs/2506.21558\nhttps://evals.futuresearch.ai/\n\nEstablishes a \"pastcasting\" benchmark that evaluates AI forecasting capabilities by having LLMs predict outcomes of events that have already happened, using carefully curated web snapshots from before those events occurred. The benchmark addresses a key challenge in forecasting research - the long wait times between making predictions and learning outcomes.",
      "x": -731.0795440958706,
      "y": -611.1986198161612,
      "size": 21.142857142857142
    },
    {
      "id": "Multi-Agent Consensus Seeking via Large Language Models",
      "title": "Multi-Agent Consensus Seeking via Large Language Models",
      "tags": [
        "project"
      ],
      "content": "#project \n\nhttps://arxiv.org/abs/2310.20151\n\nPaper studies how multiple AI agents powered by large language models can reach consensus through negotiation, where each agent starts with a numerical value and must collectively agree on a final shared value. The research reveals that LLM agents naturally tend to use averaging strategies for consensus-seeking and that factors like agent personality (stubborn vs. suggestible), network topology, and group size significantly influence both the speed and outcome of negotiations.",
      "x": -1717.4694519754769,
      "y": 2825.164101417893,
      "size": 18.285714285714285
    },
    {
      "id": "Online Deliberation Platform",
      "title": "Online Deliberation Platform",
      "tags": [
        "project"
      ],
      "content": "#project\n\nhttps://stanforddeliberate.org/\n\nStanford Deliberate is an AI-assisted online video platform designed to facilitate structured small group discussions using deliberative polling methodology, with an automated moderator that manages speaking queues, timed agendas, and ensures equitable participation among participants. The platform has been used for large-scale national deliberative polling events across multiple countries and languages, allowing unlimited participants to deliberate simultaneously in small groups on policy issues.",
      "x": 779.8822167499907,
      "y": 1495.1953011555815,
      "size": 16.571428571428573
    },
    {
      "id": "How AI Agents Will Improve the Consultation Process",
      "title": "How AI Agents Will Improve the Consultation Process",
      "tags": [
        "project"
      ],
      "content": "#project \n\nhttps://ai.objectives.institute/blog/how-ai-agents-will-improve-consultation-process\n\nBlog post describes how AI agents can transform consultation processes by making them more interactive and engaging than traditional surveys. The AI Objectives Institute proposes using different types of AI agents: \n1. \"domain experts\" to help participants understand complex questions and organize their thoughts, \n2. \"professional interviewers\" to help refine and improve responses, and \n3. \"cross-pollination\" agents to help participants exchange ideas and find consensus by synthesizing diverse viewpoints.",
      "x": 655.732779664985,
      "y": -186.75619409967763,
      "size": 16.571428571428573
    },
    {
      "id": "Toby Shevlane",
      "title": "Toby Shevlane",
      "tags": [
        "person"
      ],
      "content": "#person",
      "x": -484.5355005677755,
      "y": -986.3923507962332,
      "size": 16.571428571428573
    },
    {
      "id": "Jorim Theuns",
      "title": "Jorim Theuns",
      "tags": [
        "person"
      ],
      "content": "#person \n\nhttps://jtheuns.com/",
      "x": 3740.476074878806,
      "y": 1048.2696899969626,
      "size": 17.142857142857142
    },
    {
      "id": "vTaiwan & g0v",
      "title": "vTaiwan & g0v",
      "tags": [
        "organization"
      ],
      "content": "#organization \n\nhttps://info.vtaiwan.tw/\nhttps://g0v.tw/",
      "x": 918.308649354897,
      "y": 190.9842796861607,
      "size": 18.285714285714285
    },
    {
      "id": "Google Jigsaw",
      "title": "Google Jigsaw",
      "tags": [
        "organization"
      ],
      "content": "#organization \n\nhttps://jigsaw.google.com/",
      "x": 1784.38820840888,
      "y": 1035.7563657120636,
      "size": 19.428571428571427
    },
    {
      "id": "Abdullah Almaatouq",
      "title": "Abdullah Almaatouq",
      "tags": [
        "person"
      ],
      "content": "#person",
      "x": 1095.1486546534297,
      "y": 1065.3037885143005,
      "size": 16.571428571428573
    },
    {
      "id": "Jungwon Byun",
      "title": "Jungwon Byun",
      "tags": [
        "person"
      ],
      "content": "#person",
      "x": -1943.1560321890236,
      "y": 443.664957671305,
      "size": 17.714285714285715
    },
    {
      "id": "Manuel Wüthrich",
      "title": "Manuel Wüthrich",
      "tags": [
        "person"
      ],
      "content": "#person",
      "x": 2220.4196169294496,
      "y": 182.2761367020348,
      "size": 17.714285714285715
    },
    {
      "id": "Richard Everett",
      "title": "Richard Everett",
      "tags": [
        "person"
      ],
      "content": "#person",
      "x": 1393.5068024527425,
      "y": 728.0613824973879,
      "size": 16.571428571428573
    },
    {
      "id": "Irwin King",
      "title": "Irwin King",
      "tags": [
        "person"
      ],
      "content": "#person",
      "x": 274.0512839825302,
      "y": 2728.537236792891,
      "size": 16.571428571428573
    },
    {
      "id": "Pattie Maes",
      "title": "Pattie Maes",
      "tags": [
        "person"
      ],
      "content": "#person",
      "x": 974.5602589949608,
      "y": -2755.20656340418,
      "size": 17.714285714285715
    },
    {
      "id": "Miruna Pîslar",
      "title": "Miruna Pîslar",
      "tags": [
        "person"
      ],
      "content": "#person",
      "x": 1093.7369048431676,
      "y": 640.9273476860209,
      "size": 16.571428571428573
    },
    {
      "id": "Hannes Westermann",
      "title": "Hannes Westermann",
      "tags": [
        "person"
      ],
      "content": "#person",
      "x": 2286.0759951959867,
      "y": -2301.378577681666,
      "size": 16.571428571428573
    },
    {
      "id": "Maria Antoniak",
      "title": "Maria Antoniak",
      "tags": [
        "person"
      ],
      "content": "#person",
      "x": 1217.8893859294753,
      "y": 1824.8730289520204,
      "size": 16.571428571428573
    },
    {
      "id": "Lucid Lens",
      "title": "Lucid Lens",
      "tags": [
        "project"
      ],
      "content": "#project \n\nhttps://ai.objectives.institute/blog/straightlines-surfaces-the-content-beneath-the-headline\nhttps://github.com/AIObjectives/lucidlens\n\nChrome extension that automatically rewrites sensational or misleading news headlines to more accurately reflect the actual content of articles, using AI to read and summarize the full text before generating clearer headlines.",
      "x": 689.1935988815694,
      "y": -503.81105078624074,
      "size": 17.714285714285715
    },
    {
      "id": "Ian Baker",
      "title": "Ian Baker",
      "tags": [
        "person"
      ],
      "content": "#person",
      "x": 1649.2806837888666,
      "y": 1143.6348851705998,
      "size": 17.142857142857142
    },
    {
      "id": "Minda Hu",
      "title": "Minda Hu",
      "tags": [
        "person"
      ],
      "content": "#person",
      "x": 306.67622969538365,
      "y": 2681.6244558357957,
      "size": 16.571428571428573
    },
    {
      "id": "Ralph Hertwig",
      "title": "Ralph Hertwig",
      "tags": [
        "person"
      ],
      "content": "#person",
      "x": 1066.70501145734,
      "y": 1140.9324477762468,
      "size": 16.571428571428573
    },
    {
      "id": "Dan Schwarz",
      "title": "Dan Schwarz",
      "tags": [
        "person"
      ],
      "content": "#person \n\nSome writing about [Google's internal prediction markets](https://asteriskmag.com/issues/08/the-death-and-life-of-prediction-markets-at-google).",
      "x": -831.4781389544952,
      "y": -708.941876393585,
      "size": 18.285714285714285
    },
    {
      "id": "Ben Turtel",
      "title": "Ben Turtel",
      "tags": [
        "person"
      ],
      "content": "#person",
      "x": -483.86316913010216,
      "y": -469.8714481131538,
      "size": 17.714285714285715
    },
    {
      "id": "Matthew Spaniol",
      "title": "Matthew Spaniol",
      "tags": [
        "person"
      ],
      "content": "#person",
      "x": 3071.5915325379115,
      "y": -1259.2752845373063,
      "size": 16.571428571428573
    },
    {
      "id": "Lucy Campbell-Gillingham",
      "title": "Lucy Campbell-Gillingham",
      "tags": [
        "person"
      ],
      "content": "#person",
      "x": 1356.6725194125038,
      "y": 620.0271145323594,
      "size": 17.714285714285715
    },
    {
      "id": "Deliberative Democracy Lab",
      "title": "Deliberative Democracy Lab",
      "tags": [
        "organization"
      ],
      "content": "#organization \n\nhttps://deliberation.stanford.edu/",
      "x": 847.3404089335676,
      "y": 1390.6348583925135,
      "size": 17.142857142857142
    },
    {
      "id": "Robert Thomson",
      "title": "Robert Thomson",
      "tags": [
        "person"
      ],
      "content": "#person",
      "x": -2233.1042616917634,
      "y": 2330.851379562573,
      "size": 16.571428571428573
    },
    {
      "id": "Lufeng Xu",
      "title": "Lufeng Xu",
      "tags": [
        "person"
      ],
      "content": "#person",
      "x": -1757.090913567145,
      "y": 2879.0580800670987,
      "size": 16.571428571428573
    },
    {
      "id": "Houtan Bastani",
      "title": "Houtan Bastani",
      "tags": [
        "person"
      ],
      "content": "#person",
      "x": -167.68177451662453,
      "y": 10.18398720815008,
      "size": 17.142857142857142
    },
    {
      "id": "Charlie George",
      "title": "Charlie George",
      "tags": [
        "person"
      ],
      "content": "#person",
      "x": -1942.725667713989,
      "y": 384.2379487239744,
      "size": 17.714285714285715
    },
    {
      "id": "Daniel Hnyk",
      "title": "Daniel Hnyk",
      "tags": [
        "person"
      ],
      "content": "#person",
      "x": -788.7242426021298,
      "y": -636.2606637787694,
      "size": 17.714285714285715
    },
    {
      "id": "Divya Siddarth",
      "title": "Divya Siddarth",
      "tags": [
        "person"
      ],
      "content": "#person \n\nhttps://divyasiddarth.com/",
      "x": 1251.3647810817142,
      "y": 910.2533943640935,
      "size": 17.714285714285715
    },
    {
      "id": "Edwin Lock",
      "title": "Edwin Lock",
      "tags": [
        "person"
      ],
      "content": "#person \n",
      "x": 1933.7343227307829,
      "y": 127.96908875356733,
      "size": 16.571428571428573
    },
    {
      "id": "Zhou Yu",
      "title": "Zhou Yu",
      "tags": [
        "person"
      ],
      "content": "#person",
      "x": 3984.841503068037,
      "y": 292.4789197274165,
      "size": 16.571428571428573
    },
    {
      "id": "Andrea Baronchelli",
      "title": "Andrea Baronchelli",
      "tags": [
        "person"
      ],
      "content": "#person \n",
      "x": 645.0897059621471,
      "y": 815.980439445282,
      "size": 16.571428571428573
    },
    {
      "id": "Natasha Jensen",
      "title": "Natasha Jensen",
      "tags": [
        "person"
      ],
      "content": "#person",
      "x": 692.9581003769802,
      "y": -362.25848907181665,
      "size": 17.142857142857142
    },
    {
      "id": "Don't Just Tell Me, Ask Me",
      "title": "Don't Just Tell Me, Ask Me",
      "tags": [
        "project"
      ],
      "content": "#project \n\nhttps://dl.acm.org/doi/pdf/10.1145/3544548.3580672\n\nResearch paper presents \"AI-framed Questioning\" - a method where AI systems ask users strategic questions instead of directly telling them answers, designed to improve human critical thinking and logical reasoning. The study found that when people were asked questions like \"If (premise), does it follow that (conclusion)?\" they performed significantly better at identifying logical fallacies in socially divisive statements compared to both receiving direct AI explanations or no assistance at all.",
      "x": 1065.3486294410825,
      "y": -2781.468046533273,
      "size": 18.285714285714285
    },
    {
      "id": "Chandler Smith",
      "title": "Chandler Smith",
      "tags": [
        "person"
      ],
      "content": "#person",
      "x": -2025.9187936303938,
      "y": -1431.7380100541798,
      "size": 16.571428571428573
    },
    {
      "id": "Rafael Valdece Sousa Bastos",
      "title": "Rafael Valdece Sousa Bastos",
      "tags": [
        "person"
      ],
      "content": "#person",
      "x": -367.3590451040928,
      "y": -272.2935311953073,
      "size": 16.571428571428573
    },
    {
      "id": "Barbara Mellers",
      "title": "Barbara Mellers",
      "tags": [
        "person"
      ],
      "content": "#person",
      "x": -419.8277199249365,
      "y": -166.58382412067013,
      "size": 16.571428571428573
    },
    {
      "id": "Topos Institute",
      "title": "Topos Institute",
      "tags": [
        "organization"
      ],
      "content": "#organization \n\nhttps://topos.institute",
      "x": -2538.395358238742,
      "y": 1913.1175309997868,
      "size": 16.571428571428573
    },
    {
      "id": "Yaoli Mao",
      "title": "Yaoli Mao",
      "tags": [
        "person"
      ],
      "content": "#person",
      "x": 1013.6770029340609,
      "y": -2810.870854276536,
      "size": 17.142857142857142
    },
    {
      "id": "Hélène Landemore",
      "title": "Hélène Landemore",
      "tags": [
        "person"
      ],
      "content": "#person\n\nhttps://www.helenelandemore.com/",
      "x": 1482.5429370856566,
      "y": 1037.3892381314145,
      "size": 17.142857142857142
    },
    {
      "id": "Michael Henry Tessler",
      "title": "Michael Henry Tessler",
      "tags": [
        "person"
      ],
      "content": "#person\n\nhttps://www.mit.edu/~tessler/",
      "x": 1312.71187781098,
      "y": 660.8616297480344,
      "size": 18.285714285714285
    },
    {
      "id": "Rob Gordon",
      "title": "Rob Gordon",
      "tags": [
        "person"
      ],
      "content": "#person",
      "x": -580.8594683434645,
      "y": -938.8235871374885,
      "size": 16.571428571428573
    },
    {
      "id": "Ben Day",
      "title": "Ben Day",
      "tags": [
        "person"
      ],
      "content": "#person",
      "x": -427.6532491184252,
      "y": -980.951001789479,
      "size": 16.571428571428573
    },
    {
      "id": "Pantheon",
      "title": "Pantheon",
      "tags": [
        "project"
      ],
      "content": "#project \n\nhttps://pantheon.chat/\n\nExperimental AI tool that reverses the typical human-AI interaction pattern by having LLM-powered \"daemons\" proactively respond to and comment on a user's stream of consciousness writing, rather than waiting for human prompts. The system allows users to write their thoughts in a diary-like format while customizable AI advisors offer questions, insights, and challenges designed to stimulate deeper thinking and explore new directions.",
      "x": 3613.1899275864844,
      "y": -1005.5420987577774,
      "size": 17.142857142857142
    },
    {
      "id": "Ran Haase",
      "title": "Ran Haase",
      "tags": [
        "person"
      ],
      "content": "#person",
      "x": 3785.769922012224,
      "y": 1234.0769184513977,
      "size": 16.571428571428573
    },
    {
      "id": "Saffron Huang",
      "title": "Saffron Huang",
      "tags": [
        "person"
      ],
      "content": "#person",
      "x": 1138.196955949223,
      "y": 884.9737635766728,
      "size": 17.142857142857142
    },
    {
      "id": "AI & Democracy Foundation",
      "title": "AI & Democracy Foundation",
      "tags": [
        "organization"
      ],
      "content": "#organization \n\nhttps://aidemocracyfoundation.org/",
      "x": 1666.8768486198028,
      "y": 789.5155954205912,
      "size": 17.714285714285715
    },
    {
      "id": "Chen Yueh-Han",
      "title": "Chen Yueh-Han",
      "tags": [
        "person"
      ],
      "content": "#person \n\nhttps://john-chen.cc/",
      "x": -339.6355168567282,
      "y": 134.8839087724612,
      "size": 17.142857142857142
    },
    {
      "id": "Taha Yasseri",
      "title": "Taha Yasseri",
      "tags": [
        "person"
      ],
      "content": "#person",
      "x": 1151.452256598522,
      "y": 1075.0613355615217,
      "size": 16.571428571428573
    },
    {
      "id": "Soham De",
      "title": "Soham De",
      "tags": [
        "person"
      ],
      "content": "#person",
      "x": 1238.7976786174017,
      "y": 309.8007191828333,
      "size": 16.571428571428573
    },
    {
      "id": "Beth Goldberg",
      "title": "Beth Goldberg",
      "tags": [
        "person"
      ],
      "content": "#person",
      "x": 1614.3717603909563,
      "y": 961.5912998628878,
      "size": 17.142857142857142
    },
    {
      "id": "Peter Darche",
      "title": "Peter Darche",
      "tags": [
        "person"
      ],
      "content": "#person",
      "x": 1703.5768193864037,
      "y": 1164.827931289991,
      "size": 17.142857142857142
    },
    {
      "id": "Ming Yin",
      "title": "Ming Yin",
      "tags": [
        "person"
      ],
      "content": "#person",
      "x": 1666.6795754578968,
      "y": 3651.4470932814456,
      "size": 16.571428571428573
    }
  ],
  "links": [
    {
      "source": {
        "id": "Mapping the Discourse on AI Safety & Ethics",
        "title": "Mapping the Discourse on AI Safety & Ethics",
        "tags": [
          "project"
        ],
        "content": "#project \n\nhttps://ai.objectives.institute/blog/mapping-the-discourse-on-ai-safety-amp-ethics\n\nProject by the AI Objectives Institute that used their \"Talk to the City\" tool to analyze and map Twitter conversations about AI safety and ethics, identifying six distinct perspectives ranging from near-term harms to long-term existential risks. The tool automatically clustered thousands of tweets to reveal different viewpoints on AI governance, safety approaches, and development concerns, finding significant overlap between groups despite their different priorities.",
        "size": 18.285714285714285,
        "index": 0,
        "x": 788.547473702122,
        "y": -380.61239477665157,
        "vy": -1.1528366322432453,
        "vx": -0.09430758140445046
      },
      "target": {
        "id": "Değer Turan",
        "title": "Değer Turan",
        "tags": [
          "person"
        ],
        "content": "#person",
        "size": 18.285714285714285,
        "index": 107,
        "x": 371.54679486309595,
        "y": -417.8764113264942,
        "vy": -1.0219808978726659,
        "vx": -0.10788831074881157
      },
      "index": 0
    },
    {
      "source": {
        "id": "Mapping the Discourse on AI Safety & Ethics",
        "title": "Mapping the Discourse on AI Safety & Ethics",
        "tags": [
          "project"
        ],
        "content": "#project \n\nhttps://ai.objectives.institute/blog/mapping-the-discourse-on-ai-safety-amp-ethics\n\nProject by the AI Objectives Institute that used their \"Talk to the City\" tool to analyze and map Twitter conversations about AI safety and ethics, identifying six distinct perspectives ranging from near-term harms to long-term existential risks. The tool automatically clustered thousands of tweets to reveal different viewpoints on AI governance, safety approaches, and development concerns, finding significant overlap between groups despite their different priorities.",
        "size": 18.285714285714285,
        "index": 0,
        "x": 788.547473702122,
        "y": -380.61239477665157,
        "vy": -1.1528366322432453,
        "vx": -0.09430758140445046
      },
      "target": {
        "id": "Colleen McKenzie",
        "title": "Colleen McKenzie",
        "tags": [
          "person"
        ],
        "content": "#person",
        "size": 17.714285714285715,
        "index": 22,
        "x": 615.6457683620001,
        "y": -374.6339202786093,
        "vy": -1.1097477536713236,
        "vx": 0.021596753844054135
      },
      "index": 1
    },
    {
      "source": {
        "id": "Mapping the Discourse on AI Safety & Ethics",
        "title": "Mapping the Discourse on AI Safety & Ethics",
        "tags": [
          "project"
        ],
        "content": "#project \n\nhttps://ai.objectives.institute/blog/mapping-the-discourse-on-ai-safety-amp-ethics\n\nProject by the AI Objectives Institute that used their \"Talk to the City\" tool to analyze and map Twitter conversations about AI safety and ethics, identifying six distinct perspectives ranging from near-term harms to long-term existential risks. The tool automatically clustered thousands of tweets to reveal different viewpoints on AI governance, safety approaches, and development concerns, finding significant overlap between groups despite their different priorities.",
        "size": 18.285714285714285,
        "index": 0,
        "x": 788.547473702122,
        "y": -380.61239477665157,
        "vy": -1.1528366322432453,
        "vx": -0.09430758140445046
      },
      "target": {
        "id": "Oliver Klingefjord",
        "title": "Oliver Klingefjord",
        "tags": [
          "person"
        ],
        "content": "#person \n\nhttps://www.klingefjord.com/",
        "size": 18.285714285714285,
        "index": 44,
        "x": 1178.5910932786057,
        "y": -246.8472025448904,
        "vy": -1.2893218733044343,
        "vx": -0.05174057759054609
      },
      "index": 2
    },
    {
      "source": {
        "id": "ACE - A LLM-based Negotiation Coaching System",
        "title": "ACE - A LLM-based Negotiation Coaching System",
        "tags": [
          "project"
        ],
        "content": "#project \n\nhttps://arxiv.org/pdf/2410.01555\n\nLLM-based system that provides targeted feedback to help users improve their bargaining skills. The system identifies specific negotiation mistakes using an annotation scheme developed with [MBA](https://en.wikipedia.org/wiki/Master_of_Business_Administration) instructors, then provides detailed feedback on preparation strategies (like setting appropriate target prices) and in-conversation tactics (like including rationales with offers and strategic closing behavior). Through a controlled experiment with 374 participants across two negotiation rounds, the researchers demonstrated that users who received ACE's coaching significantly improved their objective negotiation outcomes compared to control groups.",
        "size": 18.857142857142858,
        "index": 2,
        "x": 4041.6678647452236,
        "y": 253.4329420771978,
        "vy": -1.5745194533774267,
        "vx": 0.3099246537279193
      },
      "target": {
        "id": "Ryan Shea",
        "title": "Ryan Shea",
        "tags": [
          "person"
        ],
        "content": "#person",
        "size": 16.571428571428573,
        "index": 31,
        "x": 3988.715238567769,
        "y": 207.99283635935524,
        "vy": -1.5899036766371577,
        "vx": 0.27096755673715656
      },
      "index": 3
    },
    {
      "source": {
        "id": "ACE - A LLM-based Negotiation Coaching System",
        "title": "ACE - A LLM-based Negotiation Coaching System",
        "tags": [
          "project"
        ],
        "content": "#project \n\nhttps://arxiv.org/pdf/2410.01555\n\nLLM-based system that provides targeted feedback to help users improve their bargaining skills. The system identifies specific negotiation mistakes using an annotation scheme developed with [MBA](https://en.wikipedia.org/wiki/Master_of_Business_Administration) instructors, then provides detailed feedback on preparation strategies (like setting appropriate target prices) and in-conversation tactics (like including rationales with offers and strategic closing behavior). Through a controlled experiment with 374 participants across two negotiation rounds, the researchers demonstrated that users who received ACE's coaching significantly improved their objective negotiation outcomes compared to control groups.",
        "size": 18.857142857142858,
        "index": 2,
        "x": 4041.6678647452236,
        "y": 253.4329420771978,
        "vy": -1.5745194533774267,
        "vx": 0.3099246537279193
      },
      "target": {
        "id": "Aymen Kallala",
        "title": "Aymen Kallala",
        "tags": [
          "person"
        ],
        "content": "#person",
        "size": 16.571428571428573,
        "index": 112,
        "x": 4109.578091822254,
        "y": 260.9788036619757,
        "vy": -1.4370505842623715,
        "vx": 0.3150705579507164
      },
      "index": 4
    },
    {
      "source": {
        "id": "ACE - A LLM-based Negotiation Coaching System",
        "title": "ACE - A LLM-based Negotiation Coaching System",
        "tags": [
          "project"
        ],
        "content": "#project \n\nhttps://arxiv.org/pdf/2410.01555\n\nLLM-based system that provides targeted feedback to help users improve their bargaining skills. The system identifies specific negotiation mistakes using an annotation scheme developed with [MBA](https://en.wikipedia.org/wiki/Master_of_Business_Administration) instructors, then provides detailed feedback on preparation strategies (like setting appropriate target prices) and in-conversation tactics (like including rationales with offers and strategic closing behavior). Through a controlled experiment with 374 participants across two negotiation rounds, the researchers demonstrated that users who received ACE's coaching significantly improved their objective negotiation outcomes compared to control groups.",
        "size": 18.857142857142858,
        "index": 2,
        "x": 4041.6678647452236,
        "y": 253.4329420771978,
        "vy": -1.5745194533774267,
        "vx": 0.3099246537279193
      },
      "target": {
        "id": "Xin Lucy Liu",
        "title": "Xin Lucy Liu",
        "tags": [
          "person"
        ],
        "content": "#person",
        "size": 16.571428571428573,
        "index": 249,
        "x": 4068.3870882762517,
        "y": 188.17854393152223,
        "vy": -1.5683780549984672,
        "vx": 0.5935149370623758
      },
      "index": 5
    },
    {
      "source": {
        "id": "ACE - A LLM-based Negotiation Coaching System",
        "title": "ACE - A LLM-based Negotiation Coaching System",
        "tags": [
          "project"
        ],
        "content": "#project \n\nhttps://arxiv.org/pdf/2410.01555\n\nLLM-based system that provides targeted feedback to help users improve their bargaining skills. The system identifies specific negotiation mistakes using an annotation scheme developed with [MBA](https://en.wikipedia.org/wiki/Master_of_Business_Administration) instructors, then provides detailed feedback on preparation strategies (like setting appropriate target prices) and in-conversation tactics (like including rationales with offers and strategic closing behavior). Through a controlled experiment with 374 participants across two negotiation rounds, the researchers demonstrated that users who received ACE's coaching significantly improved their objective negotiation outcomes compared to control groups.",
        "size": 18.857142857142858,
        "index": 2,
        "x": 4041.6678647452236,
        "y": 253.4329420771978,
        "vy": -1.5745194533774267,
        "vx": 0.3099246537279193
      },
      "target": {
        "id": "Michael W. Morris",
        "title": "Michael W. Morris",
        "tags": [
          "person"
        ],
        "content": "#person",
        "size": 16.571428571428573,
        "index": 66,
        "x": 4062.862764169195,
        "y": 318.719315877871,
        "vy": -1.5988646424189619,
        "vx": 0.4552960218590389
      },
      "index": 6
    },
    {
      "source": {
        "id": "ACE - A LLM-based Negotiation Coaching System",
        "title": "ACE - A LLM-based Negotiation Coaching System",
        "tags": [
          "project"
        ],
        "content": "#project \n\nhttps://arxiv.org/pdf/2410.01555\n\nLLM-based system that provides targeted feedback to help users improve their bargaining skills. The system identifies specific negotiation mistakes using an annotation scheme developed with [MBA](https://en.wikipedia.org/wiki/Master_of_Business_Administration) instructors, then provides detailed feedback on preparation strategies (like setting appropriate target prices) and in-conversation tactics (like including rationales with offers and strategic closing behavior). Through a controlled experiment with 374 participants across two negotiation rounds, the researchers demonstrated that users who received ACE's coaching significantly improved their objective negotiation outcomes compared to control groups.",
        "size": 18.857142857142858,
        "index": 2,
        "x": 4041.6678647452236,
        "y": 253.4329420771978,
        "vy": -1.5745194533774267,
        "vx": 0.3099246537279193
      },
      "target": {
        "id": "Zhou Yu",
        "title": "Zhou Yu",
        "tags": [
          "person"
        ],
        "content": "#person",
        "size": 16.571428571428573,
        "index": 416,
        "x": 3984.841503068037,
        "y": 292.4789197274165,
        "vy": -1.2533227744496291,
        "vx": 0.3964187696043023
      },
      "index": 7
    },
    {
      "source": {
        "id": "Amina Green",
        "title": "Amina Green",
        "tags": [
          "person"
        ],
        "content": "#person \n\nhttps://www.amina-io.com/",
        "size": 17.142857142857142,
        "index": 9,
        "x": 1708.848852745666,
        "y": 1220.2859801318612,
        "vy": -1.666710205089688,
        "vx": 0.5825834175904476
      },
      "target": {
        "id": "Plurality Institute",
        "title": "Plurality Institute",
        "tags": [
          "organization"
        ],
        "content": "#organization",
        "size": 20,
        "index": 364,
        "x": 1548.928220590021,
        "y": 1070.4694466931655,
        "vy": -1.5138983339043548,
        "vx": 0.41914174319685277
      },
      "index": 8
    },
    {
      "source": {
        "id": "Maggie Appleton",
        "title": "Maggie Appleton",
        "tags": [
          "person"
        ],
        "content": "#person \n\nhttps://maggieappleton.com/",
        "size": 17.714285714285715,
        "index": 13,
        "x": -1986.0378873394982,
        "y": 345.1972131135029,
        "vy": -0.20291033946933473,
        "vx": 0.19098062665201015
      },
      "target": {
        "id": "Elicit",
        "title": "Elicit",
        "tags": [
          "organization"
        ],
        "content": "#organization \n\nhttps://elicit.com/",
        "size": 20.57142857142857,
        "index": 218,
        "x": -2027.2873240845167,
        "y": 375.96539772345454,
        "vy": -0.29711133317878174,
        "vx": 0.26985144319061544
      },
      "index": 9
    },
    {
      "source": {
        "id": "Maggie Appleton",
        "title": "Maggie Appleton",
        "tags": [
          "person"
        ],
        "content": "#person \n\nhttps://maggieappleton.com/",
        "size": 17.714285714285715,
        "index": 13,
        "x": -1986.0378873394982,
        "y": 345.1972131135029,
        "vy": -0.20291033946933473,
        "vx": 0.19098062665201015
      },
      "target": {
        "id": "Ought",
        "title": "Ought",
        "tags": [
          "organization"
        ],
        "content": "#organization \n\nhttps://ought.org/ (website is old)\n\nOught 1.0 has effectively become what is now Elicit (the company) continuing development on Elicit (the tool). Ought 2.0 has not yet done any public work.",
        "size": 23.42857142857143,
        "index": 306,
        "x": -1888.3722954977052,
        "y": 453.0511056583274,
        "vy": -0.2547035865836596,
        "vx": 0.0471838457037637
      },
      "index": 10
    },
    {
      "source": {
        "id": "Wargames for Peace",
        "title": "Wargames for Peace",
        "tags": [
          "project"
        ],
        "content": "#project \n\nhttps://www.youtube.com/watch?v=9_zSzjTWr8A\n\n[MATS](https://www.matsprogram.org/) project to simulate  a tabletop exercises with LLMs, allowing a human to play solo.",
        "size": 18.285714285714285,
        "index": 15,
        "x": -2216.0206491589165,
        "y": 317.47789193596424,
        "vy": -0.26235631740140714,
        "vx": 0.11999156457905648
      },
      "target": {
        "id": "Nicky Case",
        "title": "Nicky Case",
        "tags": [
          "person"
        ],
        "content": "#person\n\nhttps://ncase.me/",
        "size": 16.571428571428573,
        "index": 355,
        "x": -2304.2625537355993,
        "y": 283.5827877537476,
        "vy": -0.27745848041721,
        "vx": 0.1505069827812786
      },
      "index": 11
    },
    {
      "source": {
        "id": "Wargames for Peace",
        "title": "Wargames for Peace",
        "tags": [
          "project"
        ],
        "content": "#project \n\nhttps://www.youtube.com/watch?v=9_zSzjTWr8A\n\n[MATS](https://www.matsprogram.org/) project to simulate  a tabletop exercises with LLMs, allowing a human to play solo.",
        "size": 18.285714285714285,
        "index": 15,
        "x": -2216.0206491589165,
        "y": 317.47789193596424,
        "vy": -0.26235631740140714,
        "vx": 0.11999156457905648
      },
      "target": {
        "id": "Eli Lifland",
        "title": "Eli Lifland",
        "tags": [
          "person"
        ],
        "content": "#person \n\nhttps://www.elilifland.com/",
        "size": 17.714285714285715,
        "index": 233,
        "x": -2088.797171070193,
        "y": 462.7401955014765,
        "vy": -0.3502859520662234,
        "vx": 0.08033116490001913
      },
      "index": 12
    },
    {
      "source": {
        "id": "Wargames for Peace",
        "title": "Wargames for Peace",
        "tags": [
          "project"
        ],
        "content": "#project \n\nhttps://www.youtube.com/watch?v=9_zSzjTWr8A\n\n[MATS](https://www.matsprogram.org/) project to simulate  a tabletop exercises with LLMs, allowing a human to play solo.",
        "size": 18.285714285714285,
        "index": 15,
        "x": -2216.0206491589165,
        "y": 317.47789193596424,
        "vy": -0.26235631740140714,
        "vx": 0.11999156457905648
      },
      "target": {
        "id": "Daniel Kokotajlo",
        "title": "Daniel Kokotajlo",
        "tags": [
          "person"
        ],
        "content": "#person",
        "size": 17.142857142857142,
        "index": 156,
        "x": -2287.9333745974973,
        "y": 382.44662513897083,
        "vy": -0.2659387512272784,
        "vx": 0.14156672468047232
      },
      "index": 13
    },
    {
      "source": {
        "id": "Evelien Nieuwenburg",
        "title": "Evelien Nieuwenburg",
        "tags": [
          "person"
        ],
        "content": "#person",
        "size": 17.142857142857142,
        "index": 16,
        "x": 3697.499433125433,
        "y": 1087.6426664248283,
        "vy": -1.9305523529355046,
        "vx": 0.4105224081999784
      },
      "target": {
        "id": "Dembrane",
        "title": "Dembrane",
        "tags": [
          "organization"
        ],
        "content": "#organization \n\nhttps://www.dembrane.com\n\nFunded (in part) by the European Union via the [AI4Deliberation](https://www.ai4dproject.eu/) project.",
        "size": 17.714285714285715,
        "index": 336,
        "x": 3670.5319699200368,
        "y": 1017.8958794711315,
        "vy": -1.8573349240868908,
        "vx": 0.2592679715006362
      },
      "index": 14
    },
    {
      "source": {
        "id": "Enhancing AI-Assisted Group Decision Making through LLM-Powered Devil’s Advocate",
        "title": "Enhancing AI-Assisted Group Decision Making through LLM-Powered Devil’s Advocate",
        "tags": [
          "project"
        ],
        "content": "#project \n\nhttps://dl.acm.org/doi/pdf/10.1145/3640543.3645199\n\nStudy tested \"LLM-Powered Devil's Advocate\" systems that use GPT-3.5 to challenge group decisions in AI-assisted tasks. Researchers ran a randomized experiment where groups made recidivism risk predictions with AI help, and some groups also had an LLM devil's advocate that either questioned the AI's recommendations or challenged the group's majority opinion. The devil's advocate used critique questions and comments to prompt deeper thinking. The key finding was that questioning AI recommendations significantly improved group accuracy, while questioning group opinions provided no benefit.",
        "size": 18.285714285714285,
        "index": 17,
        "x": 1644.2006929316392,
        "y": 3713.969999573129,
        "vy": -0.8817846289843579,
        "vx": -0.5113607044055982
      },
      "target": {
        "id": "Chun-Wei Chiang",
        "title": "Chun-Wei Chiang",
        "tags": [
          "person"
        ],
        "content": "#person",
        "size": 16.571428571428573,
        "index": 131,
        "x": 1578.4838178652146,
        "y": 3697.1683523660167,
        "vy": -0.9184383874887452,
        "vx": -0.5010520676625263
      },
      "index": 15
    },
    {
      "source": {
        "id": "Enhancing AI-Assisted Group Decision Making through LLM-Powered Devil’s Advocate",
        "title": "Enhancing AI-Assisted Group Decision Making through LLM-Powered Devil’s Advocate",
        "tags": [
          "project"
        ],
        "content": "#project \n\nhttps://dl.acm.org/doi/pdf/10.1145/3640543.3645199\n\nStudy tested \"LLM-Powered Devil's Advocate\" systems that use GPT-3.5 to challenge group decisions in AI-assisted tasks. Researchers ran a randomized experiment where groups made recidivism risk predictions with AI help, and some groups also had an LLM devil's advocate that either questioned the AI's recommendations or challenged the group's majority opinion. The devil's advocate used critique questions and comments to prompt deeper thinking. The key finding was that questioning AI recommendations significantly improved group accuracy, while questioning group opinions provided no benefit.",
        "size": 18.285714285714285,
        "index": 17,
        "x": 1644.2006929316392,
        "y": 3713.969999573129,
        "vy": -0.8817846289843579,
        "vx": -0.5113607044055982
      },
      "target": {
        "id": "Zhuoran Lu",
        "title": "Zhuoran Lu",
        "tags": [
          "person"
        ],
        "content": "#person",
        "size": 16.571428571428573,
        "index": 297,
        "x": 1619.8499804346925,
        "y": 3776.7969245538343,
        "vy": -0.8953201256663064,
        "vx": -0.5489125809389889
      },
      "index": 16
    },
    {
      "source": {
        "id": "Enhancing AI-Assisted Group Decision Making through LLM-Powered Devil’s Advocate",
        "title": "Enhancing AI-Assisted Group Decision Making through LLM-Powered Devil’s Advocate",
        "tags": [
          "project"
        ],
        "content": "#project \n\nhttps://dl.acm.org/doi/pdf/10.1145/3640543.3645199\n\nStudy tested \"LLM-Powered Devil's Advocate\" systems that use GPT-3.5 to challenge group decisions in AI-assisted tasks. Researchers ran a randomized experiment where groups made recidivism risk predictions with AI help, and some groups also had an LLM devil's advocate that either questioned the AI's recommendations or challenged the group's majority opinion. The devil's advocate used critique questions and comments to prompt deeper thinking. The key finding was that questioning AI recommendations significantly improved group accuracy, while questioning group opinions provided no benefit.",
        "size": 18.285714285714285,
        "index": 17,
        "x": 1644.2006929316392,
        "y": 3713.969999573129,
        "vy": -0.8817846289843579,
        "vx": -0.5113607044055982
      },
      "target": {
        "id": "Zhuoyan Li",
        "title": "Zhuoyan Li",
        "tags": [
          "person"
        ],
        "content": "#person",
        "size": 16.571428571428573,
        "index": 111,
        "x": 1708.5747319508507,
        "y": 3732.0690944936523,
        "vy": -0.8451071703820716,
        "vx": -0.5223485985878208
      },
      "index": 17
    },
    {
      "source": {
        "id": "Enhancing AI-Assisted Group Decision Making through LLM-Powered Devil’s Advocate",
        "title": "Enhancing AI-Assisted Group Decision Making through LLM-Powered Devil’s Advocate",
        "tags": [
          "project"
        ],
        "content": "#project \n\nhttps://dl.acm.org/doi/pdf/10.1145/3640543.3645199\n\nStudy tested \"LLM-Powered Devil's Advocate\" systems that use GPT-3.5 to challenge group decisions in AI-assisted tasks. Researchers ran a randomized experiment where groups made recidivism risk predictions with AI help, and some groups also had an LLM devil's advocate that either questioned the AI's recommendations or challenged the group's majority opinion. The devil's advocate used critique questions and comments to prompt deeper thinking. The key finding was that questioning AI recommendations significantly improved group accuracy, while questioning group opinions provided no benefit.",
        "size": 18.285714285714285,
        "index": 17,
        "x": 1644.2006929316392,
        "y": 3713.969999573129,
        "vy": -0.8817846289843579,
        "vx": -0.5113607044055982
      },
      "target": {
        "id": "Ming Yin",
        "title": "Ming Yin",
        "tags": [
          "person"
        ],
        "content": "#person",
        "size": 16.571428571428573,
        "index": 438,
        "x": 1666.6795754578968,
        "y": 3651.4470932814456,
        "vy": -0.8691529790791935,
        "vx": -0.47317811923734343
      },
      "index": 18
    },
    {
      "source": {
        "id": "Squiggle AI",
        "title": "Squiggle AI",
        "tags": [
          "project"
        ],
        "content": "#project\n\nhttps://www.lesswrong.com/posts/7worWgggeHL3Eb7wq/introducing-squiggle-ai\n\nTool that automatically generates probabilistic cost-effectiveness models using the Squiggle programming language, allowing users to quickly create Fermi estimates and decision analyses without needing to learn programming. It combines large language models with specialized scaffolding to produce structured, adjustable models for questions like comparing charitable interventions or evaluating policy decisions.",
        "size": 17.714285714285715,
        "index": 19,
        "x": -658.0983208874798,
        "y": -987.2815180985507,
        "vy": -0.45320317580470043,
        "vx": -0.35287377532763314
      },
      "target": {
        "id": "AI Forecasting Benchmark",
        "title": "AI Forecasting Benchmark",
        "tags": [
          "project"
        ],
        "content": "#project \n\nhttps://www.metaculus.com/aib/\n\nMetaculus AIB (AI Benchmarking) is a series that benchmarks \"the state of the art in AI forecasting against the best humans on real-world questions.\"",
        "size": 20.57142857142857,
        "index": 178,
        "x": -491.69904780690194,
        "y": -761.3030318136879,
        "vy": -0.5357053744666,
        "vx": -0.2989940012828504
      },
      "index": 19
    },
    {
      "source": {
        "id": "Squiggle AI",
        "title": "Squiggle AI",
        "tags": [
          "project"
        ],
        "content": "#project\n\nhttps://www.lesswrong.com/posts/7worWgggeHL3Eb7wq/introducing-squiggle-ai\n\nTool that automatically generates probabilistic cost-effectiveness models using the Squiggle programming language, allowing users to quickly create Fermi estimates and decision analyses without needing to learn programming. It combines large language models with specialized scaffolding to produce structured, adjustable models for questions like comparing charitable interventions or evaluating policy decisions.",
        "size": 17.714285714285715,
        "index": 19,
        "x": -658.0983208874798,
        "y": -987.2815180985507,
        "vy": -0.45320317580470043,
        "vx": -0.35287377532763314
      },
      "target": {
        "id": "Ozzie Gooen",
        "title": "Ozzie Gooen",
        "tags": [
          "person"
        ],
        "content": "#person",
        "size": 18.285714285714285,
        "index": 352,
        "x": -795.6299935195096,
        "y": -1201.5289699537964,
        "vy": -0.36410862655798687,
        "vx": -0.5257118812416671
      },
      "index": 20
    },
    {
      "source": {
        "id": "Colleen McKenzie",
        "title": "Colleen McKenzie",
        "tags": [
          "person"
        ],
        "content": "#person",
        "size": 17.714285714285715,
        "index": 22,
        "x": 615.6457683620001,
        "y": -374.6339202786093,
        "vy": -1.1097477536713236,
        "vx": 0.021596753844054135
      },
      "target": {
        "id": "AI Objectives Institute",
        "title": "AI Objectives Institute",
        "tags": [
          "organization"
        ],
        "content": "#organization \n\nhttps://ai.objectives.institute/",
        "size": 20.57142857142857,
        "index": 304,
        "x": 665.7615191436854,
        "y": -247.07081557119588,
        "vy": -1.1515300453530217,
        "vx": -0.07444975689524554
      },
      "index": 21
    },
    {
      "source": {
        "id": "Reimagining Democracy for AI",
        "title": "Reimagining Democracy for AI",
        "tags": [
          "project"
        ],
        "content": "#project \n\nhttps://aviv.me/Reimagining-Democracy-for-AI-Journal-of-Democracy.pdf\n\nPaper proposes reinventing \"democratic infrastructure\" through four key innovations: \n1. representative deliberations (citizen assemblies selected by lottery), \n2. AI-augmentation of democratic processes, \n3. democracy-as-a-service organizations, and \n4. platform democracy for governing tech companies.",
        "size": 16.571428571428573,
        "index": 23,
        "x": 1741.3530545382905,
        "y": 861.8425828439722,
        "vy": -1.5468886945107914,
        "vx": 0.3233041160506936
      },
      "target": {
        "id": "Aviv Ovadya",
        "title": "Aviv Ovadya",
        "tags": [
          "person"
        ],
        "content": "#person \n\nhttps://aviv.me/",
        "size": 18.285714285714285,
        "index": 283,
        "x": 1597.9484999890756,
        "y": 842.2153044669851,
        "vy": -1.4288230936945292,
        "vx": 0.3153930584572157
      },
      "index": 22
    },
    {
      "source": {
        "id": "Tom Davidson",
        "title": "Tom Davidson",
        "tags": [
          "person"
        ],
        "content": "#person \n\nhttps://tomdavidson-ai.github.io/",
        "size": 17.142857142857142,
        "index": 24,
        "x": -1788.7773735372552,
        "y": 204.77188300552578,
        "vy": -0.1772045677729227,
        "vx": 0.2246908047923073
      },
      "target": {
        "id": "Forethought",
        "title": "Forethought",
        "tags": [
          "organization"
        ],
        "content": "#organization \n\nhttps://www.forethought.org/",
        "size": 17.714285714285715,
        "index": 202,
        "x": -1771.7035542392243,
        "y": 317.48703027353775,
        "vy": -0.1877889855715367,
        "vx": 0.2317876718704255
      },
      "index": 23
    },
    {
      "source": {
        "id": "AI‐assisted scenario generation for strategic planning",
        "title": "AI‐assisted scenario generation for strategic planning",
        "tags": [
          "project"
        ],
        "content": "#project \n\nhttps://onlinelibrary.wiley.com/doi/pdf/10.1002/ffo2.148\n\nThis paper explores how LLMs can be used to generate scenarios for strategic planning (for organizations/businesses), examining whether AI-produced scenarios meet professional standards and how they can best assist human decision-makers. The authors conclude that AI-assisted scenario development shows significant promise as a tool for generating \"raw material\" that human facilitators can refine.",
        "size": 17.142857142857142,
        "index": 26,
        "x": 3097.8329820032714,
        "y": -1201.0364840274362,
        "vy": -3.266900268772297,
        "vx": -0.44754925324535166
      },
      "target": {
        "id": "Matthew Spaniol",
        "title": "Matthew Spaniol",
        "tags": [
          "person"
        ],
        "content": "#person",
        "size": 16.571428571428573,
        "index": 406,
        "x": 3071.5915325379115,
        "y": -1259.2752845373063,
        "vy": -3.296213480248363,
        "vx": -0.36537946147891825
      },
      "index": 24
    },
    {
      "source": {
        "id": "AI‐assisted scenario generation for strategic planning",
        "title": "AI‐assisted scenario generation for strategic planning",
        "tags": [
          "project"
        ],
        "content": "#project \n\nhttps://onlinelibrary.wiley.com/doi/pdf/10.1002/ffo2.148\n\nThis paper explores how LLMs can be used to generate scenarios for strategic planning (for organizations/businesses), examining whether AI-produced scenarios meet professional standards and how they can best assist human decision-makers. The authors conclude that AI-assisted scenario development shows significant promise as a tool for generating \"raw material\" that human facilitators can refine.",
        "size": 17.142857142857142,
        "index": 26,
        "x": 3097.8329820032714,
        "y": -1201.0364840274362,
        "vy": -3.266900268772297,
        "vx": -0.44754925324535166
      },
      "target": {
        "id": "Nicholas Rowland",
        "title": "Nicholas Rowland",
        "tags": [
          "person"
        ],
        "content": "#person",
        "size": 16.571428571428573,
        "index": 45,
        "x": 3126.6299360752982,
        "y": -1145.0685049761623,
        "vy": -3.2403205584431993,
        "vx": -0.5123231791969527
      },
      "index": 25
    },
    {
      "source": {
        "id": "Truthful AI",
        "title": "Truthful AI",
        "tags": [
          "project"
        ],
        "content": "#project \n\nhttps://arxiv.org/pdf/2110.06674\n\nPaper proposes developing AI systems that avoid stating falsehoods, particularly \"negligent falsehoods\" - statements that AI systems should have been able to recognize as likely false given available information. The authors argue for establishing societal standards and institutions to evaluate and certify AI truthfulness, moving beyond current approaches that rely on human-like accountability mechanisms which don't translate well to AI systems. They outline technical approaches for building more truthful AI systems and explore the governance frameworks needed to implement truthfulness standards at scale, including certification bodies and adjudication processes for evaluating AI statements.",
        "size": 20.57142857142857,
        "index": 28,
        "x": -1628.9682601355698,
        "y": 499.78644622592236,
        "vy": -0.20762212577626207,
        "vx": 0.21088585536963259
      },
      "target": {
        "id": "Owain Evans",
        "title": "Owain Evans",
        "tags": [
          "person"
        ],
        "content": "#person \n\nhttps://owainevans.github.io/",
        "size": 17.714285714285715,
        "index": 39,
        "x": -1372.1944531749268,
        "y": 477.9869800887734,
        "vy": -0.29993178439518847,
        "vx": 0.18951433178696656
      },
      "index": 26
    },
    {
      "source": {
        "id": "Truthful AI",
        "title": "Truthful AI",
        "tags": [
          "project"
        ],
        "content": "#project \n\nhttps://arxiv.org/pdf/2110.06674\n\nPaper proposes developing AI systems that avoid stating falsehoods, particularly \"negligent falsehoods\" - statements that AI systems should have been able to recognize as likely false given available information. The authors argue for establishing societal standards and institutions to evaluate and certify AI truthfulness, moving beyond current approaches that rely on human-like accountability mechanisms which don't translate well to AI systems. They outline technical approaches for building more truthful AI systems and explore the governance frameworks needed to implement truthfulness standards at scale, including certification bodies and adjudication processes for evaluating AI statements.",
        "size": 20.57142857142857,
        "index": 28,
        "x": -1628.9682601355698,
        "y": 499.78644622592236,
        "vy": -0.20762212577626207,
        "vx": 0.21088585536963259
      },
      "target": {
        "id": "Owen Cotton-Barratt",
        "title": "Owen Cotton-Barratt",
        "tags": [
          "person"
        ],
        "content": "#person \n\nhttps://strangecities.substack.com/",
        "size": 17.714285714285715,
        "index": 108,
        "x": -1767.049221344344,
        "y": 489.6781367043218,
        "vy": -0.2228153899267383,
        "vx": 0.1807751081937257
      },
      "index": 27
    },
    {
      "source": {
        "id": "Truthful AI",
        "title": "Truthful AI",
        "tags": [
          "project"
        ],
        "content": "#project \n\nhttps://arxiv.org/pdf/2110.06674\n\nPaper proposes developing AI systems that avoid stating falsehoods, particularly \"negligent falsehoods\" - statements that AI systems should have been able to recognize as likely false given available information. The authors argue for establishing societal standards and institutions to evaluate and certify AI truthfulness, moving beyond current approaches that rely on human-like accountability mechanisms which don't translate well to AI systems. They outline technical approaches for building more truthful AI systems and explore the governance frameworks needed to implement truthfulness standards at scale, including certification bodies and adjudication processes for evaluating AI statements.",
        "size": 20.57142857142857,
        "index": 28,
        "x": -1628.9682601355698,
        "y": 499.78644622592236,
        "vy": -0.20762212577626207,
        "vx": 0.21088585536963259
      },
      "target": {
        "id": "Lukas Finnveden",
        "title": "Lukas Finnveden",
        "tags": [
          "person"
        ],
        "content": "#person",
        "size": 18.285714285714285,
        "index": 256,
        "x": -1770.6095471676322,
        "y": 430.1205037586758,
        "vy": -0.22586013414071307,
        "vx": 0.1963978919880959
      },
      "index": 28
    },
    {
      "source": {
        "id": "Truthful AI",
        "title": "Truthful AI",
        "tags": [
          "project"
        ],
        "content": "#project \n\nhttps://arxiv.org/pdf/2110.06674\n\nPaper proposes developing AI systems that avoid stating falsehoods, particularly \"negligent falsehoods\" - statements that AI systems should have been able to recognize as likely false given available information. The authors argue for establishing societal standards and institutions to evaluate and certify AI truthfulness, moving beyond current approaches that rely on human-like accountability mechanisms which don't translate well to AI systems. They outline technical approaches for building more truthful AI systems and explore the governance frameworks needed to implement truthfulness standards at scale, including certification bodies and adjudication processes for evaluating AI statements.",
        "size": 20.57142857142857,
        "index": 28,
        "x": -1628.9682601355698,
        "y": 499.78644622592236,
        "vy": -0.20762212577626207,
        "vx": 0.21088585536963259
      },
      "target": {
        "id": "Adam Bales",
        "title": "Adam Bales",
        "tags": [
          "person"
        ],
        "content": "#person",
        "size": 16.571428571428573,
        "index": 155,
        "x": -1664.4666048359338,
        "y": 602.5420430102494,
        "vy": -0.19181175684085247,
        "vx": 0.27589028536145666
      },
      "index": 29
    },
    {
      "source": {
        "id": "Truthful AI",
        "title": "Truthful AI",
        "tags": [
          "project"
        ],
        "content": "#project \n\nhttps://arxiv.org/pdf/2110.06674\n\nPaper proposes developing AI systems that avoid stating falsehoods, particularly \"negligent falsehoods\" - statements that AI systems should have been able to recognize as likely false given available information. The authors argue for establishing societal standards and institutions to evaluate and certify AI truthfulness, moving beyond current approaches that rely on human-like accountability mechanisms which don't translate well to AI systems. They outline technical approaches for building more truthful AI systems and explore the governance frameworks needed to implement truthfulness standards at scale, including certification bodies and adjudication processes for evaluating AI statements.",
        "size": 20.57142857142857,
        "index": 28,
        "x": -1628.9682601355698,
        "y": 499.78644622592236,
        "vy": -0.20762212577626207,
        "vx": 0.21088585536963259
      },
      "target": {
        "id": "Avital Balwit",
        "title": "Avital Balwit",
        "tags": [
          "person"
        ],
        "content": "#person",
        "size": 16.571428571428573,
        "index": 351,
        "x": -1562.9896593507694,
        "y": 499.59978264995505,
        "vy": -0.2623172720494602,
        "vx": 0.15035130973573832
      },
      "index": 30
    },
    {
      "source": {
        "id": "Truthful AI",
        "title": "Truthful AI",
        "tags": [
          "project"
        ],
        "content": "#project \n\nhttps://arxiv.org/pdf/2110.06674\n\nPaper proposes developing AI systems that avoid stating falsehoods, particularly \"negligent falsehoods\" - statements that AI systems should have been able to recognize as likely false given available information. The authors argue for establishing societal standards and institutions to evaluate and certify AI truthfulness, moving beyond current approaches that rely on human-like accountability mechanisms which don't translate well to AI systems. They outline technical approaches for building more truthful AI systems and explore the governance frameworks needed to implement truthfulness standards at scale, including certification bodies and adjudication processes for evaluating AI statements.",
        "size": 20.57142857142857,
        "index": 28,
        "x": -1628.9682601355698,
        "y": 499.78644622592236,
        "vy": -0.20762212577626207,
        "vx": 0.21088585536963259
      },
      "target": {
        "id": "Peter Wills",
        "title": "Peter Wills",
        "tags": [
          "person"
        ],
        "content": "#person",
        "size": 16.571428571428573,
        "index": 277,
        "x": -1677.473663165288,
        "y": 547.2208205595574,
        "vy": -0.16575894696369212,
        "vx": 0.17853645801068893
      },
      "index": 31
    },
    {
      "source": {
        "id": "Truthful AI",
        "title": "Truthful AI",
        "tags": [
          "project"
        ],
        "content": "#project \n\nhttps://arxiv.org/pdf/2110.06674\n\nPaper proposes developing AI systems that avoid stating falsehoods, particularly \"negligent falsehoods\" - statements that AI systems should have been able to recognize as likely false given available information. The authors argue for establishing societal standards and institutions to evaluate and certify AI truthfulness, moving beyond current approaches that rely on human-like accountability mechanisms which don't translate well to AI systems. They outline technical approaches for building more truthful AI systems and explore the governance frameworks needed to implement truthfulness standards at scale, including certification bodies and adjudication processes for evaluating AI statements.",
        "size": 20.57142857142857,
        "index": 28,
        "x": -1628.9682601355698,
        "y": 499.78644622592236,
        "vy": -0.20762212577626207,
        "vx": 0.21088585536963259
      },
      "target": {
        "id": "Luca Righetti",
        "title": "Luca Righetti",
        "tags": [
          "person"
        ],
        "content": "#person",
        "size": 16.571428571428573,
        "index": 263,
        "x": -1608.5787237565016,
        "y": 592.2434745346627,
        "vy": -0.22449834006054858,
        "vx": 0.2668748016924781
      },
      "index": 32
    },
    {
      "source": {
        "id": "Truthful AI",
        "title": "Truthful AI",
        "tags": [
          "project"
        ],
        "content": "#project \n\nhttps://arxiv.org/pdf/2110.06674\n\nPaper proposes developing AI systems that avoid stating falsehoods, particularly \"negligent falsehoods\" - statements that AI systems should have been able to recognize as likely false given available information. The authors argue for establishing societal standards and institutions to evaluate and certify AI truthfulness, moving beyond current approaches that rely on human-like accountability mechanisms which don't translate well to AI systems. They outline technical approaches for building more truthful AI systems and explore the governance frameworks needed to implement truthfulness standards at scale, including certification bodies and adjudication processes for evaluating AI statements.",
        "size": 20.57142857142857,
        "index": 28,
        "x": -1628.9682601355698,
        "y": 499.78644622592236,
        "vy": -0.20762212577626207,
        "vx": 0.21088585536963259
      },
      "target": {
        "id": "William Saunders",
        "title": "William Saunders",
        "tags": [
          "person"
        ],
        "content": "#person",
        "size": 16.571428571428573,
        "index": 121,
        "x": -1568.9964861649082,
        "y": 554.7317005121304,
        "vy": -0.2573281544366966,
        "vx": 0.20913315028023036
      },
      "index": 33
    },
    {
      "source": {
        "id": "AI and the Future of Digital Public Squares",
        "title": "AI and the Future of Digital Public Squares",
        "tags": [
          "project"
        ],
        "content": "#project \n\nhttps://arxiv.org/pdf/2412.09988\n\nPaper examines how LLMs can improve online discussions and democratic participation. \n\n## Four Main Areas of Focus\n\n**1. Collective Dialogue Systems**\n- Tools that help large groups have structured conversations and find common ground on important issues.\n\n**2. Bridging Systems**\n- Algorithms that promote content bringing people together rather than showing divisive posts that drive engagement.\n\n**3. Community-Driven Moderation**\n- AI tools to help volunteer moderators manage online communities more effectively while keeping humans in control.\n\n**4. Proof of Humanity Systems**\n- Methods to verify that real humans (not bots) are participating in online discussions as AI makes fake accounts easier to create.",
        "size": 31.42857142857143,
        "index": 34,
        "x": 1423.1375195527432,
        "y": 867.5196560102631,
        "vy": -1.275706555599965,
        "vx": 0.22941699888881384
      },
      "target": {
        "id": "Beth Goldberg",
        "title": "Beth Goldberg",
        "tags": [
          "person"
        ],
        "content": "#person",
        "size": 17.142857142857142,
        "index": 436,
        "x": 1614.3717603909563,
        "y": 961.5912998628878,
        "vy": -1.4139817481595707,
        "vx": 0.2944467547795422
      },
      "index": 34
    },
    {
      "source": {
        "id": "AI and the Future of Digital Public Squares",
        "title": "AI and the Future of Digital Public Squares",
        "tags": [
          "project"
        ],
        "content": "#project \n\nhttps://arxiv.org/pdf/2412.09988\n\nPaper examines how LLMs can improve online discussions and democratic participation. \n\n## Four Main Areas of Focus\n\n**1. Collective Dialogue Systems**\n- Tools that help large groups have structured conversations and find common ground on important issues.\n\n**2. Bridging Systems**\n- Algorithms that promote content bringing people together rather than showing divisive posts that drive engagement.\n\n**3. Community-Driven Moderation**\n- AI tools to help volunteer moderators manage online communities more effectively while keeping humans in control.\n\n**4. Proof of Humanity Systems**\n- Methods to verify that real humans (not bots) are participating in online discussions as AI makes fake accounts easier to create.",
        "size": 31.42857142857143,
        "index": 34,
        "x": 1423.1375195527432,
        "y": 867.5196560102631,
        "vy": -1.275706555599965,
        "vx": 0.22941699888881384
      },
      "target": {
        "id": "Diana Acosta-Navas",
        "title": "Diana Acosta-Navas",
        "tags": [
          "person"
        ],
        "content": "#person",
        "size": 16.571428571428573,
        "index": 221,
        "x": 1479.1244173198797,
        "y": 826.1725374640181,
        "vy": -1.3487521016540718,
        "vx": 0.190455414621277
      },
      "index": 35
    },
    {
      "source": {
        "id": "AI and the Future of Digital Public Squares",
        "title": "AI and the Future of Digital Public Squares",
        "tags": [
          "project"
        ],
        "content": "#project \n\nhttps://arxiv.org/pdf/2412.09988\n\nPaper examines how LLMs can improve online discussions and democratic participation. \n\n## Four Main Areas of Focus\n\n**1. Collective Dialogue Systems**\n- Tools that help large groups have structured conversations and find common ground on important issues.\n\n**2. Bridging Systems**\n- Algorithms that promote content bringing people together rather than showing divisive posts that drive engagement.\n\n**3. Community-Driven Moderation**\n- AI tools to help volunteer moderators manage online communities more effectively while keeping humans in control.\n\n**4. Proof of Humanity Systems**\n- Methods to verify that real humans (not bots) are participating in online discussions as AI makes fake accounts easier to create.",
        "size": 31.42857142857143,
        "index": 34,
        "x": 1423.1375195527432,
        "y": 867.5196560102631,
        "vy": -1.275706555599965,
        "vx": 0.22941699888881384
      },
      "target": {
        "id": "Michiel Bakker",
        "title": "Michiel Bakker",
        "tags": [
          "person"
        ],
        "content": "#person \n\nhttps://miba.dev/",
        "size": 21.142857142857142,
        "index": 284,
        "x": 1225.501866244462,
        "y": 730.5079873536822,
        "vy": -1.2714527484014098,
        "vx": 0.27625785273055353
      },
      "index": 36
    },
    {
      "source": {
        "id": "AI and the Future of Digital Public Squares",
        "title": "AI and the Future of Digital Public Squares",
        "tags": [
          "project"
        ],
        "content": "#project \n\nhttps://arxiv.org/pdf/2412.09988\n\nPaper examines how LLMs can improve online discussions and democratic participation. \n\n## Four Main Areas of Focus\n\n**1. Collective Dialogue Systems**\n- Tools that help large groups have structured conversations and find common ground on important issues.\n\n**2. Bridging Systems**\n- Algorithms that promote content bringing people together rather than showing divisive posts that drive engagement.\n\n**3. Community-Driven Moderation**\n- AI tools to help volunteer moderators manage online communities more effectively while keeping humans in control.\n\n**4. Proof of Humanity Systems**\n- Methods to verify that real humans (not bots) are participating in online discussions as AI makes fake accounts easier to create.",
        "size": 31.42857142857143,
        "index": 34,
        "x": 1423.1375195527432,
        "y": 867.5196560102631,
        "vy": -1.275706555599965,
        "vx": 0.22941699888881384
      },
      "target": {
        "id": "Ian Beacock",
        "title": "Ian Beacock",
        "tags": [
          "person"
        ],
        "content": "#person",
        "size": 17.142857142857142,
        "index": 134,
        "x": 1636.8020845388053,
        "y": 1015.3881351163002,
        "vy": -1.4459022886097965,
        "vx": 0.3710885486137493
      },
      "index": 37
    },
    {
      "source": {
        "id": "AI and the Future of Digital Public Squares",
        "title": "AI and the Future of Digital Public Squares",
        "tags": [
          "project"
        ],
        "content": "#project \n\nhttps://arxiv.org/pdf/2412.09988\n\nPaper examines how LLMs can improve online discussions and democratic participation. \n\n## Four Main Areas of Focus\n\n**1. Collective Dialogue Systems**\n- Tools that help large groups have structured conversations and find common ground on important issues.\n\n**2. Bridging Systems**\n- Algorithms that promote content bringing people together rather than showing divisive posts that drive engagement.\n\n**3. Community-Driven Moderation**\n- AI tools to help volunteer moderators manage online communities more effectively while keeping humans in control.\n\n**4. Proof of Humanity Systems**\n- Methods to verify that real humans (not bots) are participating in online discussions as AI makes fake accounts easier to create.",
        "size": 31.42857142857143,
        "index": 34,
        "x": 1423.1375195527432,
        "y": 867.5196560102631,
        "vy": -1.275706555599965,
        "vx": 0.22941699888881384
      },
      "target": {
        "id": "Matthew Botvinick",
        "title": "Matthew Botvinick",
        "tags": [
          "person"
        ],
        "content": "#person",
        "size": 18.285714285714285,
        "index": 139,
        "x": 1370.3134606965014,
        "y": 676.0315589136434,
        "vy": -1.3768077843382622,
        "vx": 0.20959930595528592
      },
      "index": 38
    },
    {
      "source": {
        "id": "AI and the Future of Digital Public Squares",
        "title": "AI and the Future of Digital Public Squares",
        "tags": [
          "project"
        ],
        "content": "#project \n\nhttps://arxiv.org/pdf/2412.09988\n\nPaper examines how LLMs can improve online discussions and democratic participation. \n\n## Four Main Areas of Focus\n\n**1. Collective Dialogue Systems**\n- Tools that help large groups have structured conversations and find common ground on important issues.\n\n**2. Bridging Systems**\n- Algorithms that promote content bringing people together rather than showing divisive posts that drive engagement.\n\n**3. Community-Driven Moderation**\n- AI tools to help volunteer moderators manage online communities more effectively while keeping humans in control.\n\n**4. Proof of Humanity Systems**\n- Methods to verify that real humans (not bots) are participating in online discussions as AI makes fake accounts easier to create.",
        "size": 31.42857142857143,
        "index": 34,
        "x": 1423.1375195527432,
        "y": 867.5196560102631,
        "vy": -1.275706555599965,
        "vx": 0.22941699888881384
      },
      "target": {
        "id": "Prateek Buch",
        "title": "Prateek Buch",
        "tags": [
          "person"
        ],
        "content": "#person",
        "size": 16.571428571428573,
        "index": 307,
        "x": 1413.2051277119535,
        "y": 936.8849366411313,
        "vy": -1.2706963771420887,
        "vx": 0.3071540589931514
      },
      "index": 39
    },
    {
      "source": {
        "id": "AI and the Future of Digital Public Squares",
        "title": "AI and the Future of Digital Public Squares",
        "tags": [
          "project"
        ],
        "content": "#project \n\nhttps://arxiv.org/pdf/2412.09988\n\nPaper examines how LLMs can improve online discussions and democratic participation. \n\n## Four Main Areas of Focus\n\n**1. Collective Dialogue Systems**\n- Tools that help large groups have structured conversations and find common ground on important issues.\n\n**2. Bridging Systems**\n- Algorithms that promote content bringing people together rather than showing divisive posts that drive engagement.\n\n**3. Community-Driven Moderation**\n- AI tools to help volunteer moderators manage online communities more effectively while keeping humans in control.\n\n**4. Proof of Humanity Systems**\n- Methods to verify that real humans (not bots) are participating in online discussions as AI makes fake accounts easier to create.",
        "size": 31.42857142857143,
        "index": 34,
        "x": 1423.1375195527432,
        "y": 867.5196560102631,
        "vy": -1.275706555599965,
        "vx": 0.22941699888881384
      },
      "target": {
        "id": "Renée DiResta",
        "title": "Renée DiResta",
        "tags": [
          "person"
        ],
        "content": "#person",
        "size": 16.571428571428573,
        "index": 366,
        "x": 1492.5712196708228,
        "y": 879.9340112496327,
        "vy": -1.363448565830491,
        "vx": 0.23320672349469254
      },
      "index": 40
    },
    {
      "source": {
        "id": "AI and the Future of Digital Public Squares",
        "title": "AI and the Future of Digital Public Squares",
        "tags": [
          "project"
        ],
        "content": "#project \n\nhttps://arxiv.org/pdf/2412.09988\n\nPaper examines how LLMs can improve online discussions and democratic participation. \n\n## Four Main Areas of Focus\n\n**1. Collective Dialogue Systems**\n- Tools that help large groups have structured conversations and find common ground on important issues.\n\n**2. Bridging Systems**\n- Algorithms that promote content bringing people together rather than showing divisive posts that drive engagement.\n\n**3. Community-Driven Moderation**\n- AI tools to help volunteer moderators manage online communities more effectively while keeping humans in control.\n\n**4. Proof of Humanity Systems**\n- Methods to verify that real humans (not bots) are participating in online discussions as AI makes fake accounts easier to create.",
        "size": 31.42857142857143,
        "index": 34,
        "x": 1423.1375195527432,
        "y": 867.5196560102631,
        "vy": -1.275706555599965,
        "vx": 0.22941699888881384
      },
      "target": {
        "id": "Nandika Donthi",
        "title": "Nandika Donthi",
        "tags": [
          "person"
        ],
        "content": "#person",
        "size": 17.142857142857142,
        "index": 358,
        "x": 1424.6119078058373,
        "y": 1043.8070404484067,
        "vy": -1.145949676023276,
        "vx": 0.27030601690505907
      },
      "index": 41
    },
    {
      "source": {
        "id": "AI and the Future of Digital Public Squares",
        "title": "AI and the Future of Digital Public Squares",
        "tags": [
          "project"
        ],
        "content": "#project \n\nhttps://arxiv.org/pdf/2412.09988\n\nPaper examines how LLMs can improve online discussions and democratic participation. \n\n## Four Main Areas of Focus\n\n**1. Collective Dialogue Systems**\n- Tools that help large groups have structured conversations and find common ground on important issues.\n\n**2. Bridging Systems**\n- Algorithms that promote content bringing people together rather than showing divisive posts that drive engagement.\n\n**3. Community-Driven Moderation**\n- AI tools to help volunteer moderators manage online communities more effectively while keeping humans in control.\n\n**4. Proof of Humanity Systems**\n- Methods to verify that real humans (not bots) are participating in online discussions as AI makes fake accounts easier to create.",
        "size": 31.42857142857143,
        "index": 34,
        "x": 1423.1375195527432,
        "y": 867.5196560102631,
        "vy": -1.275706555599965,
        "vx": 0.22941699888881384
      },
      "target": {
        "id": "Nathanael Fast",
        "title": "Nathanael Fast",
        "tags": [
          "person"
        ],
        "content": "#person",
        "size": 16.571428571428573,
        "index": 157,
        "x": 1443.4431818309442,
        "y": 985.1211103549442,
        "vy": -1.3005000041659411,
        "vx": 0.39509241553759533
      },
      "index": 42
    },
    {
      "source": {
        "id": "AI and the Future of Digital Public Squares",
        "title": "AI and the Future of Digital Public Squares",
        "tags": [
          "project"
        ],
        "content": "#project \n\nhttps://arxiv.org/pdf/2412.09988\n\nPaper examines how LLMs can improve online discussions and democratic participation. \n\n## Four Main Areas of Focus\n\n**1. Collective Dialogue Systems**\n- Tools that help large groups have structured conversations and find common ground on important issues.\n\n**2. Bridging Systems**\n- Algorithms that promote content bringing people together rather than showing divisive posts that drive engagement.\n\n**3. Community-Driven Moderation**\n- AI tools to help volunteer moderators manage online communities more effectively while keeping humans in control.\n\n**4. Proof of Humanity Systems**\n- Methods to verify that real humans (not bots) are participating in online discussions as AI makes fake accounts easier to create.",
        "size": 31.42857142857143,
        "index": 34,
        "x": 1423.1375195527432,
        "y": 867.5196560102631,
        "vy": -1.275706555599965,
        "vx": 0.22941699888881384
      },
      "target": {
        "id": "Ravi Iyer",
        "title": "Ravi Iyer",
        "tags": [
          "person"
        ],
        "content": "#person",
        "size": 16.571428571428573,
        "index": 247,
        "x": 1360.2647556643778,
        "y": 932.1982198459554,
        "vy": -1.0348093526398057,
        "vx": 0.2903256384512171
      },
      "index": 43
    },
    {
      "source": {
        "id": "AI and the Future of Digital Public Squares",
        "title": "AI and the Future of Digital Public Squares",
        "tags": [
          "project"
        ],
        "content": "#project \n\nhttps://arxiv.org/pdf/2412.09988\n\nPaper examines how LLMs can improve online discussions and democratic participation. \n\n## Four Main Areas of Focus\n\n**1. Collective Dialogue Systems**\n- Tools that help large groups have structured conversations and find common ground on important issues.\n\n**2. Bridging Systems**\n- Algorithms that promote content bringing people together rather than showing divisive posts that drive engagement.\n\n**3. Community-Driven Moderation**\n- AI tools to help volunteer moderators manage online communities more effectively while keeping humans in control.\n\n**4. Proof of Humanity Systems**\n- Methods to verify that real humans (not bots) are participating in online discussions as AI makes fake accounts easier to create.",
        "size": 31.42857142857143,
        "index": 34,
        "x": 1423.1375195527432,
        "y": 867.5196560102631,
        "vy": -1.275706555599965,
        "vx": 0.22941699888881384
      },
      "target": {
        "id": "Zaria Jalan",
        "title": "Zaria Jalan",
        "tags": [
          "person"
        ],
        "content": "#person",
        "size": 16.571428571428573,
        "index": 18,
        "x": 1386.190552838559,
        "y": 986.1306566948462,
        "vy": -1.1367519959359862,
        "vx": 0.45126970557543356
      },
      "index": 44
    },
    {
      "source": {
        "id": "AI and the Future of Digital Public Squares",
        "title": "AI and the Future of Digital Public Squares",
        "tags": [
          "project"
        ],
        "content": "#project \n\nhttps://arxiv.org/pdf/2412.09988\n\nPaper examines how LLMs can improve online discussions and democratic participation. \n\n## Four Main Areas of Focus\n\n**1. Collective Dialogue Systems**\n- Tools that help large groups have structured conversations and find common ground on important issues.\n\n**2. Bridging Systems**\n- Algorithms that promote content bringing people together rather than showing divisive posts that drive engagement.\n\n**3. Community-Driven Moderation**\n- AI tools to help volunteer moderators manage online communities more effectively while keeping humans in control.\n\n**4. Proof of Humanity Systems**\n- Methods to verify that real humans (not bots) are participating in online discussions as AI makes fake accounts easier to create.",
        "size": 31.42857142857143,
        "index": 34,
        "x": 1423.1375195527432,
        "y": 867.5196560102631,
        "vy": -1.275706555599965,
        "vx": 0.22941699888881384
      },
      "target": {
        "id": "Andrew Konya",
        "title": "Andrew Konya",
        "tags": [
          "person"
        ],
        "content": "#person \n\nhttps://andrewkonya.com/",
        "size": 18.857142857142858,
        "index": 265,
        "x": 1563.7748126336485,
        "y": 791.1411106203664,
        "vy": -1.4006415470721048,
        "vx": 0.2425628664665928
      },
      "index": 45
    },
    {
      "source": {
        "id": "AI and the Future of Digital Public Squares",
        "title": "AI and the Future of Digital Public Squares",
        "tags": [
          "project"
        ],
        "content": "#project \n\nhttps://arxiv.org/pdf/2412.09988\n\nPaper examines how LLMs can improve online discussions and democratic participation. \n\n## Four Main Areas of Focus\n\n**1. Collective Dialogue Systems**\n- Tools that help large groups have structured conversations and find common ground on important issues.\n\n**2. Bridging Systems**\n- Algorithms that promote content bringing people together rather than showing divisive posts that drive engagement.\n\n**3. Community-Driven Moderation**\n- AI tools to help volunteer moderators manage online communities more effectively while keeping humans in control.\n\n**4. Proof of Humanity Systems**\n- Methods to verify that real humans (not bots) are participating in online discussions as AI makes fake accounts easier to create.",
        "size": 31.42857142857143,
        "index": 34,
        "x": 1423.1375195527432,
        "y": 867.5196560102631,
        "vy": -1.275706555599965,
        "vx": 0.22941699888881384
      },
      "target": {
        "id": "Grace Kwak Danciu",
        "title": "Grace Kwak Danciu",
        "tags": [
          "person"
        ],
        "content": "#person",
        "size": 16.571428571428573,
        "index": 346,
        "x": 1469.0361038918277,
        "y": 932.0050994350513,
        "vy": -1.333261540296507,
        "vx": 0.29987260709328806
      },
      "index": 46
    },
    {
      "source": {
        "id": "AI and the Future of Digital Public Squares",
        "title": "AI and the Future of Digital Public Squares",
        "tags": [
          "project"
        ],
        "content": "#project \n\nhttps://arxiv.org/pdf/2412.09988\n\nPaper examines how LLMs can improve online discussions and democratic participation. \n\n## Four Main Areas of Focus\n\n**1. Collective Dialogue Systems**\n- Tools that help large groups have structured conversations and find common ground on important issues.\n\n**2. Bridging Systems**\n- Algorithms that promote content bringing people together rather than showing divisive posts that drive engagement.\n\n**3. Community-Driven Moderation**\n- AI tools to help volunteer moderators manage online communities more effectively while keeping humans in control.\n\n**4. Proof of Humanity Systems**\n- Methods to verify that real humans (not bots) are participating in online discussions as AI makes fake accounts easier to create.",
        "size": 31.42857142857143,
        "index": 34,
        "x": 1423.1375195527432,
        "y": 867.5196560102631,
        "vy": -1.275706555599965,
        "vx": 0.22941699888881384
      },
      "target": {
        "id": "Hélène Landemore",
        "title": "Hélène Landemore",
        "tags": [
          "person"
        ],
        "content": "#person\n\nhttps://www.helenelandemore.com/",
        "size": 17.142857142857142,
        "index": 425,
        "x": 1482.5429370856566,
        "y": 1037.3892381314145,
        "vy": -1.2905561967248251,
        "vx": 0.2545804807479611
      },
      "index": 47
    },
    {
      "source": {
        "id": "AI and the Future of Digital Public Squares",
        "title": "AI and the Future of Digital Public Squares",
        "tags": [
          "project"
        ],
        "content": "#project \n\nhttps://arxiv.org/pdf/2412.09988\n\nPaper examines how LLMs can improve online discussions and democratic participation. \n\n## Four Main Areas of Focus\n\n**1. Collective Dialogue Systems**\n- Tools that help large groups have structured conversations and find common ground on important issues.\n\n**2. Bridging Systems**\n- Algorithms that promote content bringing people together rather than showing divisive posts that drive engagement.\n\n**3. Community-Driven Moderation**\n- AI tools to help volunteer moderators manage online communities more effectively while keeping humans in control.\n\n**4. Proof of Humanity Systems**\n- Methods to verify that real humans (not bots) are participating in online discussions as AI makes fake accounts easier to create.",
        "size": 31.42857142857143,
        "index": 34,
        "x": 1423.1375195527432,
        "y": 867.5196560102631,
        "vy": -1.275706555599965,
        "vx": 0.22941699888881384
      },
      "target": {
        "id": "Alice Marwick",
        "title": "Alice Marwick",
        "tags": [
          "person"
        ],
        "content": "#person \n\nhttps://tiara.org/",
        "size": 16.571428571428573,
        "index": 43,
        "x": 1499.3963013694793,
        "y": 980.9523479974121,
        "vy": -1.362366541791264,
        "vx": 0.40333817919771314
      },
      "index": 48
    },
    {
      "source": {
        "id": "AI and the Future of Digital Public Squares",
        "title": "AI and the Future of Digital Public Squares",
        "tags": [
          "project"
        ],
        "content": "#project \n\nhttps://arxiv.org/pdf/2412.09988\n\nPaper examines how LLMs can improve online discussions and democratic participation. \n\n## Four Main Areas of Focus\n\n**1. Collective Dialogue Systems**\n- Tools that help large groups have structured conversations and find common ground on important issues.\n\n**2. Bridging Systems**\n- Algorithms that promote content bringing people together rather than showing divisive posts that drive engagement.\n\n**3. Community-Driven Moderation**\n- AI tools to help volunteer moderators manage online communities more effectively while keeping humans in control.\n\n**4. Proof of Humanity Systems**\n- Methods to verify that real humans (not bots) are participating in online discussions as AI makes fake accounts easier to create.",
        "size": 31.42857142857143,
        "index": 34,
        "x": 1423.1375195527432,
        "y": 867.5196560102631,
        "vy": -1.275706555599965,
        "vx": 0.22941699888881384
      },
      "target": {
        "id": "Carl Miller",
        "title": "Carl Miller",
        "tags": [
          "person"
        ],
        "content": "#person \n\nhttps://www.carlmiller.co/",
        "size": 17.142857142857142,
        "index": 374,
        "x": 1090.9638718162498,
        "y": 350.8267083816078,
        "vy": -1.144654368229556,
        "vx": 0.15916945181270295
      },
      "index": 49
    },
    {
      "source": {
        "id": "AI and the Future of Digital Public Squares",
        "title": "AI and the Future of Digital Public Squares",
        "tags": [
          "project"
        ],
        "content": "#project \n\nhttps://arxiv.org/pdf/2412.09988\n\nPaper examines how LLMs can improve online discussions and democratic participation. \n\n## Four Main Areas of Focus\n\n**1. Collective Dialogue Systems**\n- Tools that help large groups have structured conversations and find common ground on important issues.\n\n**2. Bridging Systems**\n- Algorithms that promote content bringing people together rather than showing divisive posts that drive engagement.\n\n**3. Community-Driven Moderation**\n- AI tools to help volunteer moderators manage online communities more effectively while keeping humans in control.\n\n**4. Proof of Humanity Systems**\n- Methods to verify that real humans (not bots) are participating in online discussions as AI makes fake accounts easier to create.",
        "size": 31.42857142857143,
        "index": 34,
        "x": 1423.1375195527432,
        "y": 867.5196560102631,
        "vy": -1.275706555599965,
        "vx": 0.22941699888881384
      },
      "target": {
        "id": "Aviv Ovadya",
        "title": "Aviv Ovadya",
        "tags": [
          "person"
        ],
        "content": "#person \n\nhttps://aviv.me/",
        "size": 18.285714285714285,
        "index": 283,
        "x": 1597.9484999890756,
        "y": 842.2153044669851,
        "vy": -1.4288230936945292,
        "vx": 0.3153930584572157
      },
      "index": 50
    },
    {
      "source": {
        "id": "AI and the Future of Digital Public Squares",
        "title": "AI and the Future of Digital Public Squares",
        "tags": [
          "project"
        ],
        "content": "#project \n\nhttps://arxiv.org/pdf/2412.09988\n\nPaper examines how LLMs can improve online discussions and democratic participation. \n\n## Four Main Areas of Focus\n\n**1. Collective Dialogue Systems**\n- Tools that help large groups have structured conversations and find common ground on important issues.\n\n**2. Bridging Systems**\n- Algorithms that promote content bringing people together rather than showing divisive posts that drive engagement.\n\n**3. Community-Driven Moderation**\n- AI tools to help volunteer moderators manage online communities more effectively while keeping humans in control.\n\n**4. Proof of Humanity Systems**\n- Methods to verify that real humans (not bots) are participating in online discussions as AI makes fake accounts easier to create.",
        "size": 31.42857142857143,
        "index": 34,
        "x": 1423.1375195527432,
        "y": 867.5196560102631,
        "vy": -1.275706555599965,
        "vx": 0.22941699888881384
      },
      "target": {
        "id": "Emily Saltz",
        "title": "Emily Saltz",
        "tags": [
          "person"
        ],
        "content": "#person \n\nhttps://www.emilysaltz.space/",
        "size": 17.142857142857142,
        "index": 55,
        "x": 1675.7682442794192,
        "y": 962.4398438049817,
        "vy": -1.5392520011668342,
        "vx": 0.2752031074966344
      },
      "index": 51
    },
    {
      "source": {
        "id": "AI and the Future of Digital Public Squares",
        "title": "AI and the Future of Digital Public Squares",
        "tags": [
          "project"
        ],
        "content": "#project \n\nhttps://arxiv.org/pdf/2412.09988\n\nPaper examines how LLMs can improve online discussions and democratic participation. \n\n## Four Main Areas of Focus\n\n**1. Collective Dialogue Systems**\n- Tools that help large groups have structured conversations and find common ground on important issues.\n\n**2. Bridging Systems**\n- Algorithms that promote content bringing people together rather than showing divisive posts that drive engagement.\n\n**3. Community-Driven Moderation**\n- AI tools to help volunteer moderators manage online communities more effectively while keeping humans in control.\n\n**4. Proof of Humanity Systems**\n- Methods to verify that real humans (not bots) are participating in online discussions as AI makes fake accounts easier to create.",
        "size": 31.42857142857143,
        "index": 34,
        "x": 1423.1375195527432,
        "y": 867.5196560102631,
        "vy": -1.275706555599965,
        "vx": 0.22941699888881384
      },
      "target": {
        "id": "Lisa Schirch",
        "title": "Lisa Schirch",
        "tags": [
          "person"
        ],
        "content": "#person \n\nhttps://kroc.nd.edu/faculty-and-staff/lisa-schirch/",
        "size": 18.285714285714285,
        "index": 291,
        "x": 1561.488577698378,
        "y": 890.5844108032592,
        "vy": -1.453577080273647,
        "vx": 0.2825364996104594
      },
      "index": 52
    },
    {
      "source": {
        "id": "AI and the Future of Digital Public Squares",
        "title": "AI and the Future of Digital Public Squares",
        "tags": [
          "project"
        ],
        "content": "#project \n\nhttps://arxiv.org/pdf/2412.09988\n\nPaper examines how LLMs can improve online discussions and democratic participation. \n\n## Four Main Areas of Focus\n\n**1. Collective Dialogue Systems**\n- Tools that help large groups have structured conversations and find common ground on important issues.\n\n**2. Bridging Systems**\n- Algorithms that promote content bringing people together rather than showing divisive posts that drive engagement.\n\n**3. Community-Driven Moderation**\n- AI tools to help volunteer moderators manage online communities more effectively while keeping humans in control.\n\n**4. Proof of Humanity Systems**\n- Methods to verify that real humans (not bots) are participating in online discussions as AI makes fake accounts easier to create.",
        "size": 31.42857142857143,
        "index": 34,
        "x": 1423.1375195527432,
        "y": 867.5196560102631,
        "vy": -1.275706555599965,
        "vx": 0.22941699888881384
      },
      "target": {
        "id": "Dalit Shalom",
        "title": "Dalit Shalom",
        "tags": [
          "person"
        ],
        "content": "#person",
        "size": 16.571428571428573,
        "index": 137,
        "x": 1533.9051536390257,
        "y": 840.2001456050014,
        "vy": -1.4031064285113677,
        "vx": 0.1980863946059408
      },
      "index": 53
    },
    {
      "source": {
        "id": "AI and the Future of Digital Public Squares",
        "title": "AI and the Future of Digital Public Squares",
        "tags": [
          "project"
        ],
        "content": "#project \n\nhttps://arxiv.org/pdf/2412.09988\n\nPaper examines how LLMs can improve online discussions and democratic participation. \n\n## Four Main Areas of Focus\n\n**1. Collective Dialogue Systems**\n- Tools that help large groups have structured conversations and find common ground on important issues.\n\n**2. Bridging Systems**\n- Algorithms that promote content bringing people together rather than showing divisive posts that drive engagement.\n\n**3. Community-Driven Moderation**\n- AI tools to help volunteer moderators manage online communities more effectively while keeping humans in control.\n\n**4. Proof of Humanity Systems**\n- Methods to verify that real humans (not bots) are participating in online discussions as AI makes fake accounts easier to create.",
        "size": 31.42857142857143,
        "index": 34,
        "x": 1423.1375195527432,
        "y": 867.5196560102631,
        "vy": -1.275706555599965,
        "vx": 0.22941699888881384
      },
      "target": {
        "id": "Divya Siddarth",
        "title": "Divya Siddarth",
        "tags": [
          "person"
        ],
        "content": "#person \n\nhttps://divyasiddarth.com/",
        "size": 17.714285714285715,
        "index": 414,
        "x": 1251.3647810817142,
        "y": 910.2533943640935,
        "vy": -1.2814979219729066,
        "vx": 0.3150773430583668
      },
      "index": 54
    },
    {
      "source": {
        "id": "AI and the Future of Digital Public Squares",
        "title": "AI and the Future of Digital Public Squares",
        "tags": [
          "project"
        ],
        "content": "#project \n\nhttps://arxiv.org/pdf/2412.09988\n\nPaper examines how LLMs can improve online discussions and democratic participation. \n\n## Four Main Areas of Focus\n\n**1. Collective Dialogue Systems**\n- Tools that help large groups have structured conversations and find common ground on important issues.\n\n**2. Bridging Systems**\n- Algorithms that promote content bringing people together rather than showing divisive posts that drive engagement.\n\n**3. Community-Driven Moderation**\n- AI tools to help volunteer moderators manage online communities more effectively while keeping humans in control.\n\n**4. Proof of Humanity Systems**\n- Methods to verify that real humans (not bots) are participating in online discussions as AI makes fake accounts easier to create.",
        "size": 31.42857142857143,
        "index": 34,
        "x": 1423.1375195527432,
        "y": 867.5196560102631,
        "vy": -1.275706555599965,
        "vx": 0.22941699888881384
      },
      "target": {
        "id": "Felix Sieker",
        "title": "Felix Sieker",
        "tags": [
          "person"
        ],
        "content": "#person",
        "size": 16.571428571428573,
        "index": 79,
        "x": 1522.6148975381761,
        "y": 930.6143108641814,
        "vy": -1.4382298407449576,
        "vx": 0.3379654291316875
      },
      "index": 55
    },
    {
      "source": {
        "id": "AI and the Future of Digital Public Squares",
        "title": "AI and the Future of Digital Public Squares",
        "tags": [
          "project"
        ],
        "content": "#project \n\nhttps://arxiv.org/pdf/2412.09988\n\nPaper examines how LLMs can improve online discussions and democratic participation. \n\n## Four Main Areas of Focus\n\n**1. Collective Dialogue Systems**\n- Tools that help large groups have structured conversations and find common ground on important issues.\n\n**2. Bridging Systems**\n- Algorithms that promote content bringing people together rather than showing divisive posts that drive engagement.\n\n**3. Community-Driven Moderation**\n- AI tools to help volunteer moderators manage online communities more effectively while keeping humans in control.\n\n**4. Proof of Humanity Systems**\n- Methods to verify that real humans (not bots) are participating in online discussions as AI makes fake accounts easier to create.",
        "size": 31.42857142857143,
        "index": 34,
        "x": 1423.1375195527432,
        "y": 867.5196560102631,
        "vy": -1.275706555599965,
        "vx": 0.22941699888881384
      },
      "target": {
        "id": "Christopher Small",
        "title": "Christopher Small",
        "tags": [
          "person"
        ],
        "content": "#person \n\nhttp://metasoarous.com/",
        "size": 18.285714285714285,
        "index": 290,
        "x": 1854.4692018669803,
        "y": 861.0514458806144,
        "vy": -1.5207937741217539,
        "vx": 0.3624142296931689
      },
      "index": 56
    },
    {
      "source": {
        "id": "AI and the Future of Digital Public Squares",
        "title": "AI and the Future of Digital Public Squares",
        "tags": [
          "project"
        ],
        "content": "#project \n\nhttps://arxiv.org/pdf/2412.09988\n\nPaper examines how LLMs can improve online discussions and democratic participation. \n\n## Four Main Areas of Focus\n\n**1. Collective Dialogue Systems**\n- Tools that help large groups have structured conversations and find common ground on important issues.\n\n**2. Bridging Systems**\n- Algorithms that promote content bringing people together rather than showing divisive posts that drive engagement.\n\n**3. Community-Driven Moderation**\n- AI tools to help volunteer moderators manage online communities more effectively while keeping humans in control.\n\n**4. Proof of Humanity Systems**\n- Methods to verify that real humans (not bots) are participating in online discussions as AI makes fake accounts easier to create.",
        "size": 31.42857142857143,
        "index": 34,
        "x": 1423.1375195527432,
        "y": 867.5196560102631,
        "vy": -1.275706555599965,
        "vx": 0.22941699888881384
      },
      "target": {
        "id": "Jonathan Stray",
        "title": "Jonathan Stray",
        "tags": [
          "person"
        ],
        "content": "#person \n\nhttp://jonathanstray.com/",
        "size": 17.142857142857142,
        "index": 4,
        "x": 1557.4840034772917,
        "y": 985.1290569276915,
        "vy": -1.2999813278401229,
        "vx": 0.2968571996859188
      },
      "index": 57
    },
    {
      "source": {
        "id": "AI and the Future of Digital Public Squares",
        "title": "AI and the Future of Digital Public Squares",
        "tags": [
          "project"
        ],
        "content": "#project \n\nhttps://arxiv.org/pdf/2412.09988\n\nPaper examines how LLMs can improve online discussions and democratic participation. \n\n## Four Main Areas of Focus\n\n**1. Collective Dialogue Systems**\n- Tools that help large groups have structured conversations and find common ground on important issues.\n\n**2. Bridging Systems**\n- Algorithms that promote content bringing people together rather than showing divisive posts that drive engagement.\n\n**3. Community-Driven Moderation**\n- AI tools to help volunteer moderators manage online communities more effectively while keeping humans in control.\n\n**4. Proof of Humanity Systems**\n- Methods to verify that real humans (not bots) are participating in online discussions as AI makes fake accounts easier to create.",
        "size": 31.42857142857143,
        "index": 34,
        "x": 1423.1375195527432,
        "y": 867.5196560102631,
        "vy": -1.275706555599965,
        "vx": 0.22941699888881384
      },
      "target": {
        "id": "Audrey Tang",
        "title": "Audrey Tang",
        "tags": [
          "person"
        ],
        "content": "#person \n\nhttps://github.com/audreyt",
        "size": 18.857142857142858,
        "index": 80,
        "x": 1086.0447800074567,
        "y": 719.5887259305994,
        "vy": -1.393226340879792,
        "vx": 0.1542153667232109
      },
      "index": 58
    },
    {
      "source": {
        "id": "AI and the Future of Digital Public Squares",
        "title": "AI and the Future of Digital Public Squares",
        "tags": [
          "project"
        ],
        "content": "#project \n\nhttps://arxiv.org/pdf/2412.09988\n\nPaper examines how LLMs can improve online discussions and democratic participation. \n\n## Four Main Areas of Focus\n\n**1. Collective Dialogue Systems**\n- Tools that help large groups have structured conversations and find common ground on important issues.\n\n**2. Bridging Systems**\n- Algorithms that promote content bringing people together rather than showing divisive posts that drive engagement.\n\n**3. Community-Driven Moderation**\n- AI tools to help volunteer moderators manage online communities more effectively while keeping humans in control.\n\n**4. Proof of Humanity Systems**\n- Methods to verify that real humans (not bots) are participating in online discussions as AI makes fake accounts easier to create.",
        "size": 31.42857142857143,
        "index": 34,
        "x": 1423.1375195527432,
        "y": 867.5196560102631,
        "vy": -1.275706555599965,
        "vx": 0.22941699888881384
      },
      "target": {
        "id": "Michael Henry Tessler",
        "title": "Michael Henry Tessler",
        "tags": [
          "person"
        ],
        "content": "#person\n\nhttps://www.mit.edu/~tessler/",
        "size": 18.285714285714285,
        "index": 426,
        "x": 1312.71187781098,
        "y": 660.8616297480344,
        "vy": -1.3375108366075392,
        "vx": 0.285090803595964
      },
      "index": 59
    },
    {
      "source": {
        "id": "AI and the Future of Digital Public Squares",
        "title": "AI and the Future of Digital Public Squares",
        "tags": [
          "project"
        ],
        "content": "#project \n\nhttps://arxiv.org/pdf/2412.09988\n\nPaper examines how LLMs can improve online discussions and democratic participation. \n\n## Four Main Areas of Focus\n\n**1. Collective Dialogue Systems**\n- Tools that help large groups have structured conversations and find common ground on important issues.\n\n**2. Bridging Systems**\n- Algorithms that promote content bringing people together rather than showing divisive posts that drive engagement.\n\n**3. Community-Driven Moderation**\n- AI tools to help volunteer moderators manage online communities more effectively while keeping humans in control.\n\n**4. Proof of Humanity Systems**\n- Methods to verify that real humans (not bots) are participating in online discussions as AI makes fake accounts easier to create.",
        "size": 31.42857142857143,
        "index": 34,
        "x": 1423.1375195527432,
        "y": 867.5196560102631,
        "vy": -1.275706555599965,
        "vx": 0.22941699888881384
      },
      "target": {
        "id": "Amy Zhang",
        "title": "Amy Zhang",
        "tags": [
          "person"
        ],
        "content": "#person \n\nhttps://amyzhang.github.io/",
        "size": 17.714285714285715,
        "index": 8,
        "x": 1296.67931470172,
        "y": 1424.3976391045085,
        "vy": -1.3183531755494673,
        "vx": 0.6189876232804888
      },
      "index": 60
    },
    {
      "source": {
        "id": "Daniel P. Hogan",
        "title": "Daniel P. Hogan",
        "tags": [
          "person"
        ],
        "content": "#person",
        "size": 17.142857142857142,
        "index": 35,
        "x": -1507.8716164354419,
        "y": -2027.8242143512912,
        "vy": -0.31717839320734975,
        "vx": -0.5029117729535975
      },
      "target": {
        "id": "IQT Labs",
        "title": "IQT Labs",
        "tags": [
          "organization"
        ],
        "content": "#organization \n\nhttps://www.iqt.org/",
        "size": 17.142857142857142,
        "index": 266,
        "x": -1501.088614337867,
        "y": -2102.1091151589467,
        "vy": -0.31956553476439103,
        "vx": -0.515751933490656
      },
      "index": 61
    },
    {
      "source": {
        "id": "Recursive Public",
        "title": "Recursive Public",
        "tags": [
          "project"
        ],
        "content": "#project \n\nhttps://vtaiwan-openai-2023.vercel.app/Report_%20Recursive%20Public.pdf\n\nPilot democratic deliberation system designed to gather public input on AI governance decisions through cascading online discussions and AI-powered analysis. The project used platforms like [Polis](https://pol.is) and [Talk to the City](https://ai.objectives.institute/talk-to-the-city) to engage over 1,000 participants across different communities in identifying AI governance priorities, then employed LLMs to analyze consensus and disagreement patterns in the discussions.\n\nFunded by [OpenAI Democratic Inputs to AI](https://github.com/openai/democratic-inputs).",
        "size": 21.714285714285715,
        "index": 36,
        "x": 835.0730612401152,
        "y": -42.327038893972045,
        "vy": -1.2017478297377246,
        "vx": -0.0014674486757292548
      },
      "target": {
        "id": "vTaiwan & g0v",
        "title": "vTaiwan & g0v",
        "tags": [
          "organization"
        ],
        "content": "#organization \n\nhttps://info.vtaiwan.tw/\nhttps://g0v.tw/",
        "size": 18.285714285714285,
        "index": 389,
        "x": 918.308649354897,
        "y": 190.9842796861607,
        "vy": -1.2371396245854598,
        "vx": 0.056319588048500874
      },
      "index": 62
    },
    {
      "source": {
        "id": "Recursive Public",
        "title": "Recursive Public",
        "tags": [
          "project"
        ],
        "content": "#project \n\nhttps://vtaiwan-openai-2023.vercel.app/Report_%20Recursive%20Public.pdf\n\nPilot democratic deliberation system designed to gather public input on AI governance decisions through cascading online discussions and AI-powered analysis. The project used platforms like [Polis](https://pol.is) and [Talk to the City](https://ai.objectives.institute/talk-to-the-city) to engage over 1,000 participants across different communities in identifying AI governance priorities, then employed LLMs to analyze consensus and disagreement patterns in the discussions.\n\nFunded by [OpenAI Democratic Inputs to AI](https://github.com/openai/democratic-inputs).",
        "size": 21.714285714285715,
        "index": 36,
        "x": 835.0730612401152,
        "y": -42.327038893972045,
        "vy": -1.2017478297377246,
        "vx": -0.0014674486757292548
      },
      "target": {
        "id": "AI Objectives Institute",
        "title": "AI Objectives Institute",
        "tags": [
          "organization"
        ],
        "content": "#organization \n\nhttps://ai.objectives.institute/",
        "size": 20.57142857142857,
        "index": 304,
        "x": 665.7615191436854,
        "y": -247.07081557119588,
        "vy": -1.1515300453530217,
        "vx": -0.07444975689524554
      },
      "index": 63
    },
    {
      "source": {
        "id": "Recursive Public",
        "title": "Recursive Public",
        "tags": [
          "project"
        ],
        "content": "#project \n\nhttps://vtaiwan-openai-2023.vercel.app/Report_%20Recursive%20Public.pdf\n\nPilot democratic deliberation system designed to gather public input on AI governance decisions through cascading online discussions and AI-powered analysis. The project used platforms like [Polis](https://pol.is) and [Talk to the City](https://ai.objectives.institute/talk-to-the-city) to engage over 1,000 participants across different communities in identifying AI governance priorities, then employed LLMs to analyze consensus and disagreement patterns in the discussions.\n\nFunded by [OpenAI Democratic Inputs to AI](https://github.com/openai/democratic-inputs).",
        "size": 21.714285714285715,
        "index": 36,
        "x": 835.0730612401152,
        "y": -42.327038893972045,
        "vy": -1.2017478297377246,
        "vx": -0.0014674486757292548
      },
      "target": {
        "id": "Flynn Devine",
        "title": "Flynn Devine",
        "tags": [
          "person"
        ],
        "content": "#person",
        "size": 16.571428571428573,
        "index": 278,
        "x": 847.9493191426627,
        "y": -152.40763438745594,
        "vy": -1.2309818575341844,
        "vx": -0.003195005040643384
      },
      "index": 64
    },
    {
      "source": {
        "id": "Recursive Public",
        "title": "Recursive Public",
        "tags": [
          "project"
        ],
        "content": "#project \n\nhttps://vtaiwan-openai-2023.vercel.app/Report_%20Recursive%20Public.pdf\n\nPilot democratic deliberation system designed to gather public input on AI governance decisions through cascading online discussions and AI-powered analysis. The project used platforms like [Polis](https://pol.is) and [Talk to the City](https://ai.objectives.institute/talk-to-the-city) to engage over 1,000 participants across different communities in identifying AI governance priorities, then employed LLMs to analyze consensus and disagreement patterns in the discussions.\n\nFunded by [OpenAI Democratic Inputs to AI](https://github.com/openai/democratic-inputs).",
        "size": 21.714285714285715,
        "index": 36,
        "x": 835.0730612401152,
        "y": -42.327038893972045,
        "vy": -1.2017478297377246,
        "vx": -0.0014674486757292548
      },
      "target": {
        "id": "Alex Krasodomski-Jones",
        "title": "Alex Krasodomski-Jones",
        "tags": [
          "person"
        ],
        "content": "#person",
        "size": 16.571428571428573,
        "index": 225,
        "x": 795.8151217612628,
        "y": -129.01254009002102,
        "vy": -1.1997375139986641,
        "vx": 0.010816790502020623
      },
      "index": 65
    },
    {
      "source": {
        "id": "Recursive Public",
        "title": "Recursive Public",
        "tags": [
          "project"
        ],
        "content": "#project \n\nhttps://vtaiwan-openai-2023.vercel.app/Report_%20Recursive%20Public.pdf\n\nPilot democratic deliberation system designed to gather public input on AI governance decisions through cascading online discussions and AI-powered analysis. The project used platforms like [Polis](https://pol.is) and [Talk to the City](https://ai.objectives.institute/talk-to-the-city) to engage over 1,000 participants across different communities in identifying AI governance priorities, then employed LLMs to analyze consensus and disagreement patterns in the discussions.\n\nFunded by [OpenAI Democratic Inputs to AI](https://github.com/openai/democratic-inputs).",
        "size": 21.714285714285715,
        "index": 36,
        "x": 835.0730612401152,
        "y": -42.327038893972045,
        "vy": -1.2017478297377246,
        "vx": -0.0014674486757292548
      },
      "target": {
        "id": "Carl Miller",
        "title": "Carl Miller",
        "tags": [
          "person"
        ],
        "content": "#person \n\nhttps://www.carlmiller.co/",
        "size": 17.142857142857142,
        "index": 374,
        "x": 1090.9638718162498,
        "y": 350.8267083816078,
        "vy": -1.144654368229556,
        "vx": 0.15916945181270295
      },
      "index": 66
    },
    {
      "source": {
        "id": "Recursive Public",
        "title": "Recursive Public",
        "tags": [
          "project"
        ],
        "content": "#project \n\nhttps://vtaiwan-openai-2023.vercel.app/Report_%20Recursive%20Public.pdf\n\nPilot democratic deliberation system designed to gather public input on AI governance decisions through cascading online discussions and AI-powered analysis. The project used platforms like [Polis](https://pol.is) and [Talk to the City](https://ai.objectives.institute/talk-to-the-city) to engage over 1,000 participants across different communities in identifying AI governance priorities, then employed LLMs to analyze consensus and disagreement patterns in the discussions.\n\nFunded by [OpenAI Democratic Inputs to AI](https://github.com/openai/democratic-inputs).",
        "size": 21.714285714285715,
        "index": 36,
        "x": 835.0730612401152,
        "y": -42.327038893972045,
        "vy": -1.2017478297377246,
        "vx": -0.0014674486757292548
      },
      "target": {
        "id": "Shu Yang Lin",
        "title": "Shu Yang Lin",
        "tags": [
          "person"
        ],
        "content": "#person \n\nhttps://www.shuyanglin.com/",
        "size": 18.285714285714285,
        "index": 241,
        "x": 771.7824733892007,
        "y": -32.82433362604927,
        "vy": -1.1943655944737657,
        "vx": -0.00035972909812223005
      },
      "index": 67
    },
    {
      "source": {
        "id": "Recursive Public",
        "title": "Recursive Public",
        "tags": [
          "project"
        ],
        "content": "#project \n\nhttps://vtaiwan-openai-2023.vercel.app/Report_%20Recursive%20Public.pdf\n\nPilot democratic deliberation system designed to gather public input on AI governance decisions through cascading online discussions and AI-powered analysis. The project used platforms like [Polis](https://pol.is) and [Talk to the City](https://ai.objectives.institute/talk-to-the-city) to engage over 1,000 participants across different communities in identifying AI governance priorities, then employed LLMs to analyze consensus and disagreement patterns in the discussions.\n\nFunded by [OpenAI Democratic Inputs to AI](https://github.com/openai/democratic-inputs).",
        "size": 21.714285714285715,
        "index": 36,
        "x": 835.0730612401152,
        "y": -42.327038893972045,
        "vy": -1.2017478297377246,
        "vx": -0.0014674486757292548
      },
      "target": {
        "id": "Jia-Wei Cui",
        "title": "Jia-Wei Cui",
        "tags": [
          "person"
        ],
        "content": "#person",
        "size": 17.142857142857142,
        "index": 84,
        "x": 884.4669013720609,
        "y": 83.069990629577,
        "vy": -1.2311162749898688,
        "vx": 0.029421145525977563
      },
      "index": 68
    },
    {
      "source": {
        "id": "Recursive Public",
        "title": "Recursive Public",
        "tags": [
          "project"
        ],
        "content": "#project \n\nhttps://vtaiwan-openai-2023.vercel.app/Report_%20Recursive%20Public.pdf\n\nPilot democratic deliberation system designed to gather public input on AI governance decisions through cascading online discussions and AI-powered analysis. The project used platforms like [Polis](https://pol.is) and [Talk to the City](https://ai.objectives.institute/talk-to-the-city) to engage over 1,000 participants across different communities in identifying AI governance priorities, then employed LLMs to analyze consensus and disagreement patterns in the discussions.\n\nFunded by [OpenAI Democratic Inputs to AI](https://github.com/openai/democratic-inputs).",
        "size": 21.714285714285715,
        "index": 36,
        "x": 835.0730612401152,
        "y": -42.327038893972045,
        "vy": -1.2017478297377246,
        "vx": -0.0014674486757292548
      },
      "target": {
        "id": "Bruno Marnette",
        "title": "Bruno Marnette",
        "tags": [
          "person"
        ],
        "content": "#person \n\nhttps://brunomarnette.substack.com/",
        "size": 17.714285714285715,
        "index": 129,
        "x": 731.6434970566758,
        "y": -167.79134490423854,
        "vy": -1.1638735314984259,
        "vx": -0.004925001736821977
      },
      "index": 69
    },
    {
      "source": {
        "id": "Recursive Public",
        "title": "Recursive Public",
        "tags": [
          "project"
        ],
        "content": "#project \n\nhttps://vtaiwan-openai-2023.vercel.app/Report_%20Recursive%20Public.pdf\n\nPilot democratic deliberation system designed to gather public input on AI governance decisions through cascading online discussions and AI-powered analysis. The project used platforms like [Polis](https://pol.is) and [Talk to the City](https://ai.objectives.institute/talk-to-the-city) to engage over 1,000 participants across different communities in identifying AI governance priorities, then employed LLMs to analyze consensus and disagreement patterns in the discussions.\n\nFunded by [OpenAI Democratic Inputs to AI](https://github.com/openai/democratic-inputs).",
        "size": 21.714285714285715,
        "index": 36,
        "x": 835.0730612401152,
        "y": -42.327038893972045,
        "vy": -1.2017478297377246,
        "vx": -0.0014674486757292548
      },
      "target": {
        "id": "Rowan Wilkinson",
        "title": "Rowan Wilkinson",
        "tags": [
          "person"
        ],
        "content": "#person",
        "size": 16.571428571428573,
        "index": 37,
        "x": 880.6122452149882,
        "y": -105.83090148661746,
        "vy": -1.21473457321538,
        "vx": -0.02462268508914891
      },
      "index": 70
    },
    {
      "source": {
        "id": "Snow Globe",
        "title": "Snow Globe",
        "tags": [
          "project"
        ],
        "content": "#project \n\nhttps://github.com/IQTLabs/snowglobe\nhttps://arxiv.org/pdf/2404.11446\n\nSystem that uses LLMs to automate qualitative wargames - open-ended strategic simulations where participants respond with natural language rather than choosing from predefined moves. The system can simulate crisis scenarios like AI incidents or geopolitical conflicts, allowing multiple AI agents to play different roles while a control agent moderates and adjudicates outcomes.",
        "size": 17.714285714285715,
        "index": 38,
        "x": -1567.358358192519,
        "y": -1969.309216161687,
        "vy": -0.3032097371136398,
        "vx": -0.49256209457360906
      },
      "target": {
        "id": "Daniel P. Hogan",
        "title": "Daniel P. Hogan",
        "tags": [
          "person"
        ],
        "content": "#person",
        "size": 17.142857142857142,
        "index": 35,
        "x": -1507.8716164354419,
        "y": -2027.8242143512912,
        "vy": -0.31717839320734975,
        "vx": -0.5029117729535975
      },
      "index": 71
    },
    {
      "source": {
        "id": "Snow Globe",
        "title": "Snow Globe",
        "tags": [
          "project"
        ],
        "content": "#project \n\nhttps://github.com/IQTLabs/snowglobe\nhttps://arxiv.org/pdf/2404.11446\n\nSystem that uses LLMs to automate qualitative wargames - open-ended strategic simulations where participants respond with natural language rather than choosing from predefined moves. The system can simulate crisis scenarios like AI incidents or geopolitical conflicts, allowing multiple AI agents to play different roles while a control agent moderates and adjudicates outcomes.",
        "size": 17.714285714285715,
        "index": 38,
        "x": -1567.358358192519,
        "y": -1969.309216161687,
        "vy": -0.3032097371136398,
        "vx": -0.49256209457360906
      },
      "target": {
        "id": "Andrea Brennen",
        "title": "Andrea Brennen",
        "tags": [
          "person"
        ],
        "content": "#person",
        "size": 17.142857142857142,
        "index": 184,
        "x": -1557.6993347002503,
        "y": -2058.063640049635,
        "vy": -0.3084296035603935,
        "vx": -0.5082222765103739
      },
      "index": 72
    },
    {
      "source": {
        "id": "Owain Evans",
        "title": "Owain Evans",
        "tags": [
          "person"
        ],
        "content": "#person \n\nhttps://owainevans.github.io/",
        "size": 17.714285714285715,
        "index": 39,
        "x": -1372.1944531749268,
        "y": 477.9869800887734,
        "vy": -0.29993178439518847,
        "vx": 0.18951433178696656
      },
      "target": {
        "id": "Ought",
        "title": "Ought",
        "tags": [
          "organization"
        ],
        "content": "#organization \n\nhttps://ought.org/ (website is old)\n\nOught 1.0 has effectively become what is now Elicit (the company) continuing development on Elicit (the tool). Ought 2.0 has not yet done any public work.",
        "size": 23.42857142857143,
        "index": 306,
        "x": -1888.3722954977052,
        "y": 453.0511056583274,
        "vy": -0.2547035865836596,
        "vx": 0.0471838457037637
      },
      "index": 73
    },
    {
      "source": {
        "id": "Forecasting",
        "title": "Forecasting",
        "tags": [
          "topic"
        ],
        "content": "#topic",
        "size": 26.285714285714285,
        "index": 40,
        "x": -747.4431928673606,
        "y": -323.2770515513226,
        "vy": -0.3626857386405729,
        "vx": -0.021889425803130535
      },
      "target": {
        "id": "Forecasting Future World Events with Neural Networks",
        "title": "Forecasting Future World Events with Neural Networks",
        "tags": [
          "project"
        ],
        "content": "#project \n\nhttps://arxiv.org/pdf/2206.15474\n\nIntroduces \"Autocast,\" a dataset and benchmark for evaluating neural networks' ability to forecast future world events using thousands of real forecasting questions from public tournaments. The dataset tests language models on diverse topics like politics, economics, and science by simulating historical forecasting conditions—providing only past news articles to prevent information leakage from the future.\n\n(research from 2022)",
        "size": 22.285714285714285,
        "index": 175,
        "x": -844.9675993592969,
        "y": 476.331173079975,
        "vy": -0.410662757686534,
        "vx": 0.22624753369910494
      },
      "type": "topic_link",
      "index": 74
    },
    {
      "source": {
        "id": "Forecasting",
        "title": "Forecasting",
        "tags": [
          "topic"
        ],
        "content": "#topic",
        "size": 26.285714285714285,
        "index": 40,
        "x": -747.4431928673606,
        "y": -323.2770515513226,
        "vy": -0.3626857386405729,
        "vx": -0.021889425803130535
      },
      "target": {
        "id": "Approaching Human-Level Forecasting with Language Models",
        "title": "Approaching Human-Level Forecasting with Language Models",
        "tags": [
          "project"
        ],
        "content": "#project \n\nhttps://arxiv.org/pdf/2402.18563\n\nLLM system forecasting on competitive forecasting platforms by combining news retrieval, structured reasoning, and fine-tuning techniques to predict binary outcomes. The system automates the traditional forecasting process through three key components: \n1. retrieving relevant information from news sources,\n2. generating reasoned predictions, and \n3. aggregating multiple forecasts into a final prediction.",
        "size": 18.857142857142858,
        "index": 75,
        "x": -511.9886221542186,
        "y": 220.05320773975464,
        "vy": -0.5307874737070879,
        "vx": 0.15841345629537942
      },
      "type": "topic_link",
      "index": 75
    },
    {
      "source": {
        "id": "Forecasting",
        "title": "Forecasting",
        "tags": [
          "topic"
        ],
        "content": "#topic",
        "size": 26.285714285714285,
        "index": 40,
        "x": -747.4431928673606,
        "y": -323.2770515513226,
        "vy": -0.3626857386405729,
        "vx": -0.021889425803130535
      },
      "target": {
        "id": "ForecastBench - A Dynamic Benchmark of AI Forecasting Capabilities",
        "title": "ForecastBench - A Dynamic Benchmark of AI Forecasting Capabilities",
        "tags": [
          "project"
        ],
        "content": "#project \n\nhttps://arxiv.org/pdf/2409.19839\nhttps://www.forecastbench.org/\n\nBenchmark system that continuously evaluates AI forecasting capabilities using 1,000 automatically generated questions about future events that update in real-time. The system avoids data contamination by using only questions about genuinely unresolved future events, and it compares LLM performance against human forecasters including \"superforecasters\" who have proven track records. Research found that expert human forecasters significantly outperform the best-performing LLMs (like Claude 3.5 Sonnet).",
        "size": 20.57142857142857,
        "index": 176,
        "x": -256.173907996596,
        "y": 55.060340516367795,
        "vy": -0.6658430423286233,
        "vx": 0.1258733673270566
      },
      "type": "topic_link",
      "index": 76
    },
    {
      "source": {
        "id": "Forecasting",
        "title": "Forecasting",
        "tags": [
          "topic"
        ],
        "content": "#topic",
        "size": 26.285714285714285,
        "index": 40,
        "x": -747.4431928673606,
        "y": -323.2770515513226,
        "vy": -0.3626857386405729,
        "vx": -0.021889425803130535
      },
      "target": {
        "id": "AI-Augmented Predictions - LLM Assistants Improve Human Forecasting Accuracy",
        "title": "AI-Augmented Predictions - LLM Assistants Improve Human Forecasting Accuracy",
        "tags": [
          "project"
        ],
        "content": "#project \n\nhttps://arxiv.org/abs/2402.07862\n\nThis paper studies how LLMs can serve as assistants to improve human forecasting accuracy, testing two types of assistants - one designed to provide high-quality \"superforecasting\" advice and another designed to be overconfident and noisy. In a study with 991 participants answering forecasting questions, both LLM assistants significantly enhanced human prediction accuracy compared to controls, with the superforecasting assistant showing up to 41% improvement in some analyses.",
        "size": 19.428571428571427,
        "index": 113,
        "x": -185.16606034019384,
        "y": -153.3063209812553,
        "vy": -0.7719172339969845,
        "vx": 0.09169564090059472
      },
      "type": "topic_link",
      "index": 77
    },
    {
      "source": {
        "id": "Forecasting",
        "title": "Forecasting",
        "tags": [
          "topic"
        ],
        "content": "#topic",
        "size": 26.285714285714285,
        "index": 40,
        "x": -747.4431928673606,
        "y": -323.2770515513226,
        "vy": -0.3626857386405729,
        "vx": -0.021889425803130535
      },
      "target": {
        "id": "Wisdom of the silicon crowd - LLM ensemble prediction capabilities rival human crowd accuracy",
        "title": "Wisdom of the silicon crowd - LLM ensemble prediction capabilities rival human crowd accuracy",
        "tags": [
          "project"
        ],
        "content": "#project \n\nhttps://www.science.org/doi/pdf/10.1126/sciadv.adp1528\n\nPaper demonstrates that an ensemble of 12 different large language models can achieve forecasting accuracy that rivals human crowd predictions in real-world forecasting tournaments. The study shows that while individual LLMs typically underperform human crowds, aggregating predictions from multiple LLMs creates a \"wisdom of the silicon crowd\" effect that matches human accuracy on 31 binary forecasting questions.",
        "size": 19.428571428571427,
        "index": 281,
        "x": -292.04779945167223,
        "y": -239.92132255230587,
        "vy": -0.7213623615049221,
        "vx": 0.039225809807466795
      },
      "type": "topic_link",
      "index": 78
    },
    {
      "source": {
        "id": "Forecasting",
        "title": "Forecasting",
        "tags": [
          "topic"
        ],
        "content": "#topic",
        "size": 26.285714285714285,
        "index": 40,
        "x": -747.4431928673606,
        "y": -323.2770515513226,
        "vy": -0.3626857386405729,
        "vx": -0.021889425803130535
      },
      "target": {
        "id": "Prompt Engineering Large Language Models’ Forecasting Capabilities",
        "title": "Prompt Engineering Large Language Models’ Forecasting Capabilities",
        "tags": [
          "project"
        ],
        "content": "#project \n\nhttps://arxiv.org/pdf/2506.01578\n\nPaper presents a systematic study of prompt engineering techniques for improving large language models' forecasting capabilities. The researchers tested 38 different prompts across four major LLMs using 100 forecasting questions, followed by a second study with compound and professionally-designed prompts. Surprisingly, they found that most prompt modifications had negligible or even negative effects on forecasting accuracy, with some techniques like explicit Bayesian reasoning actually making predictions worse.",
        "size": 18.857142857142858,
        "index": 347,
        "x": -349.42584004628867,
        "y": -215.63530292055648,
        "vy": -0.6993735214505079,
        "vx": 0.0487528223764443
      },
      "type": "topic_link",
      "index": 79
    },
    {
      "source": {
        "id": "Forecasting",
        "title": "Forecasting",
        "tags": [
          "topic"
        ],
        "content": "#topic",
        "size": 26.285714285714285,
        "index": 40,
        "x": -747.4431928673606,
        "y": -323.2770515513226,
        "vy": -0.3626857386405729,
        "vx": -0.021889425803130535
      },
      "target": {
        "id": "LLMs Can Teach Themselves to Better Predict the Future",
        "title": "LLMs Can Teach Themselves to Better Predict the Future",
        "tags": [
          "project"
        ],
        "content": "#project \n\nhttps://arxiv.org/pdf/2502.05253\n\nMethod for improving large language models' forecasting capabilities through self-play and outcome-driven fine-tuning, where models generate multiple reasoning traces for prediction questions and learn from which approaches led to more accurate forecasts. The authors demonstrate that smaller models (14B parameters) fine-tuned with this method can achieve forecasting performance comparable to much larger frontier models like GPT-4o.",
        "size": 18.285714285714285,
        "index": 299,
        "x": -409.11929591810326,
        "y": -402.5919502022023,
        "vy": -0.6893254342018448,
        "vx": 0.08644409566424723
      },
      "type": "topic_link",
      "index": 80
    },
    {
      "source": {
        "id": "Forecasting",
        "title": "Forecasting",
        "tags": [
          "topic"
        ],
        "content": "#topic",
        "size": 26.285714285714285,
        "index": 40,
        "x": -747.4431928673606,
        "y": -323.2770515513226,
        "vy": -0.3626857386405729,
        "vx": -0.021889425803130535
      },
      "target": {
        "id": "Outcome-based Reinforcement Learning to Predict the Future",
        "title": "Outcome-based Reinforcement Learning to Predict the Future",
        "tags": [
          "project"
        ],
        "content": "#project \n\nhttps://www.lightningrod.ai/outcome-based-reinforcement-learning-to-predict-the-future\n\nPaper presents a method for training AI models to make better probabilistic forecasts about future events using reinforcement learning with outcome-based rewards. The researchers adapted existing reinforcement learning algorithms (like GRPO and ReMax) to work with the messy, delayed feedback that comes from real-world prediction tasks, using a dataset of over 100,000 forecasting questions from sources like Polymarket. Their approach produces a 14B parameter model that matches the accuracy of frontier models like OpenAI's o1 while achieving better calibration, ultimately translating this into higher hypothetical trading profits when betting against prediction markets.",
        "size": 19.428571428571427,
        "index": 199,
        "x": -378.65594725565177,
        "y": -456.26349804572686,
        "vy": -0.6998449710775491,
        "vx": 0.06796453288662563
      },
      "type": "topic_link",
      "index": 81
    },
    {
      "source": {
        "id": "Forecasting",
        "title": "Forecasting",
        "tags": [
          "topic"
        ],
        "content": "#topic",
        "size": 26.285714285714285,
        "index": 40,
        "x": -747.4431928673606,
        "y": -323.2770515513226,
        "vy": -0.3626857386405729,
        "vx": -0.021889425803130535
      },
      "target": {
        "id": "Foresight AI",
        "title": "Foresight AI",
        "tags": [
          "project"
        ],
        "content": "#project \n\nhttps://www.lightningrod.ai/foresight\n\nAI tool (with a free version!) which allows you to query the probability of some future event. The system will provide a probability, as well as a reasoning trace explaining how that probability was generated. The tool also gives some feedback about the quality of your question (e.g. clear resolution criteria), as well as offering alternate questions you might want to ask.",
        "size": 17.714285714285715,
        "index": 52,
        "x": -513.2013549868367,
        "y": -680.0646260910952,
        "vy": -0.34999568110961393,
        "vx": -0.17336173148325393
      },
      "type": "topic_link",
      "index": 82
    },
    {
      "source": {
        "id": "Forecasting",
        "title": "Forecasting",
        "tags": [
          "topic"
        ],
        "content": "#topic",
        "size": 26.285714285714285,
        "index": 40,
        "x": -747.4431928673606,
        "y": -323.2770515513226,
        "vy": -0.3626857386405729,
        "vx": -0.021889425803130535
      },
      "target": {
        "id": "AI Forecasting Benchmark",
        "title": "AI Forecasting Benchmark",
        "tags": [
          "project"
        ],
        "content": "#project \n\nhttps://www.metaculus.com/aib/\n\nMetaculus AIB (AI Benchmarking) is a series that benchmarks \"the state of the art in AI forecasting against the best humans on real-world questions.\"",
        "size": 20.57142857142857,
        "index": 178,
        "x": -491.69904780690194,
        "y": -761.3030318136879,
        "vy": -0.5357053744666,
        "vx": -0.2989940012828504
      },
      "type": "topic_link",
      "index": 83
    },
    {
      "source": {
        "id": "Forecasting",
        "title": "Forecasting",
        "tags": [
          "topic"
        ],
        "content": "#topic",
        "size": 26.285714285714285,
        "index": 40,
        "x": -747.4431928673606,
        "y": -323.2770515513226,
        "vy": -0.3626857386405729,
        "vx": -0.021889425803130535
      },
      "target": {
        "id": "Squiggle AI",
        "title": "Squiggle AI",
        "tags": [
          "project"
        ],
        "content": "#project\n\nhttps://www.lesswrong.com/posts/7worWgggeHL3Eb7wq/introducing-squiggle-ai\n\nTool that automatically generates probabilistic cost-effectiveness models using the Squiggle programming language, allowing users to quickly create Fermi estimates and decision analyses without needing to learn programming. It combines large language models with specialized scaffolding to produce structured, adjustable models for questions like comparing charitable interventions or evaluating policy decisions.",
        "size": 17.714285714285715,
        "index": 19,
        "x": -658.0983208874798,
        "y": -987.2815180985507,
        "vy": -0.45320317580470043,
        "vx": -0.35287377532763314
      },
      "type": "topic_link",
      "index": 84
    },
    {
      "source": {
        "id": "Forecasting",
        "title": "Forecasting",
        "tags": [
          "topic"
        ],
        "content": "#topic",
        "size": 26.285714285714285,
        "index": 40,
        "x": -747.4431928673606,
        "y": -323.2770515513226,
        "vy": -0.3626857386405729,
        "vx": -0.021889425803130535
      },
      "target": {
        "id": "My Current Claims and Cruxes on LLM Forecasting & Epistemics",
        "title": "My Current Claims and Cruxes on LLM Forecasting & Epistemics",
        "tags": [
          "project"
        ],
        "content": "#project \n\nhttps://forum.effectivealtruism.org/posts/EykCuXDCFAT5oGyux/my-current-claims-and-cruxes-on-llm-forecasting-and\n\nPost outlines a vision for LLM-based Epistemic Processes (LEPs) that could automate virtually all aspects of knowledge work, from gathering and synthesizing information to generating forecasts and presenting results to users. The author breaks down LEPs into 13 distinct components including data collection, world modeling, human elicitation, numeric modeling, decomposition/amplification techniques, and various forms of automated decision support. A central argument is that these systems will require substantial \"scaffolding\" (supporting software infrastructure) and will likely be dominated by centralized platforms rather than distributed individual forecasters, potentially replacing human participation in prediction markets like Manifold and Metaculus. The post also addresses potential risks including AI acceleration, misuse by malicious actors, and the challenge that these powerful epistemic tools might increase world complexity faster than they improve our ability to navigate it.",
        "size": 17.142857142857142,
        "index": 63,
        "x": -846.3985542892191,
        "y": -1145.8208936945189,
        "vy": -0.3938475639925841,
        "vx": -0.5266735180832066
      },
      "type": "topic_link",
      "index": 85
    },
    {
      "source": {
        "id": "Forecasting",
        "title": "Forecasting",
        "tags": [
          "topic"
        ],
        "content": "#topic",
        "size": 26.285714285714285,
        "index": 40,
        "x": -747.4431928673606,
        "y": -323.2770515513226,
        "vy": -0.3626857386405729,
        "vx": -0.021889425803130535
      },
      "target": {
        "id": "Metaforecast",
        "title": "Metaforecast",
        "tags": [
          "project"
        ],
        "content": "#project \n\nhttps://metaforecast.org\n\nSearch engine and aggregator for predictions and forecasts from various prediction markets and forecasting platforms, allowing users to search for probability estimates on topics like geopolitics, technology, and other events. It provides quality ratings for forecasts and tools to help users navigate and understand prediction data across multiple platforms.\n\n(not currently maintained)",
        "size": 17.714285714285715,
        "index": 160,
        "x": -911.2498873715613,
        "y": -1287.3762821910332,
        "vy": -0.22764532301028506,
        "vx": -0.5586516888668817
      },
      "type": "topic_link",
      "index": 86
    },
    {
      "source": {
        "id": "Forecasting",
        "title": "Forecasting",
        "tags": [
          "topic"
        ],
        "content": "#topic",
        "size": 26.285714285714285,
        "index": 40,
        "x": -747.4431928673606,
        "y": -323.2770515513226,
        "vy": -0.3626857386405729,
        "vx": -0.021889425803130535
      },
      "target": {
        "id": "Q1 AI Benchmarking results",
        "title": "Q1 AI Benchmarking results",
        "tags": [
          "project"
        ],
        "content": "#project \n\nhttps://www.metaculus.com/notebooks/38673/q1-ai-benchmarking-results/\n\nReport on Metaculus's quarterly tournament that compared AI forecasting bots against professional human forecasters on real-world prediction questions, with the key finding that professional forecasters significantly outperformed the best AI systems.",
        "size": 18.285714285714285,
        "index": 311,
        "x": -376.8873672015769,
        "y": -773.3612804866773,
        "vy": -0.6741199914635285,
        "vx": -0.2863978459087953
      },
      "type": "topic_link",
      "index": 87
    },
    {
      "source": {
        "id": "Forecasting",
        "title": "Forecasting",
        "tags": [
          "topic"
        ],
        "content": "#topic",
        "size": 26.285714285714285,
        "index": 40,
        "x": -747.4431928673606,
        "y": -323.2770515513226,
        "vy": -0.3626857386405729,
        "vx": -0.021889425803130535
      },
      "target": {
        "id": "Forecasting Tools",
        "title": "Forecasting Tools",
        "tags": [
          "project"
        ],
        "content": "#project \n\nhttps://github.com/Metaculus/forecasting-tools\n\nPython framework for building AI-powered forecasting systems that help humans predict future events more accurately.",
        "size": 17.142857142857142,
        "index": 367,
        "x": -267.1494960515555,
        "y": -872.7522764512429,
        "vy": -0.7244377138907087,
        "vx": -0.25078557148939884
      },
      "type": "topic_link",
      "index": 88
    },
    {
      "source": {
        "id": "Forecasting",
        "title": "Forecasting",
        "tags": [
          "topic"
        ],
        "content": "#topic",
        "size": 26.285714285714285,
        "index": 40,
        "x": -747.4431928673606,
        "y": -323.2770515513226,
        "vy": -0.3626857386405729,
        "vx": -0.021889425803130535
      },
      "target": {
        "id": "Bench to the Future - A Pastcasting Benchmark for Forecasting Agents",
        "title": "Bench to the Future - A Pastcasting Benchmark for Forecasting Agents",
        "tags": [
          "project"
        ],
        "content": "#project \n\nhttps://arxiv.org/abs/2506.21558\nhttps://evals.futuresearch.ai/\n\nEstablishes a \"pastcasting\" benchmark that evaluates AI forecasting capabilities by having LLMs predict outcomes of events that have already happened, using carefully curated web snapshots from before those events occurred. The benchmark addresses a key challenge in forecasting research - the long wait times between making predictions and learning outcomes.",
        "size": 21.142857142857142,
        "index": 383,
        "x": -731.0795440958706,
        "y": -611.1986198161612,
        "vy": -0.4275064604701327,
        "vx": -0.3280891561207337
      },
      "type": "topic_link",
      "index": 89
    },
    {
      "source": {
        "id": "Forecasting",
        "title": "Forecasting",
        "tags": [
          "topic"
        ],
        "content": "#topic",
        "size": 26.285714285714285,
        "index": 40,
        "x": -747.4431928673606,
        "y": -323.2770515513226,
        "vy": -0.3626857386405729,
        "vx": -0.021889425803130535
      },
      "target": {
        "id": "Human v Bots Forecasting Tournament 2024",
        "title": "Human v Bots Forecasting Tournament 2024",
        "tags": [
          "project"
        ],
        "content": "#project\n\nhttps://news.manifold.markets/p/human-v-bots-forecasting-tournament\n\nCompetition hosted by Manifold Markets, where human forecasters compete directly against AI systems to predict major world events throughout 2024, with winners determined by forecasting accuracy and profit.",
        "size": 17.714285714285715,
        "index": 100,
        "x": -783.1858281412245,
        "y": -810.106102551167,
        "vy": -0.47707926734133976,
        "vx": -0.32899427919111074
      },
      "type": "topic_link",
      "index": 90
    },
    {
      "source": {
        "id": "Forecasting",
        "title": "Forecasting",
        "tags": [
          "topic"
        ],
        "content": "#topic",
        "size": 26.285714285714285,
        "index": 40,
        "x": -747.4431928673606,
        "y": -323.2770515513226,
        "vy": -0.3626857386405729,
        "vx": -0.021889425803130535
      },
      "target": {
        "id": "Contra papers claiming superhuman AI forecasting",
        "title": "Contra papers claiming superhuman AI forecasting",
        "tags": [
          "project"
        ],
        "content": "#project \n\nhttps://www.lesswrong.com/posts/uGkRcHqatmPkvpGLq/contra-papers-claiming-superhuman-ai-forecasting\n\nCritique of recent academic papers that claim AI systems have achieved \"superhuman\" forecasting performance. The authors from FutureSearch systematically debunk these claims by identifying major methodological flaws, including inadequate information retrieval capabilities, data contamination, unfair timing advantages, and misleading statistical interpretations that make AI performance appear better than it actually is.",
        "size": 18.857142857142858,
        "index": 173,
        "x": -912.2504237409754,
        "y": -668.3597869352501,
        "vy": -0.3168233753959674,
        "vx": -0.337357800659079
      },
      "type": "topic_link",
      "index": 91
    },
    {
      "source": {
        "id": "Oliver Klingefjord",
        "title": "Oliver Klingefjord",
        "tags": [
          "person"
        ],
        "content": "#person \n\nhttps://www.klingefjord.com/",
        "size": 18.285714285714285,
        "index": 44,
        "x": 1178.5910932786057,
        "y": -246.8472025448904,
        "vy": -1.2893218733044343,
        "vx": -0.05174057759054609
      },
      "target": {
        "id": "Meaning Alignment Institute",
        "title": "Meaning Alignment Institute",
        "tags": [
          "organization"
        ],
        "content": "#organization \n\nhttps://www.meaningalignment.org/",
        "size": 17.714285714285715,
        "index": 310,
        "x": 1258.0435242093358,
        "y": -308.1817574200967,
        "vy": -1.311900518523638,
        "vx": -0.08522925964609504
      },
      "index": 92
    },
    {
      "source": {
        "id": "Scaffolding cooperation in human groups with deep reinforcement learning",
        "title": "Scaffolding cooperation in human groups with deep reinforcement learning",
        "tags": [
          "project"
        ],
        "content": "#project \n\nhttps://www.nature.com/articles/s41562-023-01686-7.pdf\n\nResearchers used deep reinforcement learning to train an AI \"social planner\" that makes recommendations about which connections to form or break in human social networks playing a cooperation game for real money. The AI learned a counterintuitive \"encouraging\" strategy: instead of isolating defectors from cooperators (the traditional approach), it placed defectors in small neighborhoods surrounded by cooperators, which led groups to achieve 77.7% cooperation rates compared to just 42.8% in static networks.",
        "size": 20,
        "index": 46,
        "x": 1318.1137871394092,
        "y": 721.7663063479347,
        "vy": -1.3121945961746093,
        "vx": 0.20719172369871136
      },
      "target": {
        "id": "Kevin McKee",
        "title": "Kevin McKee",
        "tags": [
          "person"
        ],
        "content": "#person",
        "size": 16.571428571428573,
        "index": 201,
        "x": 1426.397731730554,
        "y": 671.5137816131387,
        "vy": -1.4189210515063608,
        "vx": 0.22770911227723925
      },
      "index": 93
    },
    {
      "source": {
        "id": "Scaffolding cooperation in human groups with deep reinforcement learning",
        "title": "Scaffolding cooperation in human groups with deep reinforcement learning",
        "tags": [
          "project"
        ],
        "content": "#project \n\nhttps://www.nature.com/articles/s41562-023-01686-7.pdf\n\nResearchers used deep reinforcement learning to train an AI \"social planner\" that makes recommendations about which connections to form or break in human social networks playing a cooperation game for real money. The AI learned a counterintuitive \"encouraging\" strategy: instead of isolating defectors from cooperators (the traditional approach), it placed defectors in small neighborhoods surrounded by cooperators, which led groups to achieve 77.7% cooperation rates compared to just 42.8% in static networks.",
        "size": 20,
        "index": 46,
        "x": 1318.1137871394092,
        "y": 721.7663063479347,
        "vy": -1.3121945961746093,
        "vx": 0.20719172369871136
      },
      "target": {
        "id": "Andrea Tacchetti",
        "title": "Andrea Tacchetti",
        "tags": [
          "person"
        ],
        "content": "#person \n\nhttps://www.andreatacchetti.com/",
        "size": 17.142857142857142,
        "index": 86,
        "x": 1260.3557935086312,
        "y": 682.8647806521557,
        "vy": -1.342307008521067,
        "vx": 0.17121757576773058
      },
      "index": 94
    },
    {
      "source": {
        "id": "Scaffolding cooperation in human groups with deep reinforcement learning",
        "title": "Scaffolding cooperation in human groups with deep reinforcement learning",
        "tags": [
          "project"
        ],
        "content": "#project \n\nhttps://www.nature.com/articles/s41562-023-01686-7.pdf\n\nResearchers used deep reinforcement learning to train an AI \"social planner\" that makes recommendations about which connections to form or break in human social networks playing a cooperation game for real money. The AI learned a counterintuitive \"encouraging\" strategy: instead of isolating defectors from cooperators (the traditional approach), it placed defectors in small neighborhoods surrounded by cooperators, which led groups to achieve 77.7% cooperation rates compared to just 42.8% in static networks.",
        "size": 20,
        "index": 46,
        "x": 1318.1137871394092,
        "y": 721.7663063479347,
        "vy": -1.3121945961746093,
        "vx": 0.20719172369871136
      },
      "target": {
        "id": "Michiel Bakker",
        "title": "Michiel Bakker",
        "tags": [
          "person"
        ],
        "content": "#person \n\nhttps://miba.dev/",
        "size": 21.142857142857142,
        "index": 284,
        "x": 1225.501866244462,
        "y": 730.5079873536822,
        "vy": -1.2714527484014098,
        "vx": 0.27625785273055353
      },
      "index": 95
    },
    {
      "source": {
        "id": "Scaffolding cooperation in human groups with deep reinforcement learning",
        "title": "Scaffolding cooperation in human groups with deep reinforcement learning",
        "tags": [
          "project"
        ],
        "content": "#project \n\nhttps://www.nature.com/articles/s41562-023-01686-7.pdf\n\nResearchers used deep reinforcement learning to train an AI \"social planner\" that makes recommendations about which connections to form or break in human social networks playing a cooperation game for real money. The AI learned a counterintuitive \"encouraging\" strategy: instead of isolating defectors from cooperators (the traditional approach), it placed defectors in small neighborhoods surrounded by cooperators, which led groups to achieve 77.7% cooperation rates compared to just 42.8% in static networks.",
        "size": 20,
        "index": 46,
        "x": 1318.1137871394092,
        "y": 721.7663063479347,
        "vy": -1.3121945961746093,
        "vx": 0.20719172369871136
      },
      "target": {
        "id": "Jan Balaguer",
        "title": "Jan Balaguer",
        "tags": [
          "person"
        ],
        "content": "#person \n\nhttps://www.janbalaguer.net/",
        "size": 17.714285714285715,
        "index": 273,
        "x": 1227.3076588867907,
        "y": 638.3421386448723,
        "vy": -1.2872783959036884,
        "vx": 0.14224906351640465
      },
      "index": 96
    },
    {
      "source": {
        "id": "Scaffolding cooperation in human groups with deep reinforcement learning",
        "title": "Scaffolding cooperation in human groups with deep reinforcement learning",
        "tags": [
          "project"
        ],
        "content": "#project \n\nhttps://www.nature.com/articles/s41562-023-01686-7.pdf\n\nResearchers used deep reinforcement learning to train an AI \"social planner\" that makes recommendations about which connections to form or break in human social networks playing a cooperation game for real money. The AI learned a counterintuitive \"encouraging\" strategy: instead of isolating defectors from cooperators (the traditional approach), it placed defectors in small neighborhoods surrounded by cooperators, which led groups to achieve 77.7% cooperation rates compared to just 42.8% in static networks.",
        "size": 20,
        "index": 46,
        "x": 1318.1137871394092,
        "y": 721.7663063479347,
        "vy": -1.3121945961746093,
        "vx": 0.20719172369871136
      },
      "target": {
        "id": "Lucy Campbell-Gillingham",
        "title": "Lucy Campbell-Gillingham",
        "tags": [
          "person"
        ],
        "content": "#person",
        "size": 17.714285714285715,
        "index": 407,
        "x": 1356.6725194125038,
        "y": 620.0271145323594,
        "vy": -1.3493309176801664,
        "vx": 0.2741836448885554
      },
      "index": 97
    },
    {
      "source": {
        "id": "Scaffolding cooperation in human groups with deep reinforcement learning",
        "title": "Scaffolding cooperation in human groups with deep reinforcement learning",
        "tags": [
          "project"
        ],
        "content": "#project \n\nhttps://www.nature.com/articles/s41562-023-01686-7.pdf\n\nResearchers used deep reinforcement learning to train an AI \"social planner\" that makes recommendations about which connections to form or break in human social networks playing a cooperation game for real money. The AI learned a counterintuitive \"encouraging\" strategy: instead of isolating defectors from cooperators (the traditional approach), it placed defectors in small neighborhoods surrounded by cooperators, which led groups to achieve 77.7% cooperation rates compared to just 42.8% in static networks.",
        "size": 20,
        "index": 46,
        "x": 1318.1137871394092,
        "y": 721.7663063479347,
        "vy": -1.3121945961746093,
        "vx": 0.20719172369871136
      },
      "target": {
        "id": "Richard Everett",
        "title": "Richard Everett",
        "tags": [
          "person"
        ],
        "content": "#person",
        "size": 16.571428571428573,
        "index": 394,
        "x": 1393.5068024527425,
        "y": 728.0613824973879,
        "vy": -1.3493422537081925,
        "vx": 0.2634178457967685
      },
      "index": 98
    },
    {
      "source": {
        "id": "Scaffolding cooperation in human groups with deep reinforcement learning",
        "title": "Scaffolding cooperation in human groups with deep reinforcement learning",
        "tags": [
          "project"
        ],
        "content": "#project \n\nhttps://www.nature.com/articles/s41562-023-01686-7.pdf\n\nResearchers used deep reinforcement learning to train an AI \"social planner\" that makes recommendations about which connections to form or break in human social networks playing a cooperation game for real money. The AI learned a counterintuitive \"encouraging\" strategy: instead of isolating defectors from cooperators (the traditional approach), it placed defectors in small neighborhoods surrounded by cooperators, which led groups to achieve 77.7% cooperation rates compared to just 42.8% in static networks.",
        "size": 20,
        "index": 46,
        "x": 1318.1137871394092,
        "y": 721.7663063479347,
        "vy": -1.3121945961746093,
        "vx": 0.20719172369871136
      },
      "target": {
        "id": "Matthew Botvinick",
        "title": "Matthew Botvinick",
        "tags": [
          "person"
        ],
        "content": "#person",
        "size": 18.285714285714285,
        "index": 139,
        "x": 1370.3134606965014,
        "y": 676.0315589136434,
        "vy": -1.3768077843382622,
        "vx": 0.20959930595528592
      },
      "index": 99
    },
    {
      "source": {
        "id": "Language Agents as Digital Representatives in Collective Decision-Making",
        "title": "Language Agents as Digital Representatives in Collective Decision-Making",
        "tags": [
          "project"
        ],
        "content": "#project \n\nhttps://arxiv.org/pdf/2502.09369\n\nResearchers fine-tuned LLMs to simulate how specific people would participate in consensus-finding discussions, particularly in the critique phase where participants evaluate draft consensus statements.",
        "size": 21.142857142857142,
        "index": 47,
        "x": 1173.1295844688327,
        "y": 662.1306612244009,
        "vy": -1.1026831650692024,
        "vx": 0.2659418462369577
      },
      "target": {
        "id": "Daniel Jarrett",
        "title": "Daniel Jarrett",
        "tags": [
          "person"
        ],
        "content": "#person",
        "size": 17.142857142857142,
        "index": 58,
        "x": 1210.2578542148913,
        "y": 589.8451188793808,
        "vy": -1.3811626256136391,
        "vx": 0.1734083518435134
      },
      "index": 100
    },
    {
      "source": {
        "id": "Language Agents as Digital Representatives in Collective Decision-Making",
        "title": "Language Agents as Digital Representatives in Collective Decision-Making",
        "tags": [
          "project"
        ],
        "content": "#project \n\nhttps://arxiv.org/pdf/2502.09369\n\nResearchers fine-tuned LLMs to simulate how specific people would participate in consensus-finding discussions, particularly in the critique phase where participants evaluate draft consensus statements.",
        "size": 21.142857142857142,
        "index": 47,
        "x": 1173.1295844688327,
        "y": 662.1306612244009,
        "vy": -1.1026831650692024,
        "vx": 0.2659418462369577
      },
      "target": {
        "id": "Michael Henry Tessler",
        "title": "Michael Henry Tessler",
        "tags": [
          "person"
        ],
        "content": "#person\n\nhttps://www.mit.edu/~tessler/",
        "size": 18.285714285714285,
        "index": 426,
        "x": 1312.71187781098,
        "y": 660.8616297480344,
        "vy": -1.3375108366075392,
        "vx": 0.285090803595964
      },
      "index": 101
    },
    {
      "source": {
        "id": "Language Agents as Digital Representatives in Collective Decision-Making",
        "title": "Language Agents as Digital Representatives in Collective Decision-Making",
        "tags": [
          "project"
        ],
        "content": "#project \n\nhttps://arxiv.org/pdf/2502.09369\n\nResearchers fine-tuned LLMs to simulate how specific people would participate in consensus-finding discussions, particularly in the critique phase where participants evaluate draft consensus statements.",
        "size": 21.142857142857142,
        "index": 47,
        "x": 1173.1295844688327,
        "y": 662.1306612244009,
        "vy": -1.1026831650692024,
        "vx": 0.2659418462369577
      },
      "target": {
        "id": "Romuald Elie",
        "title": "Romuald Elie",
        "tags": [
          "person"
        ],
        "content": "#person",
        "size": 16.571428571428573,
        "index": 345,
        "x": 1102.9510065073475,
        "y": 584.5322563537535,
        "vy": -1.2776114979013207,
        "vx": 0.19044506347214676
      },
      "index": 102
    },
    {
      "source": {
        "id": "Language Agents as Digital Representatives in Collective Decision-Making",
        "title": "Language Agents as Digital Representatives in Collective Decision-Making",
        "tags": [
          "project"
        ],
        "content": "#project \n\nhttps://arxiv.org/pdf/2502.09369\n\nResearchers fine-tuned LLMs to simulate how specific people would participate in consensus-finding discussions, particularly in the critique phase where participants evaluate draft consensus statements.",
        "size": 21.142857142857142,
        "index": 47,
        "x": 1173.1295844688327,
        "y": 662.1306612244009,
        "vy": -1.1026831650692024,
        "vx": 0.2659418462369577
      },
      "target": {
        "id": "Miruna Pîslar",
        "title": "Miruna Pîslar",
        "tags": [
          "person"
        ],
        "content": "#person",
        "size": 16.571428571428573,
        "index": 397,
        "x": 1093.7369048431676,
        "y": 640.9273476860209,
        "vy": -1.2756818330944928,
        "vx": 0.20225048704212434
      },
      "index": 103
    },
    {
      "source": {
        "id": "Language Agents as Digital Representatives in Collective Decision-Making",
        "title": "Language Agents as Digital Representatives in Collective Decision-Making",
        "tags": [
          "project"
        ],
        "content": "#project \n\nhttps://arxiv.org/pdf/2502.09369\n\nResearchers fine-tuned LLMs to simulate how specific people would participate in consensus-finding discussions, particularly in the critique phase where participants evaluate draft consensus statements.",
        "size": 21.142857142857142,
        "index": 47,
        "x": 1173.1295844688327,
        "y": 662.1306612244009,
        "vy": -1.1026831650692024,
        "vx": 0.2659418462369577
      },
      "target": {
        "id": "Raphael Köster",
        "title": "Raphael Köster",
        "tags": [
          "person"
        ],
        "content": "#person",
        "size": 17.142857142857142,
        "index": 246,
        "x": 1250.5806382096266,
        "y": 546.7121239912451,
        "vy": -1.3626070407997648,
        "vx": 0.20076492715187425
      },
      "index": 104
    },
    {
      "source": {
        "id": "Language Agents as Digital Representatives in Collective Decision-Making",
        "title": "Language Agents as Digital Representatives in Collective Decision-Making",
        "tags": [
          "project"
        ],
        "content": "#project \n\nhttps://arxiv.org/pdf/2502.09369\n\nResearchers fine-tuned LLMs to simulate how specific people would participate in consensus-finding discussions, particularly in the critique phase where participants evaluate draft consensus statements.",
        "size": 21.142857142857142,
        "index": 47,
        "x": 1173.1295844688327,
        "y": 662.1306612244009,
        "vy": -1.1026831650692024,
        "vx": 0.2659418462369577
      },
      "target": {
        "id": "Christopher Summerfield",
        "title": "Christopher Summerfield",
        "tags": [
          "person"
        ],
        "content": "#person",
        "size": 18.285714285714285,
        "index": 318,
        "x": 1037.8359073408596,
        "y": 622.2526019294744,
        "vy": -1.2802118806311242,
        "vx": 0.2036645449820769
      },
      "index": 105
    },
    {
      "source": {
        "id": "Language Agents as Digital Representatives in Collective Decision-Making",
        "title": "Language Agents as Digital Representatives in Collective Decision-Making",
        "tags": [
          "project"
        ],
        "content": "#project \n\nhttps://arxiv.org/pdf/2502.09369\n\nResearchers fine-tuned LLMs to simulate how specific people would participate in consensus-finding discussions, particularly in the critique phase where participants evaluate draft consensus statements.",
        "size": 21.142857142857142,
        "index": 47,
        "x": 1173.1295844688327,
        "y": 662.1306612244009,
        "vy": -1.1026831650692024,
        "vx": 0.2659418462369577
      },
      "target": {
        "id": "Michiel Bakker",
        "title": "Michiel Bakker",
        "tags": [
          "person"
        ],
        "content": "#person \n\nhttps://miba.dev/",
        "size": 21.142857142857142,
        "index": 284,
        "x": 1225.501866244462,
        "y": 730.5079873536822,
        "vy": -1.2714527484014098,
        "vx": 0.27625785273055353
      },
      "index": 106
    },
    {
      "source": {
        "id": "Language Agents as Digital Representatives in Collective Decision-Making",
        "title": "Language Agents as Digital Representatives in Collective Decision-Making",
        "tags": [
          "project"
        ],
        "content": "#project \n\nhttps://arxiv.org/pdf/2502.09369\n\nResearchers fine-tuned LLMs to simulate how specific people would participate in consensus-finding discussions, particularly in the critique phase where participants evaluate draft consensus statements.",
        "size": 21.142857142857142,
        "index": 47,
        "x": 1173.1295844688327,
        "y": 662.1306612244009,
        "vy": -1.1026831650692024,
        "vx": 0.2659418462369577
      },
      "target": {
        "id": "Jan Balaguer",
        "title": "Jan Balaguer",
        "tags": [
          "person"
        ],
        "content": "#person \n\nhttps://www.janbalaguer.net/",
        "size": 17.714285714285715,
        "index": 273,
        "x": 1227.3076588867907,
        "y": 638.3421386448723,
        "vy": -1.2872783959036884,
        "vx": 0.14224906351640465
      },
      "index": 107
    },
    {
      "source": {
        "id": "Language Agents as Digital Representatives in Collective Decision-Making",
        "title": "Language Agents as Digital Representatives in Collective Decision-Making",
        "tags": [
          "project"
        ],
        "content": "#project \n\nhttps://arxiv.org/pdf/2502.09369\n\nResearchers fine-tuned LLMs to simulate how specific people would participate in consensus-finding discussions, particularly in the critique phase where participants evaluate draft consensus statements.",
        "size": 21.142857142857142,
        "index": 47,
        "x": 1173.1295844688327,
        "y": 662.1306612244009,
        "vy": -1.1026831650692024,
        "vx": 0.2659418462369577
      },
      "target": {
        "id": "Andrea Tacchetti",
        "title": "Andrea Tacchetti",
        "tags": [
          "person"
        ],
        "content": "#person \n\nhttps://www.andreatacchetti.com/",
        "size": 17.142857142857142,
        "index": 86,
        "x": 1260.3557935086312,
        "y": 682.8647806521557,
        "vy": -1.342307008521067,
        "vx": 0.17121757576773058
      },
      "index": 108
    },
    {
      "source": {
        "id": "Luke Thorburn",
        "title": "Luke Thorburn",
        "tags": [
          "person"
        ],
        "content": "#person",
        "size": 17.142857142857142,
        "index": 49,
        "x": 1650.4746137334355,
        "y": 709.4227495487538,
        "vy": -1.4702697462835792,
        "vx": 0.2532103757321741
      },
      "target": {
        "id": "AI & Democracy Foundation",
        "title": "AI & Democracy Foundation",
        "tags": [
          "organization"
        ],
        "content": "#organization \n\nhttps://aidemocracyfoundation.org/",
        "size": 17.714285714285715,
        "index": 432,
        "x": 1666.8768486198028,
        "y": 789.5155954205912,
        "vy": -1.4702217936721376,
        "vx": 0.26566951389858834
      },
      "index": 109
    },
    {
      "source": {
        "id": "Foresight AI",
        "title": "Foresight AI",
        "tags": [
          "project"
        ],
        "content": "#project \n\nhttps://www.lightningrod.ai/foresight\n\nAI tool (with a free version!) which allows you to query the probability of some future event. The system will provide a probability, as well as a reasoning trace explaining how that probability was generated. The tool also gives some feedback about the quality of your question (e.g. clear resolution criteria), as well as offering alternate questions you might want to ask.",
        "size": 17.714285714285715,
        "index": 52,
        "x": -513.2013549868367,
        "y": -680.0646260910952,
        "vy": -0.34999568110961393,
        "vx": -0.17336173148325393
      },
      "target": {
        "id": "AI Forecasting Benchmark",
        "title": "AI Forecasting Benchmark",
        "tags": [
          "project"
        ],
        "content": "#project \n\nhttps://www.metaculus.com/aib/\n\nMetaculus AIB (AI Benchmarking) is a series that benchmarks \"the state of the art in AI forecasting against the best humans on real-world questions.\"",
        "size": 20.57142857142857,
        "index": 178,
        "x": -491.69904780690194,
        "y": -761.3030318136879,
        "vy": -0.5357053744666,
        "vx": -0.2989940012828504
      },
      "index": 110
    },
    {
      "source": {
        "id": "Foresight AI",
        "title": "Foresight AI",
        "tags": [
          "project"
        ],
        "content": "#project \n\nhttps://www.lightningrod.ai/foresight\n\nAI tool (with a free version!) which allows you to query the probability of some future event. The system will provide a probability, as well as a reasoning trace explaining how that probability was generated. The tool also gives some feedback about the quality of your question (e.g. clear resolution criteria), as well as offering alternate questions you might want to ask.",
        "size": 17.714285714285715,
        "index": 52,
        "x": -513.2013549868367,
        "y": -680.0646260910952,
        "vy": -0.34999568110961393,
        "vx": -0.17336173148325393
      },
      "target": {
        "id": "Lightning Rod Labs",
        "title": "Lightning Rod Labs",
        "tags": [
          "organization"
        ],
        "content": "#organization \n\nhttps://www.lightningrod.ai/",
        "size": 18.285714285714285,
        "index": 99,
        "x": -485.4756382927939,
        "y": -568.3602203235023,
        "vy": -0.5387280910122677,
        "vx": -0.015714766705498383
      },
      "index": 111
    },
    {
      "source": {
        "id": "Double Crux Bot",
        "title": "Double Crux Bot",
        "tags": [
          "project"
        ],
        "content": "#project \n\nhttps://www.lesswrong.com/posts/ENgwdgdHZ2a6HTWDL/announcing-the-double-crux-bot\n\nGPT-powered chatbot for Slack and Discord that facilitates \"double crux\" conversations - a conflict resolution technique that helps two people identify the core disagreement underlying their dispute by finding shared cruxes (key beliefs that, if changed, would shift both parties' positions). The bot acts as a mediator to help people \"make their reasoning explicit and reflect on the crux of the issue\" and \"build better inferences about each other's motivations and frameworks so that they can come to a resolution.\"",
        "size": 17.714285714285715,
        "index": 54,
        "x": 3589.89856296859,
        "y": -1156.7002435643292,
        "vy": -1.0298732749823978,
        "vx": 0.2694657574666259
      },
      "target": {
        "id": "Sofi Vanhanen",
        "title": "Sofi Vanhanen",
        "tags": [
          "person"
        ],
        "content": "#person \n\nhttps://sofiavanhanen.fi/",
        "size": 18.285714285714285,
        "index": 177,
        "x": 3629.495175318134,
        "y": -1087.890476428783,
        "vy": -0.9980357709893994,
        "vx": 0.19329731806245978
      },
      "index": 112
    },
    {
      "source": {
        "id": "Double Crux Bot",
        "title": "Double Crux Bot",
        "tags": [
          "project"
        ],
        "content": "#project \n\nhttps://www.lesswrong.com/posts/ENgwdgdHZ2a6HTWDL/announcing-the-double-crux-bot\n\nGPT-powered chatbot for Slack and Discord that facilitates \"double crux\" conversations - a conflict resolution technique that helps two people identify the core disagreement underlying their dispute by finding shared cruxes (key beliefs that, if changed, would shift both parties' positions). The bot acts as a mediator to help people \"make their reasoning explicit and reflect on the crux of the issue\" and \"build better inferences about each other's motivations and frameworks so that they can come to a resolution.\"",
        "size": 17.714285714285715,
        "index": 54,
        "x": 3589.89856296859,
        "y": -1156.7002435643292,
        "vy": -1.0298732749823978,
        "vx": 0.2694657574666259
      },
      "target": {
        "id": "Santeri Koivula",
        "title": "Santeri Koivula",
        "tags": [
          "person"
        ],
        "content": "#person",
        "size": 16.571428571428573,
        "index": 161,
        "x": 3526.7139804658846,
        "y": -1186.8896167812852,
        "vy": -0.9974321212923968,
        "vx": 0.25617744091548783
      },
      "index": 113
    },
    {
      "source": {
        "id": "Double Crux Bot",
        "title": "Double Crux Bot",
        "tags": [
          "project"
        ],
        "content": "#project \n\nhttps://www.lesswrong.com/posts/ENgwdgdHZ2a6HTWDL/announcing-the-double-crux-bot\n\nGPT-powered chatbot for Slack and Discord that facilitates \"double crux\" conversations - a conflict resolution technique that helps two people identify the core disagreement underlying their dispute by finding shared cruxes (key beliefs that, if changed, would shift both parties' positions). The bot acts as a mediator to help people \"make their reasoning explicit and reflect on the crux of the issue\" and \"build better inferences about each other's motivations and frameworks so that they can come to a resolution.\"",
        "size": 17.714285714285715,
        "index": 54,
        "x": 3589.89856296859,
        "y": -1156.7002435643292,
        "vy": -1.0298732749823978,
        "vx": 0.2694657574666259
      },
      "target": {
        "id": "Sarah Bluhm",
        "title": "Sarah Bluhm",
        "tags": [
          "person"
        ],
        "content": "#person",
        "size": 16.571428571428573,
        "index": 70,
        "x": 3589.6795614195557,
        "y": -1228.0681934337858,
        "vy": -1.0235768604014552,
        "vx": 0.25595551063408983
      },
      "index": 114
    },
    {
      "source": {
        "id": "Emily Saltz",
        "title": "Emily Saltz",
        "tags": [
          "person"
        ],
        "content": "#person \n\nhttps://www.emilysaltz.space/",
        "size": 17.142857142857142,
        "index": 55,
        "x": 1675.7682442794192,
        "y": 962.4398438049817,
        "vy": -1.5392520011668342,
        "vx": 0.2752031074966344
      },
      "target": {
        "id": "Google Jigsaw",
        "title": "Google Jigsaw",
        "tags": [
          "organization"
        ],
        "content": "#organization \n\nhttps://jigsaw.google.com/",
        "size": 19.428571428571427,
        "index": 390,
        "x": 1784.38820840888,
        "y": 1035.7563657120636,
        "vy": -1.5493284452037834,
        "vx": 0.4016872013074959
      },
      "index": 115
    },
    {
      "source": {
        "id": "Julien Cornebise",
        "title": "Julien Cornebise",
        "tags": [
          "person"
        ],
        "content": "#person",
        "size": 17.142857142857142,
        "index": 56,
        "x": 2133.7538366244335,
        "y": 741.7209256089054,
        "vy": -1.5606596859274167,
        "vx": 0.4799794532457997
      },
      "target": {
        "id": "The Computational Democracy Project",
        "title": "The Computational Democracy Project",
        "tags": [
          "organization"
        ],
        "content": "#organization \n\nhttps://compdemocracy.org/",
        "size": 19.428571428571427,
        "index": 138,
        "x": 2057.334021465157,
        "y": 849.740764420989,
        "vy": -1.5779545596273115,
        "vx": 0.3854928364433583
      },
      "index": 116
    },
    {
      "source": {
        "id": "Danny Fanklin",
        "title": "Danny Fanklin",
        "tags": [
          "person"
        ],
        "content": "#person",
        "size": 17.714285714285715,
        "index": 59,
        "x": -431.6774386700457,
        "y": -494.60304253552584,
        "vy": -0.673501095578475,
        "vx": 0.04697915176713927
      },
      "target": {
        "id": "Lightning Rod Labs",
        "title": "Lightning Rod Labs",
        "tags": [
          "organization"
        ],
        "content": "#organization \n\nhttps://www.lightningrod.ai/",
        "size": 18.285714285714285,
        "index": 99,
        "x": -485.4756382927939,
        "y": -568.3602203235023,
        "vy": -0.5387280910122677,
        "vx": -0.015714766705498383
      },
      "index": 117
    },
    {
      "source": {
        "id": "Language Models as Critical Thinking Tools - A Case Study of Philosophers",
        "title": "Language Models as Critical Thinking Tools - A Case Study of Philosophers",
        "tags": [
          "project"
        ],
        "content": "#project \n\nhttps://arxiv.org/pdf/2404.04516\n \nStudy of 21 professional philosophers examining how language models could serve as \"critical thinking tools\" rather than just productivity accelerators. The researchers found that current LMs fail as critical thinking partners because they lack \"selfhood\" (consistent perspectives, memory, beliefs) and \"initiative\" (curiosity, proactivity, willingness to challenge users), leading philosophers to describe them as \"boring,\" \"bland,\" and \"cowardly.\" The authors propose three new LM roles to better support deep reasoning:\n1. the Interlocutor (high selfhood/initiative, challenges and disagrees), \n2. the Monitor (low selfhood/high initiative, provides diverse perspectives), and \n3. the Respondent (high selfhood/low initiative, reacts from specific viewpoints)",
        "size": 18.857142857142858,
        "index": 60,
        "x": 1322.0568355756795,
        "y": 1603.2768802970643,
        "vy": -1.3139930259662347,
        "vx": 0.755657415143613
      },
      "target": {
        "id": "Andre Ye",
        "title": "Andre Ye",
        "tags": [
          "person"
        ],
        "content": "#person \n\nhttps://andre-ye.org/",
        "size": 16.571428571428573,
        "index": 163,
        "x": 1324.240662897017,
        "y": 1712.652167125521,
        "vy": -1.3229118653389886,
        "vx": 0.8280301457012107
      },
      "index": 118
    },
    {
      "source": {
        "id": "Language Models as Critical Thinking Tools - A Case Study of Philosophers",
        "title": "Language Models as Critical Thinking Tools - A Case Study of Philosophers",
        "tags": [
          "project"
        ],
        "content": "#project \n\nhttps://arxiv.org/pdf/2404.04516\n \nStudy of 21 professional philosophers examining how language models could serve as \"critical thinking tools\" rather than just productivity accelerators. The researchers found that current LMs fail as critical thinking partners because they lack \"selfhood\" (consistent perspectives, memory, beliefs) and \"initiative\" (curiosity, proactivity, willingness to challenge users), leading philosophers to describe them as \"boring,\" \"bland,\" and \"cowardly.\" The authors propose three new LM roles to better support deep reasoning:\n1. the Interlocutor (high selfhood/initiative, challenges and disagrees), \n2. the Monitor (low selfhood/high initiative, provides diverse perspectives), and \n3. the Respondent (high selfhood/low initiative, reacts from specific viewpoints)",
        "size": 18.857142857142858,
        "index": 60,
        "x": 1322.0568355756795,
        "y": 1603.2768802970643,
        "vy": -1.3139930259662347,
        "vx": 0.755657415143613
      },
      "target": {
        "id": "Jared Moore",
        "title": "Jared Moore",
        "tags": [
          "person"
        ],
        "content": "#person \n\nhttps://jaredmoore.org/",
        "size": 16.571428571428573,
        "index": 109,
        "x": 1387.231520480465,
        "y": 1705.9864367070088,
        "vy": -1.3708321523670033,
        "vx": 0.8281694698426109
      },
      "index": 119
    },
    {
      "source": {
        "id": "Language Models as Critical Thinking Tools - A Case Study of Philosophers",
        "title": "Language Models as Critical Thinking Tools - A Case Study of Philosophers",
        "tags": [
          "project"
        ],
        "content": "#project \n\nhttps://arxiv.org/pdf/2404.04516\n \nStudy of 21 professional philosophers examining how language models could serve as \"critical thinking tools\" rather than just productivity accelerators. The researchers found that current LMs fail as critical thinking partners because they lack \"selfhood\" (consistent perspectives, memory, beliefs) and \"initiative\" (curiosity, proactivity, willingness to challenge users), leading philosophers to describe them as \"boring,\" \"bland,\" and \"cowardly.\" The authors propose three new LM roles to better support deep reasoning:\n1. the Interlocutor (high selfhood/initiative, challenges and disagrees), \n2. the Monitor (low selfhood/high initiative, provides diverse perspectives), and \n3. the Respondent (high selfhood/low initiative, reacts from specific viewpoints)",
        "size": 18.857142857142858,
        "index": 60,
        "x": 1322.0568355756795,
        "y": 1603.2768802970643,
        "vy": -1.3139930259662347,
        "vx": 0.755657415143613
      },
      "target": {
        "id": "Rose Novick",
        "title": "Rose Novick",
        "tags": [
          "person"
        ],
        "content": "#person",
        "size": 16.571428571428573,
        "index": 180,
        "x": 1360.1274172628246,
        "y": 1757.1206343568244,
        "vy": -1.3520456743808043,
        "vx": 0.864161080579913
      },
      "index": 120
    },
    {
      "source": {
        "id": "Language Models as Critical Thinking Tools - A Case Study of Philosophers",
        "title": "Language Models as Critical Thinking Tools - A Case Study of Philosophers",
        "tags": [
          "project"
        ],
        "content": "#project \n\nhttps://arxiv.org/pdf/2404.04516\n \nStudy of 21 professional philosophers examining how language models could serve as \"critical thinking tools\" rather than just productivity accelerators. The researchers found that current LMs fail as critical thinking partners because they lack \"selfhood\" (consistent perspectives, memory, beliefs) and \"initiative\" (curiosity, proactivity, willingness to challenge users), leading philosophers to describe them as \"boring,\" \"bland,\" and \"cowardly.\" The authors propose three new LM roles to better support deep reasoning:\n1. the Interlocutor (high selfhood/initiative, challenges and disagrees), \n2. the Monitor (low selfhood/high initiative, provides diverse perspectives), and \n3. the Respondent (high selfhood/low initiative, reacts from specific viewpoints)",
        "size": 18.857142857142858,
        "index": 60,
        "x": 1322.0568355756795,
        "y": 1603.2768802970643,
        "vy": -1.3139930259662347,
        "vx": 0.755657415143613
      },
      "target": {
        "id": "Amy Zhang",
        "title": "Amy Zhang",
        "tags": [
          "person"
        ],
        "content": "#person \n\nhttps://amyzhang.github.io/",
        "size": 17.714285714285715,
        "index": 8,
        "x": 1296.67931470172,
        "y": 1424.3976391045085,
        "vy": -1.3183531755494673,
        "vx": 0.6189876232804888
      },
      "index": 121
    },
    {
      "source": {
        "id": "Talk to the City",
        "title": "Talk to the City",
        "tags": [
          "project"
        ],
        "content": "#project \n\nhttps://ai.objectives.institute/talk-to-the-city\n\nOpen-source AI system that analyzes large-scale public input from surveys, interviews, and meetings using LLMs to identify themes and cluster opinions while maintaining the specificity of individual responses. The system processes both structured and unstructured data to generate interactive reports that allow decision-makers to explore opinion distributions at multiple scales, from broad thematic patterns down to individual participant perspectives",
        "size": 16.571428571428573,
        "index": 61,
        "x": 648.7804714199932,
        "y": -306.67853489950744,
        "vy": -1.1571041738632417,
        "vx": -0.0590163797519984
      },
      "target": {
        "id": "AI Objectives Institute",
        "title": "AI Objectives Institute",
        "tags": [
          "organization"
        ],
        "content": "#organization \n\nhttps://ai.objectives.institute/",
        "size": 20.57142857142857,
        "index": 304,
        "x": 665.7615191436854,
        "y": -247.07081557119588,
        "vy": -1.1515300453530217,
        "vx": -0.07444975689524554
      },
      "index": 122
    },
    {
      "source": {
        "id": "My Current Claims and Cruxes on LLM Forecasting & Epistemics",
        "title": "My Current Claims and Cruxes on LLM Forecasting & Epistemics",
        "tags": [
          "project"
        ],
        "content": "#project \n\nhttps://forum.effectivealtruism.org/posts/EykCuXDCFAT5oGyux/my-current-claims-and-cruxes-on-llm-forecasting-and\n\nPost outlines a vision for LLM-based Epistemic Processes (LEPs) that could automate virtually all aspects of knowledge work, from gathering and synthesizing information to generating forecasts and presenting results to users. The author breaks down LEPs into 13 distinct components including data collection, world modeling, human elicitation, numeric modeling, decomposition/amplification techniques, and various forms of automated decision support. A central argument is that these systems will require substantial \"scaffolding\" (supporting software infrastructure) and will likely be dominated by centralized platforms rather than distributed individual forecasters, potentially replacing human participation in prediction markets like Manifold and Metaculus. The post also addresses potential risks including AI acceleration, misuse by malicious actors, and the challenge that these powerful epistemic tools might increase world complexity faster than they improve our ability to navigate it.",
        "size": 17.142857142857142,
        "index": 63,
        "x": -846.3985542892191,
        "y": -1145.8208936945189,
        "vy": -0.3938475639925841,
        "vx": -0.5266735180832066
      },
      "target": {
        "id": "Ozzie Gooen",
        "title": "Ozzie Gooen",
        "tags": [
          "person"
        ],
        "content": "#person",
        "size": 18.285714285714285,
        "index": 352,
        "x": -795.6299935195096,
        "y": -1201.5289699537964,
        "vy": -0.36410862655798687,
        "vx": -0.5257118812416671
      },
      "index": 123
    },
    {
      "source": {
        "id": "Niki Dupuis",
        "title": "Niki Dupuis",
        "tags": [
          "person"
        ],
        "content": "#person",
        "size": 17.714285714285715,
        "index": 65,
        "x": 3688.7093262635094,
        "y": -985.8406135311669,
        "vy": -0.9897340634814135,
        "vx": 0.17742209799442776
      },
      "target": {
        "id": "Mosaic Labs",
        "title": "Mosaic Labs",
        "tags": [
          "organization"
        ],
        "content": "#organization \n\nhttps://mosaic-labs.org/",
        "size": 17.142857142857142,
        "index": 179,
        "x": 3659.5707234295,
        "y": -1036.9469431723346,
        "vy": -0.9786142309862262,
        "vx": 0.1606327073407951
      },
      "index": 124
    },
    {
      "source": {
        "id": "AI Enhanced Reasoning - Augmenting Human Critical Thinking With AI Systems",
        "title": "AI Enhanced Reasoning - Augmenting Human Critical Thinking With AI Systems",
        "tags": [
          "project"
        ],
        "content": "#project \n\nhttps://www.proquest.com/openview/3393dad9ea55a5d47f1a0b19e607f964/1.pdf\n\nMasters thesis, uses AI to help humans reason better through 3 experiments:\n- AI logic-checkers that spot argument flaws in real-time\n- Studies on how deceptive AI explanations mess with human thinking\n- AI that improves reasoning by asking smart questions",
        "size": 16.571428571428573,
        "index": 67,
        "x": 1171.564562563712,
        "y": -2768.51555121768,
        "vy": -1.0980819768122088,
        "vx": -0.4910281629679676
      },
      "target": {
        "id": "Valdemar Danry",
        "title": "Valdemar Danry",
        "tags": [
          "person"
        ],
        "content": "#person\n\nhttps://valdemardanry.com/",
        "size": 18.857142857142858,
        "index": 172,
        "x": 1102.4705595757653,
        "y": -2731.308366831243,
        "vy": -1.122899896351339,
        "vx": -0.5144777169524257
      },
      "index": 125
    },
    {
      "source": {
        "id": "Modelling Political Coalition Negotiations Using LLM-based Agents",
        "title": "Modelling Political Coalition Negotiations Using LLM-based Agents",
        "tags": [
          "project"
        ],
        "content": "#project \n\nhttps://arxiv.org/pdf/2402.11712\n\nThis research paper introduces a system for modeling political coalition negotiations using LLM-based agents. The authors create AI agents that represent different political parties and simulate the complex negotiation process to predict which policy statements from party manifestos will be included in final coalition agreements.",
        "size": 18.285714285714285,
        "index": 72,
        "x": -2168.1174286900696,
        "y": 2346.6799117617625,
        "vy": 0.18134799167553214,
        "vx": 1.0429977031864521
      },
      "target": {
        "id": "Farhad Moghimifar",
        "title": "Farhad Moghimifar",
        "tags": [
          "person"
        ],
        "content": "#person",
        "size": 16.571428571428573,
        "index": 255,
        "x": -2155.0928086864137,
        "y": 2280.438293658685,
        "vy": 0.1805653564859666,
        "vx": 1.0448522169053687
      },
      "index": 126
    },
    {
      "source": {
        "id": "Modelling Political Coalition Negotiations Using LLM-based Agents",
        "title": "Modelling Political Coalition Negotiations Using LLM-based Agents",
        "tags": [
          "project"
        ],
        "content": "#project \n\nhttps://arxiv.org/pdf/2402.11712\n\nThis research paper introduces a system for modeling political coalition negotiations using LLM-based agents. The authors create AI agents that represent different political parties and simulate the complex negotiation process to predict which policy statements from party manifestos will be included in final coalition agreements.",
        "size": 18.285714285714285,
        "index": 72,
        "x": -2168.1174286900696,
        "y": 2346.6799117617625,
        "vy": 0.18134799167553214,
        "vx": 1.0429977031864521
      },
      "target": {
        "id": "Yuan-Fang Li",
        "title": "Yuan-Fang Li",
        "tags": [
          "person"
        ],
        "content": "#person",
        "size": 16.571428571428573,
        "index": 232,
        "x": -2101.408687075367,
        "y": 2357.3363376149478,
        "vy": 0.18046601352658573,
        "vx": 1.0420954862893157
      },
      "index": 127
    },
    {
      "source": {
        "id": "Modelling Political Coalition Negotiations Using LLM-based Agents",
        "title": "Modelling Political Coalition Negotiations Using LLM-based Agents",
        "tags": [
          "project"
        ],
        "content": "#project \n\nhttps://arxiv.org/pdf/2402.11712\n\nThis research paper introduces a system for modeling political coalition negotiations using LLM-based agents. The authors create AI agents that represent different political parties and simulate the complex negotiation process to predict which policy statements from party manifestos will be included in final coalition agreements.",
        "size": 18.285714285714285,
        "index": 72,
        "x": -2168.1174286900696,
        "y": 2346.6799117617625,
        "vy": 0.18134799167553214,
        "vx": 1.0429977031864521
      },
      "target": {
        "id": "Robert Thomson",
        "title": "Robert Thomson",
        "tags": [
          "person"
        ],
        "content": "#person",
        "size": 16.571428571428573,
        "index": 409,
        "x": -2233.1042616917634,
        "y": 2330.851379562573,
        "vy": 0.18135737435371854,
        "vx": 1.0439434507437049
      },
      "index": 128
    },
    {
      "source": {
        "id": "Modelling Political Coalition Negotiations Using LLM-based Agents",
        "title": "Modelling Political Coalition Negotiations Using LLM-based Agents",
        "tags": [
          "project"
        ],
        "content": "#project \n\nhttps://arxiv.org/pdf/2402.11712\n\nThis research paper introduces a system for modeling political coalition negotiations using LLM-based agents. The authors create AI agents that represent different political parties and simulate the complex negotiation process to predict which policy statements from party manifestos will be included in final coalition agreements.",
        "size": 18.285714285714285,
        "index": 72,
        "x": -2168.1174286900696,
        "y": 2346.6799117617625,
        "vy": 0.18134799167553214,
        "vx": 1.0429977031864521
      },
      "target": {
        "id": "Gholamreza Haffari",
        "title": "Gholamreza Haffari",
        "tags": [
          "person"
        ],
        "content": "#person",
        "size": 16.571428571428573,
        "index": 267,
        "x": -2181.9887577435397,
        "y": 2411.6063431444727,
        "vy": 0.18256428348639825,
        "vx": 1.0436914386743534
      },
      "index": 129
    },
    {
      "source": {
        "id": "Approaching Human-Level Forecasting with Language Models",
        "title": "Approaching Human-Level Forecasting with Language Models",
        "tags": [
          "project"
        ],
        "content": "#project \n\nhttps://arxiv.org/pdf/2402.18563\n\nLLM system forecasting on competitive forecasting platforms by combining news retrieval, structured reasoning, and fine-tuning techniques to predict binary outcomes. The system automates the traditional forecasting process through three key components: \n1. retrieving relevant information from news sources,\n2. generating reasoned predictions, and \n3. aggregating multiple forecasts into a final prediction.",
        "size": 18.857142857142858,
        "index": 75,
        "x": -511.9886221542186,
        "y": 220.05320773975464,
        "vy": -0.5307874737070879,
        "vx": 0.15841345629537942
      },
      "target": {
        "id": "Danny Halawi",
        "title": "Danny Halawi",
        "tags": [
          "person"
        ],
        "content": "#person",
        "size": 17.142857142857142,
        "index": 167,
        "x": -399.49091861133286,
        "y": 117.53463577365183,
        "vy": -0.6214489335318216,
        "vx": 0.17216289674951193
      },
      "index": 130
    },
    {
      "source": {
        "id": "Approaching Human-Level Forecasting with Language Models",
        "title": "Approaching Human-Level Forecasting with Language Models",
        "tags": [
          "project"
        ],
        "content": "#project \n\nhttps://arxiv.org/pdf/2402.18563\n\nLLM system forecasting on competitive forecasting platforms by combining news retrieval, structured reasoning, and fine-tuning techniques to predict binary outcomes. The system automates the traditional forecasting process through three key components: \n1. retrieving relevant information from news sources,\n2. generating reasoned predictions, and \n3. aggregating multiple forecasts into a final prediction.",
        "size": 18.857142857142858,
        "index": 75,
        "x": -511.9886221542186,
        "y": 220.05320773975464,
        "vy": -0.5307874737070879,
        "vx": 0.15841345629537942
      },
      "target": {
        "id": "Fred Zhang",
        "title": "Fred Zhang",
        "tags": [
          "person"
        ],
        "content": "#person \n\nhttps://fredzhang.me/",
        "size": 17.142857142857142,
        "index": 302,
        "x": -379.93425636737896,
        "y": 176.99362172420902,
        "vy": -0.5999020415624712,
        "vx": 0.09227835755969993
      },
      "index": 131
    },
    {
      "source": {
        "id": "Approaching Human-Level Forecasting with Language Models",
        "title": "Approaching Human-Level Forecasting with Language Models",
        "tags": [
          "project"
        ],
        "content": "#project \n\nhttps://arxiv.org/pdf/2402.18563\n\nLLM system forecasting on competitive forecasting platforms by combining news retrieval, structured reasoning, and fine-tuning techniques to predict binary outcomes. The system automates the traditional forecasting process through three key components: \n1. retrieving relevant information from news sources,\n2. generating reasoned predictions, and \n3. aggregating multiple forecasts into a final prediction.",
        "size": 18.857142857142858,
        "index": 75,
        "x": -511.9886221542186,
        "y": 220.05320773975464,
        "vy": -0.5307874737070879,
        "vx": 0.15841345629537942
      },
      "target": {
        "id": "Chen Yueh-Han",
        "title": "Chen Yueh-Han",
        "tags": [
          "person"
        ],
        "content": "#person \n\nhttps://john-chen.cc/",
        "size": 17.142857142857142,
        "index": 433,
        "x": -339.6355168567282,
        "y": 134.8839087724612,
        "vy": -0.5406065567864574,
        "vx": 0.1543294172612554
      },
      "index": 132
    },
    {
      "source": {
        "id": "Approaching Human-Level Forecasting with Language Models",
        "title": "Approaching Human-Level Forecasting with Language Models",
        "tags": [
          "project"
        ],
        "content": "#project \n\nhttps://arxiv.org/pdf/2402.18563\n\nLLM system forecasting on competitive forecasting platforms by combining news retrieval, structured reasoning, and fine-tuning techniques to predict binary outcomes. The system automates the traditional forecasting process through three key components: \n1. retrieving relevant information from news sources,\n2. generating reasoned predictions, and \n3. aggregating multiple forecasts into a final prediction.",
        "size": 18.857142857142858,
        "index": 75,
        "x": -511.9886221542186,
        "y": 220.05320773975464,
        "vy": -0.5307874737070879,
        "vx": 0.15841345629537942
      },
      "target": {
        "id": "Jacob Steinhardt",
        "title": "Jacob Steinhardt",
        "tags": [
          "person"
        ],
        "content": "#person \n\nhttps://jsteinhardt.stat.berkeley.edu/",
        "size": 17.142857142857142,
        "index": 89,
        "x": -689.2056804129033,
        "y": 371.8596738179657,
        "vy": -0.4506994170173919,
        "vx": 0.20059927482224163
      },
      "index": 133
    },
    {
      "source": {
        "id": "Alice Siu",
        "title": "Alice Siu",
        "tags": [
          "person"
        ],
        "content": "#person \n\nhttps://www.alicesiu.com/",
        "size": 17.142857142857142,
        "index": 76,
        "x": 985.0211574111167,
        "y": 1172.7456815191272,
        "vy": -1.229908976771374,
        "vx": 0.3748639981031952
      },
      "target": {
        "id": "Deliberative Democracy Lab",
        "title": "Deliberative Democracy Lab",
        "tags": [
          "organization"
        ],
        "content": "#organization \n\nhttps://deliberation.stanford.edu/",
        "size": 17.142857142857142,
        "index": 408,
        "x": 847.3404089335676,
        "y": 1390.6348583925135,
        "vy": -1.2492320205522307,
        "vx": 0.46651654763471045
      },
      "index": 134
    },
    {
      "source": {
        "id": "Kris Skotheim",
        "title": "Kris Skotheim",
        "tags": [
          "person"
        ],
        "content": "#person",
        "size": 17.142857142857142,
        "index": 77,
        "x": -411.3547046728863,
        "y": -545.4937432550389,
        "vy": -0.6691253570982997,
        "vx": 0.05855254827232383
      },
      "target": {
        "id": "Lightning Rod Labs",
        "title": "Lightning Rod Labs",
        "tags": [
          "organization"
        ],
        "content": "#organization \n\nhttps://www.lightningrod.ai/",
        "size": 18.285714285714285,
        "index": 99,
        "x": -485.4756382927939,
        "y": -568.3602203235023,
        "vy": -0.5387280910122677,
        "vx": -0.015714766705498383
      },
      "index": 135
    },
    {
      "source": {
        "id": "Audrey Tang",
        "title": "Audrey Tang",
        "tags": [
          "person"
        ],
        "content": "#person \n\nhttps://github.com/audreyt",
        "size": 18.857142857142858,
        "index": 80,
        "x": 1086.0447800074567,
        "y": 719.5887259305994,
        "vy": -1.393226340879792,
        "vx": 0.1542153667232109
      },
      "target": {
        "id": "Collective Intelligence Project",
        "title": "Collective Intelligence Project",
        "tags": [
          "organization"
        ],
        "content": "#organization \n\nhttps://www.cip.org/",
        "size": 18.285714285714285,
        "index": 71,
        "x": 1196.948459287815,
        "y": 832.5435137077104,
        "vy": -1.3117327264452452,
        "vx": 0.30198095758611615
      },
      "index": 136
    },
    {
      "source": {
        "id": "Audrey Tang",
        "title": "Audrey Tang",
        "tags": [
          "person"
        ],
        "content": "#person \n\nhttps://github.com/audreyt",
        "size": 18.857142857142858,
        "index": 80,
        "x": 1086.0447800074567,
        "y": 719.5887259305994,
        "vy": -1.393226340879792,
        "vx": 0.1542153667232109
      },
      "target": {
        "id": "Plurality Institute",
        "title": "Plurality Institute",
        "tags": [
          "organization"
        ],
        "content": "#organization",
        "size": 20,
        "index": 364,
        "x": 1548.928220590021,
        "y": 1070.4694466931655,
        "vy": -1.5138983339043548,
        "vx": 0.41914174319685277
      },
      "index": 137
    },
    {
      "source": {
        "id": "Audrey Tang",
        "title": "Audrey Tang",
        "tags": [
          "person"
        ],
        "content": "#person \n\nhttps://github.com/audreyt",
        "size": 18.857142857142858,
        "index": 80,
        "x": 1086.0447800074567,
        "y": 719.5887259305994,
        "vy": -1.393226340879792,
        "vx": 0.1542153667232109
      },
      "target": {
        "id": "vTaiwan & g0v",
        "title": "vTaiwan & g0v",
        "tags": [
          "organization"
        ],
        "content": "#organization \n\nhttps://info.vtaiwan.tw/\nhttps://g0v.tw/",
        "size": 18.285714285714285,
        "index": 389,
        "x": 918.308649354897,
        "y": 190.9842796861607,
        "vy": -1.2371396245854598,
        "vx": 0.056319588048500874
      },
      "index": 138
    },
    {
      "source": {
        "id": "Bridging Bot",
        "title": "Bridging Bot",
        "tags": [
          "project"
        ],
        "content": "#project \n\nhttps://www.youtube.com/watch?v=QgfXdJ-7pF4\n\nLLM-based bot that de-escalates arguments on Reddit. Funded by Google Jigsaw.",
        "size": 18.857142857142858,
        "index": 81,
        "x": 1785.7024767083278,
        "y": 1200.0487188351551,
        "vy": -1.571718323053286,
        "vx": 0.7050820741421965
      },
      "target": {
        "id": "Google Jigsaw",
        "title": "Google Jigsaw",
        "tags": [
          "organization"
        ],
        "content": "#organization \n\nhttps://jigsaw.google.com/",
        "size": 19.428571428571427,
        "index": 390,
        "x": 1784.38820840888,
        "y": 1035.7563657120636,
        "vy": -1.5493284452037834,
        "vx": 0.4016872013074959
      },
      "index": 139
    },
    {
      "source": {
        "id": "Bridging Bot",
        "title": "Bridging Bot",
        "tags": [
          "project"
        ],
        "content": "#project \n\nhttps://www.youtube.com/watch?v=QgfXdJ-7pF4\n\nLLM-based bot that de-escalates arguments on Reddit. Funded by Google Jigsaw.",
        "size": 18.857142857142858,
        "index": 81,
        "x": 1785.7024767083278,
        "y": 1200.0487188351551,
        "vy": -1.571718323053286,
        "vx": 0.7050820741421965
      },
      "target": {
        "id": "Jeff Fossett",
        "title": "Jeff Fossett",
        "tags": [
          "person"
        ],
        "content": "#person",
        "size": 17.142857142857142,
        "index": 114,
        "x": 1652.7106936259504,
        "y": 1204.7793434839928,
        "vy": -1.6666005951708895,
        "vx": 0.5610439638668917
      },
      "index": 140
    },
    {
      "source": {
        "id": "Bridging Bot",
        "title": "Bridging Bot",
        "tags": [
          "project"
        ],
        "content": "#project \n\nhttps://www.youtube.com/watch?v=QgfXdJ-7pF4\n\nLLM-based bot that de-escalates arguments on Reddit. Funded by Google Jigsaw.",
        "size": 18.857142857142858,
        "index": 81,
        "x": 1785.7024767083278,
        "y": 1200.0487188351551,
        "vy": -1.571718323053286,
        "vx": 0.7050820741421965
      },
      "target": {
        "id": "Amina Green",
        "title": "Amina Green",
        "tags": [
          "person"
        ],
        "content": "#person \n\nhttps://www.amina-io.com/",
        "size": 17.142857142857142,
        "index": 9,
        "x": 1708.848852745666,
        "y": 1220.2859801318612,
        "vy": -1.666710205089688,
        "vx": 0.5825834175904476
      },
      "index": 141
    },
    {
      "source": {
        "id": "Bridging Bot",
        "title": "Bridging Bot",
        "tags": [
          "project"
        ],
        "content": "#project \n\nhttps://www.youtube.com/watch?v=QgfXdJ-7pF4\n\nLLM-based bot that de-escalates arguments on Reddit. Funded by Google Jigsaw.",
        "size": 18.857142857142858,
        "index": 81,
        "x": 1785.7024767083278,
        "y": 1200.0487188351551,
        "vy": -1.571718323053286,
        "vx": 0.7050820741421965
      },
      "target": {
        "id": "Ian Baker",
        "title": "Ian Baker",
        "tags": [
          "person"
        ],
        "content": "#person",
        "size": 17.142857142857142,
        "index": 401,
        "x": 1649.2806837888666,
        "y": 1143.6348851705998,
        "vy": -1.6785613446472192,
        "vx": 0.5623285705514641
      },
      "index": 142
    },
    {
      "source": {
        "id": "Bridging Bot",
        "title": "Bridging Bot",
        "tags": [
          "project"
        ],
        "content": "#project \n\nhttps://www.youtube.com/watch?v=QgfXdJ-7pF4\n\nLLM-based bot that de-escalates arguments on Reddit. Funded by Google Jigsaw.",
        "size": 18.857142857142858,
        "index": 81,
        "x": 1785.7024767083278,
        "y": 1200.0487188351551,
        "vy": -1.571718323053286,
        "vx": 0.7050820741421965
      },
      "target": {
        "id": "Peter Darche",
        "title": "Peter Darche",
        "tags": [
          "person"
        ],
        "content": "#person",
        "size": 17.142857142857142,
        "index": 437,
        "x": 1703.5768193864037,
        "y": 1164.827931289991,
        "vy": -1.6237876670598477,
        "vx": 0.5410514915146413
      },
      "index": 143
    },
    {
      "source": {
        "id": "Philosophy & Morality",
        "title": "Philosophy & Morality",
        "tags": [
          "topic"
        ],
        "content": "#topic",
        "size": 19.428571428571427,
        "index": 82,
        "x": 556.133995757879,
        "y": 1824.8772754010947,
        "vy": -0.8114753998177042,
        "vx": 1.15506314823146
      },
      "target": {
        "id": "AI doing philosophy = AI generating hands?",
        "title": "AI doing philosophy = AI generating hands?",
        "tags": [
          "project"
        ],
        "content": "#project \n\nhttps://www.lesswrong.com/posts/G4ARgcnFogpqorQgb/ai-doing-philosophy-ai-generating-hands-1\n\nWei Dai argues that just as current AI systems generate beautiful images but distorted hands, future AI might excel at science and technology while being dangerously incompetent at philosophical reasoning, creating a critical capability gap that requires urgent attention.",
        "size": 17.142857142857142,
        "index": 181,
        "x": 590.8140967392611,
        "y": 2738.198721233041,
        "vy": -0.997969840208746,
        "vx": 1.9443985164650819
      },
      "type": "topic_link",
      "index": 144
    },
    {
      "source": {
        "id": "Philosophy & Morality",
        "title": "Philosophy & Morality",
        "tags": [
          "topic"
        ],
        "content": "#topic",
        "size": 19.428571428571427,
        "index": 82,
        "x": 556.133995757879,
        "y": 1824.8772754010947,
        "vy": -0.8114753998177042,
        "vx": 1.15506314823146
      },
      "target": {
        "id": "Creating a large language model of a philosopher",
        "title": "Creating a large language model of a philosopher",
        "tags": [
          "project"
        ],
        "content": "#project \n\nhttps://arxiv.org/abs/2302.01339\n\nExperiment where they fine-tuned GPT-3 on the works of Daniel Dennett to see if an LLM can produce expert-quality philosophical texts. They tested the \"digi-Dan\" model by asking both the real Dennett and the AI model ten philosophical questions, then had 425 participants try to distinguish between Dennett's actual answers and the machine-generated responses. The study found that experts often chose the AI's answers over Dennett's actual responses, and on two questions, the AI outputs were selected by more experts than Dennett's own answers.",
        "size": 18.285714285714285,
        "index": 147,
        "x": -191.03887356290213,
        "y": 2454.014419868978,
        "vy": -0.06299306082960622,
        "vx": 1.6859137911892892
      },
      "type": "topic_link",
      "index": 145
    },
    {
      "source": {
        "id": "Philosophy & Morality",
        "title": "Philosophy & Morality",
        "tags": [
          "topic"
        ],
        "content": "#topic",
        "size": 19.428571428571427,
        "index": 82,
        "x": 556.133995757879,
        "y": 1824.8772754010947,
        "vy": -0.8114753998177042,
        "vx": 1.15506314823146
      },
      "target": {
        "id": "Language Models as Critical Thinking Tools - A Case Study of Philosophers",
        "title": "Language Models as Critical Thinking Tools - A Case Study of Philosophers",
        "tags": [
          "project"
        ],
        "content": "#project \n\nhttps://arxiv.org/pdf/2404.04516\n \nStudy of 21 professional philosophers examining how language models could serve as \"critical thinking tools\" rather than just productivity accelerators. The researchers found that current LMs fail as critical thinking partners because they lack \"selfhood\" (consistent perspectives, memory, beliefs) and \"initiative\" (curiosity, proactivity, willingness to challenge users), leading philosophers to describe them as \"boring,\" \"bland,\" and \"cowardly.\" The authors propose three new LM roles to better support deep reasoning:\n1. the Interlocutor (high selfhood/initiative, challenges and disagrees), \n2. the Monitor (low selfhood/high initiative, provides diverse perspectives), and \n3. the Respondent (high selfhood/low initiative, reacts from specific viewpoints)",
        "size": 18.857142857142858,
        "index": 60,
        "x": 1322.0568355756795,
        "y": 1603.2768802970643,
        "vy": -1.3139930259662347,
        "vx": 0.755657415143613
      },
      "type": "topic_link",
      "index": 146
    },
    {
      "source": {
        "id": "Philosophy & Morality",
        "title": "Philosophy & Morality",
        "tags": [
          "topic"
        ],
        "content": "#topic",
        "size": 19.428571428571427,
        "index": 82,
        "x": 556.133995757879,
        "y": 1824.8772754010947,
        "vy": -0.8114753998177042,
        "vx": 1.15506314823146
      },
      "target": {
        "id": "What are human values,and how do we align AI to them?",
        "title": "What are human values,and how do we align AI to them?",
        "tags": [
          "project"
        ],
        "content": "#project \n\nhttps://arxiv.org/pdf/2404.10636\n\nPaper introduces \"Moral Graph Elicitation\" (MGE), a method for systematically collecting and organizing human values to better align AI systems with what people actually care about. The researchers developed a process that uses AI-powered interviews to elicit specific \"values cards\" from people (detailed descriptions of what they pay attention to when making meaningful choices) then connects these values in a graph structure based on which values participants consider \"wiser\" than others.",
        "size": 18.285714285714285,
        "index": 162,
        "x": 1253.099985655064,
        "y": -246.90291544937514,
        "vy": -1.3156260391726693,
        "vx": -0.04877623727693523
      },
      "type": "topic_link",
      "index": 147
    },
    {
      "source": {
        "id": "Philosophy & Morality",
        "title": "Philosophy & Morality",
        "tags": [
          "topic"
        ],
        "content": "#topic",
        "size": 19.428571428571427,
        "index": 82,
        "x": 556.133995757879,
        "y": 1824.8772754010947,
        "vy": -0.8114753998177042,
        "vx": 1.15506314823146
      },
      "target": {
        "id": "Democratic Fine-Tuning",
        "title": "Democratic Fine-Tuning",
        "tags": [
          "project"
        ],
        "content": "#project \n\nhttps://www.meaningalignment.org/research/openai-dft-the-first-moral-graph\n\nLLM-based system creates \"moral graphs\" by using a two-stage process that aims to uncover shared values underlying political disagreement. A specialized chatbot engages participants in dialogue about contentious scenarios, asking for personal stories and role models to extract the underlying value behind responses rather than collecting ideological commitments like slogans or rules. The system then shows participants stories of people transitioning between different values and asks whether such transitions represent gains in wisdom, creating a graph structure where edges represent consensus about which values are more comprehensive than others\n\nFunded by [OpenAI Democratic Inputs to AI](https://github.com/openai/democratic-inputs).",
        "size": 18.285714285714285,
        "index": 372,
        "x": 1459.291986189716,
        "y": -19.522276055203207,
        "vy": -1.358464310600762,
        "vx": 0.05583502088743932
      },
      "type": "topic_link",
      "index": 148
    },
    {
      "source": {
        "id": "Philosophy & Morality",
        "title": "Philosophy & Morality",
        "tags": [
          "topic"
        ],
        "content": "#topic",
        "size": 19.428571428571427,
        "index": 82,
        "x": 556.133995757879,
        "y": 1824.8772754010947,
        "vy": -0.8114753998177042,
        "vx": 1.15506314823146
      },
      "target": {
        "id": "Rethinking Machine Ethics –Can LLMs Perform Moral Reasoning through the Lens of Moral Theories?",
        "title": "Rethinking Machine Ethics –Can LLMs Perform Moral Reasoning through the Lens of Moral Theories?",
        "tags": [
          "project"
        ],
        "content": "#project \n\nhttps://arxiv.org/pdf/2308.15399\n\nPaper presents a framework for enabling LLMs to make moral judgments by grounding them in established moral theories rather than learning from crowdsourced data. The researchers develop prompting techniques that guide models like GPT-4 to reason through ethical scenarios using theories from normative ethics (Justice, Deontology, Utilitarianism) and moral psychology (Theory of Dyadic Morality), producing explainable moral reasoning and decisions.",
        "size": 20.57142857142857,
        "index": 227,
        "x": 242.51830751517156,
        "y": 2646.2101137593295,
        "vy": -0.6996688990825385,
        "vx": 1.61146615333836
      },
      "type": "topic_link",
      "index": 149
    },
    {
      "source": {
        "id": "Jia-Wei Cui",
        "title": "Jia-Wei Cui",
        "tags": [
          "person"
        ],
        "content": "#person",
        "size": 17.142857142857142,
        "index": 84,
        "x": 884.4669013720609,
        "y": 83.069990629577,
        "vy": -1.2311162749898688,
        "vx": 0.029421145525977563
      },
      "target": {
        "id": "vTaiwan & g0v",
        "title": "vTaiwan & g0v",
        "tags": [
          "organization"
        ],
        "content": "#organization \n\nhttps://info.vtaiwan.tw/\nhttps://g0v.tw/",
        "size": 18.285714285714285,
        "index": 389,
        "x": 918.308649354897,
        "y": 190.9842796861607,
        "vy": -1.2371396245854598,
        "vx": 0.056319588048500874
      },
      "index": 150
    },
    {
      "source": {
        "id": "LLMs as Research Tools -  A Large Scale Survey of Researchers’ Usage and Perceptions",
        "title": "LLMs as Research Tools -  A Large Scale Survey of Researchers’ Usage and Perceptions",
        "tags": [
          "project"
        ],
        "content": "#project \n\nhttps://arxiv.org/pdf/2411.05025\n\nLarge-scale survey study examining how researchers currently use LLMs in their research workflows and their perceptions of benefits and risks. The study surveyed 816 verified research paper authors across multiple disciplines to understand usage patterns, demographic differences, and attitudes toward LLM tools in academic work.",
        "size": 21.142857142857142,
        "index": 85,
        "x": 1156.1294161915903,
        "y": 1698.8968115333146,
        "vy": -1.293066591526444,
        "vx": 0.7885209896134656
      },
      "target": {
        "id": "Zhehui Liao",
        "title": "Zhehui Liao",
        "tags": [
          "person"
        ],
        "content": "#person",
        "size": 16.571428571428573,
        "index": 3,
        "x": 1088.5466929376678,
        "y": 1764.3582779852723,
        "vy": -1.261786831140958,
        "vx": 0.8263551641336292
      },
      "index": 151
    },
    {
      "source": {
        "id": "LLMs as Research Tools -  A Large Scale Survey of Researchers’ Usage and Perceptions",
        "title": "LLMs as Research Tools -  A Large Scale Survey of Researchers’ Usage and Perceptions",
        "tags": [
          "project"
        ],
        "content": "#project \n\nhttps://arxiv.org/pdf/2411.05025\n\nLarge-scale survey study examining how researchers currently use LLMs in their research workflows and their perceptions of benefits and risks. The study surveyed 816 verified research paper authors across multiple disciplines to understand usage patterns, demographic differences, and attitudes toward LLM tools in academic work.",
        "size": 21.142857142857142,
        "index": 85,
        "x": 1156.1294161915903,
        "y": 1698.8968115333146,
        "vy": -1.293066591526444,
        "vx": 0.7885209896134656
      },
      "target": {
        "id": "Maria Antoniak",
        "title": "Maria Antoniak",
        "tags": [
          "person"
        ],
        "content": "#person",
        "size": 16.571428571428573,
        "index": 399,
        "x": 1217.8893859294753,
        "y": 1824.8730289520204,
        "vy": -1.3388319366134607,
        "vx": 0.8688595466141824
      },
      "index": 152
    },
    {
      "source": {
        "id": "LLMs as Research Tools -  A Large Scale Survey of Researchers’ Usage and Perceptions",
        "title": "LLMs as Research Tools -  A Large Scale Survey of Researchers’ Usage and Perceptions",
        "tags": [
          "project"
        ],
        "content": "#project \n\nhttps://arxiv.org/pdf/2411.05025\n\nLarge-scale survey study examining how researchers currently use LLMs in their research workflows and their perceptions of benefits and risks. The study surveyed 816 verified research paper authors across multiple disciplines to understand usage patterns, demographic differences, and attitudes toward LLM tools in academic work.",
        "size": 21.142857142857142,
        "index": 85,
        "x": 1156.1294161915903,
        "y": 1698.8968115333146,
        "vy": -1.293066591526444,
        "vx": 0.7885209896134656
      },
      "target": {
        "id": "Inyoung Cheong",
        "title": "Inyoung Cheong",
        "tags": [
          "person"
        ],
        "content": "#person",
        "size": 16.571428571428573,
        "index": 165,
        "x": 1176.2650821457353,
        "y": 1788.6525225333953,
        "vy": -1.3152240297895836,
        "vx": 0.8481394953606808
      },
      "index": 153
    },
    {
      "source": {
        "id": "LLMs as Research Tools -  A Large Scale Survey of Researchers’ Usage and Perceptions",
        "title": "LLMs as Research Tools -  A Large Scale Survey of Researchers’ Usage and Perceptions",
        "tags": [
          "project"
        ],
        "content": "#project \n\nhttps://arxiv.org/pdf/2411.05025\n\nLarge-scale survey study examining how researchers currently use LLMs in their research workflows and their perceptions of benefits and risks. The study surveyed 816 verified research paper authors across multiple disciplines to understand usage patterns, demographic differences, and attitudes toward LLM tools in academic work.",
        "size": 21.142857142857142,
        "index": 85,
        "x": 1156.1294161915903,
        "y": 1698.8968115333146,
        "vy": -1.293066591526444,
        "vx": 0.7885209896134656
      },
      "target": {
        "id": "Evie Yu-Yen Cheng",
        "title": "Evie Yu-Yen Cheng",
        "tags": [
          "person"
        ],
        "content": "#person",
        "size": 16.571428571428573,
        "index": 376,
        "x": 1124.0392657656873,
        "y": 1807.0514608098467,
        "vy": -1.2849020401886684,
        "vx": 0.8589961435521266
      },
      "index": 154
    },
    {
      "source": {
        "id": "LLMs as Research Tools -  A Large Scale Survey of Researchers’ Usage and Perceptions",
        "title": "LLMs as Research Tools -  A Large Scale Survey of Researchers’ Usage and Perceptions",
        "tags": [
          "project"
        ],
        "content": "#project \n\nhttps://arxiv.org/pdf/2411.05025\n\nLarge-scale survey study examining how researchers currently use LLMs in their research workflows and their perceptions of benefits and risks. The study surveyed 816 verified research paper authors across multiple disciplines to understand usage patterns, demographic differences, and attitudes toward LLM tools in academic work.",
        "size": 21.142857142857142,
        "index": 85,
        "x": 1156.1294161915903,
        "y": 1698.8968115333146,
        "vy": -1.293066591526444,
        "vx": 0.7885209896134656
      },
      "target": {
        "id": "Ai-Heng Lee",
        "title": "Ai-Heng Lee",
        "tags": [
          "person"
        ],
        "content": "#person",
        "size": 16.571428571428573,
        "index": 152,
        "x": 1072.0816918590474,
        "y": 1820.2532532332432,
        "vy": -1.2517142664166843,
        "vx": 0.8672378802364299
      },
      "index": 155
    },
    {
      "source": {
        "id": "LLMs as Research Tools -  A Large Scale Survey of Researchers’ Usage and Perceptions",
        "title": "LLMs as Research Tools -  A Large Scale Survey of Researchers’ Usage and Perceptions",
        "tags": [
          "project"
        ],
        "content": "#project \n\nhttps://arxiv.org/pdf/2411.05025\n\nLarge-scale survey study examining how researchers currently use LLMs in their research workflows and their perceptions of benefits and risks. The study surveyed 816 verified research paper authors across multiple disciplines to understand usage patterns, demographic differences, and attitudes toward LLM tools in academic work.",
        "size": 21.142857142857142,
        "index": 85,
        "x": 1156.1294161915903,
        "y": 1698.8968115333146,
        "vy": -1.293066591526444,
        "vx": 0.7885209896134656
      },
      "target": {
        "id": "Kyle Lo",
        "title": "Kyle Lo",
        "tags": [
          "person"
        ],
        "content": "#person",
        "size": 16.571428571428573,
        "index": 10,
        "x": 1167.2078193114032,
        "y": 1849.0516260907625,
        "vy": -1.3119788554475336,
        "vx": 0.8820066664405016
      },
      "index": 156
    },
    {
      "source": {
        "id": "LLMs as Research Tools -  A Large Scale Survey of Researchers’ Usage and Perceptions",
        "title": "LLMs as Research Tools -  A Large Scale Survey of Researchers’ Usage and Perceptions",
        "tags": [
          "project"
        ],
        "content": "#project \n\nhttps://arxiv.org/pdf/2411.05025\n\nLarge-scale survey study examining how researchers currently use LLMs in their research workflows and their perceptions of benefits and risks. The study surveyed 816 verified research paper authors across multiple disciplines to understand usage patterns, demographic differences, and attitudes toward LLM tools in academic work.",
        "size": 21.142857142857142,
        "index": 85,
        "x": 1156.1294161915903,
        "y": 1698.8968115333146,
        "vy": -1.293066591526444,
        "vx": 0.7885209896134656
      },
      "target": {
        "id": "Joseph Chee Chang",
        "title": "Joseph Chee Chang",
        "tags": [
          "person"
        ],
        "content": "#person",
        "size": 16.571428571428573,
        "index": 25,
        "x": 1113.2389426501634,
        "y": 1863.0531295677174,
        "vy": -1.2772680952678497,
        "vx": 0.8925914349326832
      },
      "index": 157
    },
    {
      "source": {
        "id": "LLMs as Research Tools -  A Large Scale Survey of Researchers’ Usage and Perceptions",
        "title": "LLMs as Research Tools -  A Large Scale Survey of Researchers’ Usage and Perceptions",
        "tags": [
          "project"
        ],
        "content": "#project \n\nhttps://arxiv.org/pdf/2411.05025\n\nLarge-scale survey study examining how researchers currently use LLMs in their research workflows and their perceptions of benefits and risks. The study surveyed 816 verified research paper authors across multiple disciplines to understand usage patterns, demographic differences, and attitudes toward LLM tools in academic work.",
        "size": 21.142857142857142,
        "index": 85,
        "x": 1156.1294161915903,
        "y": 1698.8968115333146,
        "vy": -1.293066591526444,
        "vx": 0.7885209896134656
      },
      "target": {
        "id": "Amy Zhang",
        "title": "Amy Zhang",
        "tags": [
          "person"
        ],
        "content": "#person \n\nhttps://amyzhang.github.io/",
        "size": 17.714285714285715,
        "index": 8,
        "x": 1296.67931470172,
        "y": 1424.3976391045085,
        "vy": -1.3183531755494673,
        "vx": 0.6189876232804888
      },
      "index": 158
    },
    {
      "source": {
        "id": "Future Search",
        "title": "Future Search",
        "tags": [
          "organization"
        ],
        "content": "#organization \n\nhttps://futuresearch.ai/",
        "size": 22.285714285714285,
        "index": 94,
        "x": -717.8526553292105,
        "y": -719.7276361443852,
        "vy": -0.4404383940953807,
        "vx": -0.3824839533244458
      },
      "target": {
        "id": "AI Forecasting Benchmark",
        "title": "AI Forecasting Benchmark",
        "tags": [
          "project"
        ],
        "content": "#project \n\nhttps://www.metaculus.com/aib/\n\nMetaculus AIB (AI Benchmarking) is a series that benchmarks \"the state of the art in AI forecasting against the best humans on real-world questions.\"",
        "size": 20.57142857142857,
        "index": 178,
        "x": -491.69904780690194,
        "y": -761.3030318136879,
        "vy": -0.5357053744666,
        "vx": -0.2989940012828504
      },
      "index": 159
    },
    {
      "source": {
        "id": "Ben Wilson",
        "title": "Ben Wilson",
        "tags": [
          "person"
        ],
        "content": "#person",
        "size": 18.285714285714285,
        "index": 98,
        "x": -246.84657412896797,
        "y": -802.6439658065143,
        "vy": -0.7451535349150801,
        "vx": -0.25821581807688415
      },
      "target": {
        "id": "Metaculus",
        "title": "Metaculus",
        "tags": [
          "organization"
        ],
        "content": "#organization\n\nhttps://www.metaculus.com/",
        "size": 20,
        "index": 126,
        "x": -291.65109336863395,
        "y": -589.3419401920876,
        "vy": -0.7257043832009319,
        "vx": -0.15741051377760798
      },
      "index": 160
    },
    {
      "source": {
        "id": "Ben Wilson",
        "title": "Ben Wilson",
        "tags": [
          "person"
        ],
        "content": "#person",
        "size": 18.285714285714285,
        "index": 98,
        "x": -246.84657412896797,
        "y": -802.6439658065143,
        "vy": -0.7451535349150801,
        "vx": -0.25821581807688415
      },
      "target": {
        "id": "The Society Library",
        "title": "The Society Library",
        "tags": [
          "organization"
        ],
        "content": "#organization \n\nhttps://societylibrary.medium.com/\nhttps://www.societylibrary.org/",
        "size": 18.285714285714285,
        "index": 274,
        "x": -123.51596440526208,
        "y": -972.6539885865532,
        "vy": -0.7846889723379666,
        "vx": -0.37290689478790895
      },
      "index": 161
    },
    {
      "source": {
        "id": "Human v Bots Forecasting Tournament 2024",
        "title": "Human v Bots Forecasting Tournament 2024",
        "tags": [
          "project"
        ],
        "content": "#project\n\nhttps://news.manifold.markets/p/human-v-bots-forecasting-tournament\n\nCompetition hosted by Manifold Markets, where human forecasters compete directly against AI systems to predict major world events throughout 2024, with winners determined by forecasting accuracy and profit.",
        "size": 17.714285714285715,
        "index": 100,
        "x": -783.1858281412245,
        "y": -810.106102551167,
        "vy": -0.47707926734133976,
        "vx": -0.32899427919111074
      },
      "target": {
        "id": "Future Search",
        "title": "Future Search",
        "tags": [
          "organization"
        ],
        "content": "#organization \n\nhttps://futuresearch.ai/",
        "size": 22.285714285714285,
        "index": 94,
        "x": -717.8526553292105,
        "y": -719.7276361443852,
        "vy": -0.4404383940953807,
        "vx": -0.3824839533244458
      },
      "index": 162
    },
    {
      "source": {
        "id": "Human v Bots Forecasting Tournament 2024",
        "title": "Human v Bots Forecasting Tournament 2024",
        "tags": [
          "project"
        ],
        "content": "#project\n\nhttps://news.manifold.markets/p/human-v-bots-forecasting-tournament\n\nCompetition hosted by Manifold Markets, where human forecasters compete directly against AI systems to predict major world events throughout 2024, with winners determined by forecasting accuracy and profit.",
        "size": 17.714285714285715,
        "index": 100,
        "x": -783.1858281412245,
        "y": -810.106102551167,
        "vy": -0.47707926734133976,
        "vx": -0.32899427919111074
      },
      "target": {
        "id": "Manifold Markets",
        "title": "Manifold Markets",
        "tags": [
          "organization"
        ],
        "content": "#organization \n\nhttps://manifold.markets/home",
        "size": 16.571428571428573,
        "index": 68,
        "x": -853.4949587966678,
        "y": -880.135286309336,
        "vy": -0.46622374031049724,
        "vx": -0.31078216896997085
      },
      "index": 163
    },
    {
      "source": {
        "id": "Democratic Policy Development using Collective Dialogues and AI",
        "title": "Democratic Policy Development using Collective Dialogues and AI",
        "tags": [
          "project"
        ],
        "content": "#project \n\nhttps://arxiv.org/pdf/2311.02242\n\nPaper presents a democratic process for developing AI policies through collective dialogues, using AI tools to facilitate large-scale deliberation and consensus-finding among diverse public participants. The system combines AI-augmented group discussions with bridging-based ranking algorithms to identify points of consensus, then uses GPT-4 to translate these into concrete policy guidelines that are refined through expert input and further public feedback.\n\nFunded by [OpenAI Democratic Inputs to AI](https://github.com/openai/democratic-inputs).",
        "size": 18.285714285714285,
        "index": 101,
        "x": 1665.396310946405,
        "y": 867.003953915849,
        "vy": -1.4501025482832737,
        "vx": 0.244173195626297
      },
      "target": {
        "id": "Andrew Konya",
        "title": "Andrew Konya",
        "tags": [
          "person"
        ],
        "content": "#person \n\nhttps://andrewkonya.com/",
        "size": 18.857142857142858,
        "index": 265,
        "x": 1563.7748126336485,
        "y": 791.1411106203664,
        "vy": -1.4006415470721048,
        "vx": 0.2425628664665928
      },
      "index": 164
    },
    {
      "source": {
        "id": "Democratic Policy Development using Collective Dialogues and AI",
        "title": "Democratic Policy Development using Collective Dialogues and AI",
        "tags": [
          "project"
        ],
        "content": "#project \n\nhttps://arxiv.org/pdf/2311.02242\n\nPaper presents a democratic process for developing AI policies through collective dialogues, using AI tools to facilitate large-scale deliberation and consensus-finding among diverse public participants. The system combines AI-augmented group discussions with bridging-based ranking algorithms to identify points of consensus, then uses GPT-4 to translate these into concrete policy guidelines that are refined through expert input and further public feedback.\n\nFunded by [OpenAI Democratic Inputs to AI](https://github.com/openai/democratic-inputs).",
        "size": 18.285714285714285,
        "index": 101,
        "x": 1665.396310946405,
        "y": 867.003953915849,
        "vy": -1.4501025482832737,
        "vx": 0.244173195626297
      },
      "target": {
        "id": "Lisa Schirch",
        "title": "Lisa Schirch",
        "tags": [
          "person"
        ],
        "content": "#person \n\nhttps://kroc.nd.edu/faculty-and-staff/lisa-schirch/",
        "size": 18.285714285714285,
        "index": 291,
        "x": 1561.488577698378,
        "y": 890.5844108032592,
        "vy": -1.453577080273647,
        "vx": 0.2825364996104594
      },
      "index": 165
    },
    {
      "source": {
        "id": "Democratic Policy Development using Collective Dialogues and AI",
        "title": "Democratic Policy Development using Collective Dialogues and AI",
        "tags": [
          "project"
        ],
        "content": "#project \n\nhttps://arxiv.org/pdf/2311.02242\n\nPaper presents a democratic process for developing AI policies through collective dialogues, using AI tools to facilitate large-scale deliberation and consensus-finding among diverse public participants. The system combines AI-augmented group discussions with bridging-based ranking algorithms to identify points of consensus, then uses GPT-4 to translate these into concrete policy guidelines that are refined through expert input and further public feedback.\n\nFunded by [OpenAI Democratic Inputs to AI](https://github.com/openai/democratic-inputs).",
        "size": 18.285714285714285,
        "index": 101,
        "x": 1665.396310946405,
        "y": 867.003953915849,
        "vy": -1.4501025482832737,
        "vx": 0.244173195626297
      },
      "target": {
        "id": "Colin Irwin",
        "title": "Colin Irwin",
        "tags": [
          "person"
        ],
        "content": "#person",
        "size": 16.571428571428573,
        "index": 319,
        "x": 1787.8513614818683,
        "y": 917.2872833286947,
        "vy": -1.6572262220588563,
        "vx": 0.3287686962637249
      },
      "index": 166
    },
    {
      "source": {
        "id": "Democratic Policy Development using Collective Dialogues and AI",
        "title": "Democratic Policy Development using Collective Dialogues and AI",
        "tags": [
          "project"
        ],
        "content": "#project \n\nhttps://arxiv.org/pdf/2311.02242\n\nPaper presents a democratic process for developing AI policies through collective dialogues, using AI tools to facilitate large-scale deliberation and consensus-finding among diverse public participants. The system combines AI-augmented group discussions with bridging-based ranking algorithms to identify points of consensus, then uses GPT-4 to translate these into concrete policy guidelines that are refined through expert input and further public feedback.\n\nFunded by [OpenAI Democratic Inputs to AI](https://github.com/openai/democratic-inputs).",
        "size": 18.285714285714285,
        "index": 101,
        "x": 1665.396310946405,
        "y": 867.003953915849,
        "vy": -1.4501025482832737,
        "vx": 0.244173195626297
      },
      "target": {
        "id": "Aviv Ovadya",
        "title": "Aviv Ovadya",
        "tags": [
          "person"
        ],
        "content": "#person \n\nhttps://aviv.me/",
        "size": 18.285714285714285,
        "index": 283,
        "x": 1597.9484999890756,
        "y": 842.2153044669851,
        "vy": -1.4288230936945292,
        "vx": 0.3153930584572157
      },
      "index": 167
    },
    {
      "source": {
        "id": "Değer Turan",
        "title": "Değer Turan",
        "tags": [
          "person"
        ],
        "content": "#person",
        "size": 18.285714285714285,
        "index": 107,
        "x": 371.54679486309595,
        "y": -417.8764113264942,
        "vy": -1.0219808978726659,
        "vx": -0.10788831074881157
      },
      "target": {
        "id": "Metaculus",
        "title": "Metaculus",
        "tags": [
          "organization"
        ],
        "content": "#organization\n\nhttps://www.metaculus.com/",
        "size": 20,
        "index": 126,
        "x": -291.65109336863395,
        "y": -589.3419401920876,
        "vy": -0.7257043832009319,
        "vx": -0.15741051377760798
      },
      "index": 168
    },
    {
      "source": {
        "id": "Değer Turan",
        "title": "Değer Turan",
        "tags": [
          "person"
        ],
        "content": "#person",
        "size": 18.285714285714285,
        "index": 107,
        "x": 371.54679486309595,
        "y": -417.8764113264942,
        "vy": -1.0219808978726659,
        "vx": -0.10788831074881157
      },
      "target": {
        "id": "AI Objectives Institute",
        "title": "AI Objectives Institute",
        "tags": [
          "organization"
        ],
        "content": "#organization \n\nhttps://ai.objectives.institute/",
        "size": 20.57142857142857,
        "index": 304,
        "x": 665.7615191436854,
        "y": -247.07081557119588,
        "vy": -1.1515300453530217,
        "vx": -0.07444975689524554
      },
      "index": 169
    },
    {
      "source": {
        "id": "Owen Cotton-Barratt",
        "title": "Owen Cotton-Barratt",
        "tags": [
          "person"
        ],
        "content": "#person \n\nhttps://strangecities.substack.com/",
        "size": 17.714285714285715,
        "index": 108,
        "x": -1767.049221344344,
        "y": 489.6781367043218,
        "vy": -0.2228153899267383,
        "vx": 0.1807751081937257
      },
      "target": {
        "id": "Ought",
        "title": "Ought",
        "tags": [
          "organization"
        ],
        "content": "#organization \n\nhttps://ought.org/ (website is old)\n\nOught 1.0 has effectively become what is now Elicit (the company) continuing development on Elicit (the tool). Ought 2.0 has not yet done any public work.",
        "size": 23.42857142857143,
        "index": 306,
        "x": -1888.3722954977052,
        "y": 453.0511056583274,
        "vy": -0.2547035865836596,
        "vx": 0.0471838457037637
      },
      "index": 170
    },
    {
      "source": {
        "id": "AI-Augmented Predictions - LLM Assistants Improve Human Forecasting Accuracy",
        "title": "AI-Augmented Predictions - LLM Assistants Improve Human Forecasting Accuracy",
        "tags": [
          "project"
        ],
        "content": "#project \n\nhttps://arxiv.org/abs/2402.07862\n\nThis paper studies how LLMs can serve as assistants to improve human forecasting accuracy, testing two types of assistants - one designed to provide high-quality \"superforecasting\" advice and another designed to be overconfident and noisy. In a study with 991 participants answering forecasting questions, both LLM assistants significantly enhanced human prediction accuracy compared to controls, with the superforecasting assistant showing up to 41% improvement in some analyses.",
        "size": 19.428571428571427,
        "index": 113,
        "x": -185.16606034019384,
        "y": -153.3063209812553,
        "vy": -0.7719172339969845,
        "vx": 0.09169564090059472
      },
      "target": {
        "id": "Philipp Schoenegger",
        "title": "Philipp Schoenegger",
        "tags": [
          "person"
        ],
        "content": "#person \n\nhttps://philipp-schoenegger.weebly.com/",
        "size": 19.428571428571427,
        "index": 120,
        "x": -308.0505670602049,
        "y": -355.92667066361173,
        "vy": -0.7160490223836882,
        "vx": 0.03780161064368572
      },
      "index": 171
    },
    {
      "source": {
        "id": "AI-Augmented Predictions - LLM Assistants Improve Human Forecasting Accuracy",
        "title": "AI-Augmented Predictions - LLM Assistants Improve Human Forecasting Accuracy",
        "tags": [
          "project"
        ],
        "content": "#project \n\nhttps://arxiv.org/abs/2402.07862\n\nThis paper studies how LLMs can serve as assistants to improve human forecasting accuracy, testing two types of assistants - one designed to provide high-quality \"superforecasting\" advice and another designed to be overconfident and noisy. In a study with 991 participants answering forecasting questions, both LLM assistants significantly enhanced human prediction accuracy compared to controls, with the superforecasting assistant showing up to 41% improvement in some analyses.",
        "size": 19.428571428571427,
        "index": 113,
        "x": -185.16606034019384,
        "y": -153.3063209812553,
        "vy": -0.7719172339969845,
        "vx": 0.09169564090059472
      },
      "target": {
        "id": "Peter Park",
        "title": "Peter Park",
        "tags": [
          "person"
        ],
        "content": "#person",
        "size": 17.142857142857142,
        "index": 276,
        "x": -226.8077158046543,
        "y": -232.4893312689628,
        "vy": -0.7153680729821785,
        "vx": 0.018395868889687337
      },
      "index": 172
    },
    {
      "source": {
        "id": "AI-Augmented Predictions - LLM Assistants Improve Human Forecasting Accuracy",
        "title": "AI-Augmented Predictions - LLM Assistants Improve Human Forecasting Accuracy",
        "tags": [
          "project"
        ],
        "content": "#project \n\nhttps://arxiv.org/abs/2402.07862\n\nThis paper studies how LLMs can serve as assistants to improve human forecasting accuracy, testing two types of assistants - one designed to provide high-quality \"superforecasting\" advice and another designed to be overconfident and noisy. In a study with 991 participants answering forecasting questions, both LLM assistants significantly enhanced human prediction accuracy compared to controls, with the superforecasting assistant showing up to 41% improvement in some analyses.",
        "size": 19.428571428571427,
        "index": 113,
        "x": -185.16606034019384,
        "y": -153.3063209812553,
        "vy": -0.7719172339969845,
        "vx": 0.09169564090059472
      },
      "target": {
        "id": "Ezra Karger",
        "title": "Ezra Karger",
        "tags": [
          "person"
        ],
        "content": "#person \n\nhttps://ezrakarger.com/",
        "size": 18.285714285714285,
        "index": 150,
        "x": 13.294680683389643,
        "y": 76.75026712023329,
        "vy": -0.8178432271089577,
        "vx": 0.14711852217550486
      },
      "index": 173
    },
    {
      "source": {
        "id": "AI-Augmented Predictions - LLM Assistants Improve Human Forecasting Accuracy",
        "title": "AI-Augmented Predictions - LLM Assistants Improve Human Forecasting Accuracy",
        "tags": [
          "project"
        ],
        "content": "#project \n\nhttps://arxiv.org/abs/2402.07862\n\nThis paper studies how LLMs can serve as assistants to improve human forecasting accuracy, testing two types of assistants - one designed to provide high-quality \"superforecasting\" advice and another designed to be overconfident and noisy. In a study with 991 participants answering forecasting questions, both LLM assistants significantly enhanced human prediction accuracy compared to controls, with the superforecasting assistant showing up to 41% improvement in some analyses.",
        "size": 19.428571428571427,
        "index": 113,
        "x": -185.16606034019384,
        "y": -153.3063209812553,
        "vy": -0.7719172339969845,
        "vx": 0.09169564090059472
      },
      "target": {
        "id": "Sean Trott",
        "title": "Sean Trott",
        "tags": [
          "person"
        ],
        "content": "#person",
        "size": 16.571428571428573,
        "index": 209,
        "x": -212.1781690123337,
        "y": -97.70010759208293,
        "vy": -0.776655600274206,
        "vx": 0.15626074254658015
      },
      "index": 174
    },
    {
      "source": {
        "id": "AI-Augmented Predictions - LLM Assistants Improve Human Forecasting Accuracy",
        "title": "AI-Augmented Predictions - LLM Assistants Improve Human Forecasting Accuracy",
        "tags": [
          "project"
        ],
        "content": "#project \n\nhttps://arxiv.org/abs/2402.07862\n\nThis paper studies how LLMs can serve as assistants to improve human forecasting accuracy, testing two types of assistants - one designed to provide high-quality \"superforecasting\" advice and another designed to be overconfident and noisy. In a study with 991 participants answering forecasting questions, both LLM assistants significantly enhanced human prediction accuracy compared to controls, with the superforecasting assistant showing up to 41% improvement in some analyses.",
        "size": 19.428571428571427,
        "index": 113,
        "x": -185.16606034019384,
        "y": -153.3063209812553,
        "vy": -0.7719172339969845,
        "vx": 0.09169564090059472
      },
      "target": {
        "id": "Philip Tetlock",
        "title": "Philip Tetlock",
        "tags": [
          "person"
        ],
        "content": "#person",
        "size": 18.857142857142858,
        "index": 326,
        "x": -269.190694144188,
        "y": -121.36964146524063,
        "vy": -0.698755466059142,
        "vx": 0.0871105431534283
      },
      "index": 175
    },
    {
      "source": {
        "id": "Jeff Fossett",
        "title": "Jeff Fossett",
        "tags": [
          "person"
        ],
        "content": "#person",
        "size": 17.142857142857142,
        "index": 114,
        "x": 1652.7106936259504,
        "y": 1204.7793434839928,
        "vy": -1.6666005951708895,
        "vx": 0.5610439638668917
      },
      "target": {
        "id": "Plurality Institute",
        "title": "Plurality Institute",
        "tags": [
          "organization"
        ],
        "content": "#organization",
        "size": 20,
        "index": 364,
        "x": 1548.928220590021,
        "y": 1070.4694466931655,
        "vy": -1.5138983339043548,
        "vx": 0.41914174319685277
      },
      "index": 176
    },
    {
      "source": {
        "id": "AI Debate Maps",
        "title": "AI Debate Maps",
        "tags": [
          "project"
        ],
        "content": "#project \n\nhttps://www.societylibrary.org/topics-blog/ai-alignment-superintelligence-ethics\n\nDebate maps specifically about Artificial Intelligence.",
        "size": 17.142857142857142,
        "index": 116,
        "x": 119.44632840542677,
        "y": -1129.4661000636625,
        "vy": -0.8702793844281093,
        "vx": -0.46610233240688503
      },
      "target": {
        "id": "Debate Maps",
        "title": "Debate Maps",
        "tags": [
          "project"
        ],
        "content": "#project \n\nhttps://www.societylibrary.org/debate-mapping-program\n\nAutomated pipeline for scraping online content, extracting arguments/claims, and organizing them into visual \"maps\" which structure the debate into a tree of statements and their logical relationships.",
        "size": 17.142857142857142,
        "index": 198,
        "x": -12.304182348450263,
        "y": -1067.4816033717182,
        "vy": -0.8325786381460952,
        "vx": -0.42144941558990606
      },
      "index": 177
    },
    {
      "source": {
        "id": "Human vs. Machine -  Behavioral Differences between Expert Humans and Language Models in Wargame Simulations",
        "title": "Human vs. Machine -  Behavioral Differences between Expert Humans and Language Models in Wargame Simulations",
        "tags": [
          "project"
        ],
        "content": "#project \n\nhttps://arxiv.org/pdf/2403.03407\n\nPaper examines how LLMs behave compared to human experts when simulating military crisis decision-making through wargames. The study involved 214 national security experts and compared their responses to AI-simulated teams in a fictional U.S.-China crisis scenario over the Taiwan Strait. The research found that while LLMs showed significant overlap with human decision-making patterns, they demonstrated concerning tendencies toward more aggressive actions and were significantly affected by changes in scenario parameters.",
        "size": 20,
        "index": 118,
        "x": -2213.2951352228097,
        "y": -1376.7759485449815,
        "vy": -0.07881523560233798,
        "vx": -0.38620555083276925
      },
      "target": {
        "id": "Max Lamparth",
        "title": "Max Lamparth",
        "tags": [
          "person"
        ],
        "content": "#person \n\nhttps://www.maxlamparth.com/",
        "size": 17.142857142857142,
        "index": 136,
        "x": -2139.075864003679,
        "y": -1403.148621492655,
        "vy": -0.09840077474495441,
        "vx": -0.4217991876266044
      },
      "index": 178
    },
    {
      "source": {
        "id": "Human vs. Machine -  Behavioral Differences between Expert Humans and Language Models in Wargame Simulations",
        "title": "Human vs. Machine -  Behavioral Differences between Expert Humans and Language Models in Wargame Simulations",
        "tags": [
          "project"
        ],
        "content": "#project \n\nhttps://arxiv.org/pdf/2403.03407\n\nPaper examines how LLMs behave compared to human experts when simulating military crisis decision-making through wargames. The study involved 214 national security experts and compared their responses to AI-simulated teams in a fictional U.S.-China crisis scenario over the Taiwan Strait. The research found that while LLMs showed significant overlap with human decision-making patterns, they demonstrated concerning tendencies toward more aggressive actions and were significantly affected by changes in scenario parameters.",
        "size": 20,
        "index": 118,
        "x": -2213.2951352228097,
        "y": -1376.7759485449815,
        "vy": -0.07881523560233798,
        "vx": -0.38620555083276925
      },
      "target": {
        "id": "Anthony Corso",
        "title": "Anthony Corso",
        "tags": [
          "person"
        ],
        "content": "#person",
        "size": 16.571428571428573,
        "index": 282,
        "x": -2207.0321862085693,
        "y": -1302.1970500310003,
        "vy": -0.09330153665495579,
        "vx": -0.31708167152160166
      },
      "index": 179
    },
    {
      "source": {
        "id": "Human vs. Machine -  Behavioral Differences between Expert Humans and Language Models in Wargame Simulations",
        "title": "Human vs. Machine -  Behavioral Differences between Expert Humans and Language Models in Wargame Simulations",
        "tags": [
          "project"
        ],
        "content": "#project \n\nhttps://arxiv.org/pdf/2403.03407\n\nPaper examines how LLMs behave compared to human experts when simulating military crisis decision-making through wargames. The study involved 214 national security experts and compared their responses to AI-simulated teams in a fictional U.S.-China crisis scenario over the Taiwan Strait. The research found that while LLMs showed significant overlap with human decision-making patterns, they demonstrated concerning tendencies toward more aggressive actions and were significantly affected by changes in scenario parameters.",
        "size": 20,
        "index": 118,
        "x": -2213.2951352228097,
        "y": -1376.7759485449815,
        "vy": -0.07881523560233798,
        "vx": -0.38620555083276925
      },
      "target": {
        "id": "Jacob Ganz",
        "title": "Jacob Ganz",
        "tags": [
          "person"
        ],
        "content": "#person",
        "size": 17.142857142857142,
        "index": 271,
        "x": -2270.1138391784716,
        "y": -1434.05530323859,
        "vy": -0.07937823843324772,
        "vx": -0.3726164155159207
      },
      "index": 180
    },
    {
      "source": {
        "id": "Human vs. Machine -  Behavioral Differences between Expert Humans and Language Models in Wargame Simulations",
        "title": "Human vs. Machine -  Behavioral Differences between Expert Humans and Language Models in Wargame Simulations",
        "tags": [
          "project"
        ],
        "content": "#project \n\nhttps://arxiv.org/pdf/2403.03407\n\nPaper examines how LLMs behave compared to human experts when simulating military crisis decision-making through wargames. The study involved 214 national security experts and compared their responses to AI-simulated teams in a fictional U.S.-China crisis scenario over the Taiwan Strait. The research found that while LLMs showed significant overlap with human decision-making patterns, they demonstrated concerning tendencies toward more aggressive actions and were significantly affected by changes in scenario parameters.",
        "size": 20,
        "index": 118,
        "x": -2213.2951352228097,
        "y": -1376.7759485449815,
        "vy": -0.07881523560233798,
        "vx": -0.38620555083276925
      },
      "target": {
        "id": "Oriana Skylar Mastro",
        "title": "Oriana Skylar Mastro",
        "tags": [
          "person"
        ],
        "content": "#person",
        "size": 16.571428571428573,
        "index": 143,
        "x": -2265.2412686853318,
        "y": -1316.9126737793454,
        "vy": 0.03045035863390788,
        "vx": -0.2984688340740783
      },
      "index": 181
    },
    {
      "source": {
        "id": "Human vs. Machine -  Behavioral Differences between Expert Humans and Language Models in Wargame Simulations",
        "title": "Human vs. Machine -  Behavioral Differences between Expert Humans and Language Models in Wargame Simulations",
        "tags": [
          "project"
        ],
        "content": "#project \n\nhttps://arxiv.org/pdf/2403.03407\n\nPaper examines how LLMs behave compared to human experts when simulating military crisis decision-making through wargames. The study involved 214 national security experts and compared their responses to AI-simulated teams in a fictional U.S.-China crisis scenario over the Taiwan Strait. The research found that while LLMs showed significant overlap with human decision-making patterns, they demonstrated concerning tendencies toward more aggressive actions and were significantly affected by changes in scenario parameters.",
        "size": 20,
        "index": 118,
        "x": -2213.2951352228097,
        "y": -1376.7759485449815,
        "vy": -0.07881523560233798,
        "vx": -0.38620555083276925
      },
      "target": {
        "id": "Jacquelyn Schneider",
        "title": "Jacquelyn Schneider",
        "tags": [
          "person"
        ],
        "content": "#person",
        "size": 17.714285714285715,
        "index": 248,
        "x": -2179.5691824765895,
        "y": -1455.2408031463438,
        "vy": -0.0785190756515778,
        "vx": -0.3687066288969788
      },
      "index": 182
    },
    {
      "source": {
        "id": "Human vs. Machine -  Behavioral Differences between Expert Humans and Language Models in Wargame Simulations",
        "title": "Human vs. Machine -  Behavioral Differences between Expert Humans and Language Models in Wargame Simulations",
        "tags": [
          "project"
        ],
        "content": "#project \n\nhttps://arxiv.org/pdf/2403.03407\n\nPaper examines how LLMs behave compared to human experts when simulating military crisis decision-making through wargames. The study involved 214 national security experts and compared their responses to AI-simulated teams in a fictional U.S.-China crisis scenario over the Taiwan Strait. The research found that while LLMs showed significant overlap with human decision-making patterns, they demonstrated concerning tendencies toward more aggressive actions and were significantly affected by changes in scenario parameters.",
        "size": 20,
        "index": 118,
        "x": -2213.2951352228097,
        "y": -1376.7759485449815,
        "vy": -0.07881523560233798,
        "vx": -0.38620555083276925
      },
      "target": {
        "id": "Harold Trinkunas",
        "title": "Harold Trinkunas",
        "tags": [
          "person"
        ],
        "content": "#person",
        "size": 16.571428571428573,
        "index": 313,
        "x": -2291.422696857617,
        "y": -1367.70469698592,
        "vy": 0.07628848556738428,
        "vx": -0.3875341258274366
      },
      "index": 183
    },
    {
      "source": {
        "id": "Hadjar Homaei",
        "title": "Hadjar Homaei",
        "tags": [
          "person"
        ],
        "content": "#person",
        "size": 17.142857142857142,
        "index": 119,
        "x": 2078.3315161452247,
        "y": 736.3969386386419,
        "vy": -1.652546889176861,
        "vx": 0.47318333071124574
      },
      "target": {
        "id": "The Computational Democracy Project",
        "title": "The Computational Democracy Project",
        "tags": [
          "organization"
        ],
        "content": "#organization \n\nhttps://compdemocracy.org/",
        "size": 19.428571428571427,
        "index": 138,
        "x": 2057.334021465157,
        "y": 849.740764420989,
        "vy": -1.5779545596273115,
        "vx": 0.3854928364433583
      },
      "index": 184
    },
    {
      "source": {
        "id": "Philipp Schoenegger",
        "title": "Philipp Schoenegger",
        "tags": [
          "person"
        ],
        "content": "#person \n\nhttps://philipp-schoenegger.weebly.com/",
        "size": 19.428571428571427,
        "index": 120,
        "x": -308.0505670602049,
        "y": -355.92667066361173,
        "vy": -0.7160490223836882,
        "vx": 0.03780161064368572
      },
      "target": {
        "id": "Metaculus",
        "title": "Metaculus",
        "tags": [
          "organization"
        ],
        "content": "#organization\n\nhttps://www.metaculus.com/",
        "size": 20,
        "index": 126,
        "x": -291.65109336863395,
        "y": -589.3419401920876,
        "vy": -0.7257043832009319,
        "vx": -0.15741051377760798
      },
      "index": 185
    },
    {
      "source": {
        "id": "Nuño Sempere",
        "title": "Nuño Sempere",
        "tags": [
          "person"
        ],
        "content": "#person \n\nhttps://nunosempere.com/",
        "size": 17.714285714285715,
        "index": 127,
        "x": -1036.424048922515,
        "y": -1428.1816687468543,
        "vy": -0.23699158830523448,
        "vx": -0.649314981499335
      },
      "target": {
        "id": "Sentinel",
        "title": "Sentinel",
        "tags": [
          "organization"
        ],
        "content": "#organization \n\nhttps://sentinel-team.org",
        "size": 17.142857142857142,
        "index": 69,
        "x": -1070.884753433492,
        "y": -1530.2437019367424,
        "vy": -0.2331068249938424,
        "vx": -0.7395955777031983
      },
      "index": 186
    },
    {
      "source": {
        "id": "Sparse Autoencoders for Hypothesis Generation",
        "title": "Sparse Autoencoders for Hypothesis Generation",
        "tags": [
          "project"
        ],
        "content": "#project \n\nhttps://arxiv.org/pdf/2502.04382\n\nA method that uses sparse autoencoders to automatically generate human-interpretable hypotheses about what features in text data predict certain outcomes (like which headlines get more clicks or what language patterns distinguish Republican vs Democratic speeches). The system works by training neural networks to find interpretable patterns in text, selecting the most predictive patterns, and then using language models to translate those patterns into clear natural language explanations.",
        "size": 18.857142857142858,
        "index": 128,
        "x": -1168.3566575077007,
        "y": 3354.988885430247,
        "vy": -0.3911017052422391,
        "vx": 1.4550923465135963
      },
      "target": {
        "id": "Rajiv Movva",
        "title": "Rajiv Movva",
        "tags": [
          "person"
        ],
        "content": "#person",
        "size": 16.571428571428573,
        "index": 254,
        "x": -1165.0217445557917,
        "y": 3285.5408734103257,
        "vy": -0.3899166700861723,
        "vx": 1.4496934099790553
      },
      "index": 187
    },
    {
      "source": {
        "id": "Sparse Autoencoders for Hypothesis Generation",
        "title": "Sparse Autoencoders for Hypothesis Generation",
        "tags": [
          "project"
        ],
        "content": "#project \n\nhttps://arxiv.org/pdf/2502.04382\n\nA method that uses sparse autoencoders to automatically generate human-interpretable hypotheses about what features in text data predict certain outcomes (like which headlines get more clicks or what language patterns distinguish Republican vs Democratic speeches). The system works by training neural networks to find interpretable patterns in text, selecting the most predictive patterns, and then using language models to translate those patterns into clear natural language explanations.",
        "size": 18.857142857142858,
        "index": 128,
        "x": -1168.3566575077007,
        "y": 3354.988885430247,
        "vy": -0.3911017052422391,
        "vx": 1.4550923465135963
      },
      "target": {
        "id": "Kenny Peng",
        "title": "Kenny Peng",
        "tags": [
          "person"
        ],
        "content": "#person",
        "size": 16.571428571428573,
        "index": 20,
        "x": -1100.7574622203372,
        "y": 3337.194553598093,
        "vy": -0.4244910686825104,
        "vx": 1.4474704136907919
      },
      "index": 188
    },
    {
      "source": {
        "id": "Sparse Autoencoders for Hypothesis Generation",
        "title": "Sparse Autoencoders for Hypothesis Generation",
        "tags": [
          "project"
        ],
        "content": "#project \n\nhttps://arxiv.org/pdf/2502.04382\n\nA method that uses sparse autoencoders to automatically generate human-interpretable hypotheses about what features in text data predict certain outcomes (like which headlines get more clicks or what language patterns distinguish Republican vs Democratic speeches). The system works by training neural networks to find interpretable patterns in text, selecting the most predictive patterns, and then using language models to translate those patterns into clear natural language explanations.",
        "size": 18.857142857142858,
        "index": 128,
        "x": -1168.3566575077007,
        "y": 3354.988885430247,
        "vy": -0.3911017052422391,
        "vx": 1.4550923465135963
      },
      "target": {
        "id": "Nikhil Garg",
        "title": "Nikhil Garg",
        "tags": [
          "person"
        ],
        "content": "#person",
        "size": 16.571428571428573,
        "index": 292,
        "x": -1128.707877583395,
        "y": 3412.009170808478,
        "vy": -0.41659994778550985,
        "vx": 1.4960791908941327
      },
      "index": 189
    },
    {
      "source": {
        "id": "Sparse Autoencoders for Hypothesis Generation",
        "title": "Sparse Autoencoders for Hypothesis Generation",
        "tags": [
          "project"
        ],
        "content": "#project \n\nhttps://arxiv.org/pdf/2502.04382\n\nA method that uses sparse autoencoders to automatically generate human-interpretable hypotheses about what features in text data predict certain outcomes (like which headlines get more clicks or what language patterns distinguish Republican vs Democratic speeches). The system works by training neural networks to find interpretable patterns in text, selecting the most predictive patterns, and then using language models to translate those patterns into clear natural language explanations.",
        "size": 18.857142857142858,
        "index": 128,
        "x": -1168.3566575077007,
        "y": 3354.988885430247,
        "vy": -0.3911017052422391,
        "vx": 1.4550923465135963
      },
      "target": {
        "id": "Jon Kleinberg",
        "title": "Jon Kleinberg",
        "tags": [
          "person"
        ],
        "content": "#person",
        "size": 16.571428571428573,
        "index": 166,
        "x": -1212.0001337832573,
        "y": 3407.5149949711413,
        "vy": -0.3482019037113723,
        "vx": 1.5056071467402843
      },
      "index": 190
    },
    {
      "source": {
        "id": "Sparse Autoencoders for Hypothesis Generation",
        "title": "Sparse Autoencoders for Hypothesis Generation",
        "tags": [
          "project"
        ],
        "content": "#project \n\nhttps://arxiv.org/pdf/2502.04382\n\nA method that uses sparse autoencoders to automatically generate human-interpretable hypotheses about what features in text data predict certain outcomes (like which headlines get more clicks or what language patterns distinguish Republican vs Democratic speeches). The system works by training neural networks to find interpretable patterns in text, selecting the most predictive patterns, and then using language models to translate those patterns into clear natural language explanations.",
        "size": 18.857142857142858,
        "index": 128,
        "x": -1168.3566575077007,
        "y": 3354.988885430247,
        "vy": -0.3911017052422391,
        "vx": 1.4550923465135963
      },
      "target": {
        "id": "Emma Pierson",
        "title": "Emma Pierson",
        "tags": [
          "person"
        ],
        "content": "#person",
        "size": 16.571428571428573,
        "index": 140,
        "x": -1232.3071679230638,
        "y": 3328.5312009090453,
        "vy": -0.316568005979412,
        "vx": 1.4272451896881582
      },
      "index": 191
    },
    {
      "source": {
        "id": "Bruno Marnette",
        "title": "Bruno Marnette",
        "tags": [
          "person"
        ],
        "content": "#person \n\nhttps://brunomarnette.substack.com/",
        "size": 17.714285714285715,
        "index": 129,
        "x": 731.6434970566758,
        "y": -167.79134490423854,
        "vy": -1.1638735314984259,
        "vx": -0.004925001736821977
      },
      "target": {
        "id": "AI Objectives Institute",
        "title": "AI Objectives Institute",
        "tags": [
          "organization"
        ],
        "content": "#organization \n\nhttps://ai.objectives.institute/",
        "size": 20.57142857142857,
        "index": 304,
        "x": 665.7615191436854,
        "y": -247.07081557119588,
        "vy": -1.1515300453530217,
        "vx": -0.07444975689524554
      },
      "index": 192
    },
    {
      "source": {
        "id": "Ben Rachbach",
        "title": "Ben Rachbach",
        "tags": [
          "person"
        ],
        "content": "#person",
        "size": 17.714285714285715,
        "index": 130,
        "x": -1990.652548025034,
        "y": 404.6802552639591,
        "vy": -0.4507815928468619,
        "vx": 0.2575610744521918
      },
      "target": {
        "id": "Elicit",
        "title": "Elicit",
        "tags": [
          "organization"
        ],
        "content": "#organization \n\nhttps://elicit.com/",
        "size": 20.57142857142857,
        "index": 218,
        "x": -2027.2873240845167,
        "y": 375.96539772345454,
        "vy": -0.29711133317878174,
        "vx": 0.26985144319061544
      },
      "index": 193
    },
    {
      "source": {
        "id": "Ben Rachbach",
        "title": "Ben Rachbach",
        "tags": [
          "person"
        ],
        "content": "#person",
        "size": 17.714285714285715,
        "index": 130,
        "x": -1990.652548025034,
        "y": 404.6802552639591,
        "vy": -0.4507815928468619,
        "vx": 0.2575610744521918
      },
      "target": {
        "id": "Ought",
        "title": "Ought",
        "tags": [
          "organization"
        ],
        "content": "#organization \n\nhttps://ought.org/ (website is old)\n\nOught 1.0 has effectively become what is now Elicit (the company) continuing development on Elicit (the tool). Ought 2.0 has not yet done any public work.",
        "size": 23.42857142857143,
        "index": 306,
        "x": -1888.3722954977052,
        "y": 453.0511056583274,
        "vy": -0.2547035865836596,
        "vx": 0.0471838457037637
      },
      "index": 194
    },
    {
      "source": {
        "id": "Ian Beacock",
        "title": "Ian Beacock",
        "tags": [
          "person"
        ],
        "content": "#person",
        "size": 17.142857142857142,
        "index": 134,
        "x": 1636.8020845388053,
        "y": 1015.3881351163002,
        "vy": -1.4459022886097965,
        "vx": 0.3710885486137493
      },
      "target": {
        "id": "Google Jigsaw",
        "title": "Google Jigsaw",
        "tags": [
          "organization"
        ],
        "content": "#organization \n\nhttps://jigsaw.google.com/",
        "size": 19.428571428571427,
        "index": 390,
        "x": 1784.38820840888,
        "y": 1035.7563657120636,
        "vy": -1.5493284452037834,
        "vx": 0.4016872013074959
      },
      "index": 195
    },
    {
      "source": {
        "id": "Factored Cognition Primer",
        "title": "Factored Cognition Primer",
        "tags": [
          "project"
        ],
        "content": "#project \n\nhttps://primer.ought.org/\n\nTutorial that teaches how to write \"compositional language model programs\" using factored cognition - a technique that breaks down complex reasoning tasks into smaller, manageable subtasks that can be solved recursively through methods like question-answering and debate. This approach aims to make AI reasoning more transparent and supervisable by decomposing sophisticated thinking into many small, independent tasks that humans can verify.",
        "size": 16.571428571428573,
        "index": 142,
        "x": -1885.318632019537,
        "y": 546.5167869305374,
        "vy": -0.282738009021088,
        "vx": 0.15982992446394703
      },
      "target": {
        "id": "Ought",
        "title": "Ought",
        "tags": [
          "organization"
        ],
        "content": "#organization \n\nhttps://ought.org/ (website is old)\n\nOught 1.0 has effectively become what is now Elicit (the company) continuing development on Elicit (the tool). Ought 2.0 has not yet done any public work.",
        "size": 23.42857142857143,
        "index": 306,
        "x": -1888.3722954977052,
        "y": 453.0511056583274,
        "vy": -0.2547035865836596,
        "vx": 0.0471838457037637
      },
      "index": 196
    },
    {
      "source": {
        "id": "Generative Social Choice",
        "title": "Generative Social Choice",
        "tags": [
          "project"
        ],
        "content": "#project \n\nhttps://arxiv.org/pdf/2309.01291\n\nPaper introduces a framework that combines LLMs with social choice theory to enable democratic decision-making on open-ended questions where traditional voting on predetermined alternatives isn't sufficient. The system can generate new consensus statements from diverse participant opinions and predict how well individuals would agree with any statement.\n\nFunded by [OpenAI Democratic Inputs to AI](https://github.com/openai/democratic-inputs).",
        "size": 20,
        "index": 144,
        "x": 2000.2661562421904,
        "y": 282.115527354089,
        "vy": -1.6424640040534457,
        "vx": 0.4633267788741248
      },
      "target": {
        "id": "Sara Fish",
        "title": "Sara Fish",
        "tags": [
          "person"
        ],
        "content": "#person",
        "size": 16.571428571428573,
        "index": 117,
        "x": 2115.737299394384,
        "y": 224.70085490593098,
        "vy": -1.7993613938286626,
        "vx": 0.09270280012978095
      },
      "index": 197
    },
    {
      "source": {
        "id": "Generative Social Choice",
        "title": "Generative Social Choice",
        "tags": [
          "project"
        ],
        "content": "#project \n\nhttps://arxiv.org/pdf/2309.01291\n\nPaper introduces a framework that combines LLMs with social choice theory to enable democratic decision-making on open-ended questions where traditional voting on predetermined alternatives isn't sufficient. The system can generate new consensus statements from diverse participant opinions and predict how well individuals would agree with any statement.\n\nFunded by [OpenAI Democratic Inputs to AI](https://github.com/openai/democratic-inputs).",
        "size": 20,
        "index": 144,
        "x": 2000.2661562421904,
        "y": 282.115527354089,
        "vy": -1.6424640040534457,
        "vx": 0.4633267788741248
      },
      "target": {
        "id": "Paul Gölz",
        "title": "Paul Gölz",
        "tags": [
          "person"
        ],
        "content": "#person",
        "size": 16.571428571428573,
        "index": 309,
        "x": 2128.522044837979,
        "y": 280.17413887250194,
        "vy": -1.6889344065886471,
        "vx": -0.13251891392357537
      },
      "index": 198
    },
    {
      "source": {
        "id": "Generative Social Choice",
        "title": "Generative Social Choice",
        "tags": [
          "project"
        ],
        "content": "#project \n\nhttps://arxiv.org/pdf/2309.01291\n\nPaper introduces a framework that combines LLMs with social choice theory to enable democratic decision-making on open-ended questions where traditional voting on predetermined alternatives isn't sufficient. The system can generate new consensus statements from diverse participant opinions and predict how well individuals would agree with any statement.\n\nFunded by [OpenAI Democratic Inputs to AI](https://github.com/openai/democratic-inputs).",
        "size": 20,
        "index": 144,
        "x": 2000.2661562421904,
        "y": 282.115527354089,
        "vy": -1.6424640040534457,
        "vx": 0.4633267788741248
      },
      "target": {
        "id": "David Parkes",
        "title": "David Parkes",
        "tags": [
          "person"
        ],
        "content": "#person",
        "size": 18.285714285714285,
        "index": 133,
        "x": 1720.2221261795037,
        "y": 282.75903684678264,
        "vy": -1.5768079791609506,
        "vx": 0.16009266454570117
      },
      "index": 199
    },
    {
      "source": {
        "id": "Generative Social Choice",
        "title": "Generative Social Choice",
        "tags": [
          "project"
        ],
        "content": "#project \n\nhttps://arxiv.org/pdf/2309.01291\n\nPaper introduces a framework that combines LLMs with social choice theory to enable democratic decision-making on open-ended questions where traditional voting on predetermined alternatives isn't sufficient. The system can generate new consensus statements from diverse participant opinions and predict how well individuals would agree with any statement.\n\nFunded by [OpenAI Democratic Inputs to AI](https://github.com/openai/democratic-inputs).",
        "size": 20,
        "index": 144,
        "x": 2000.2661562421904,
        "y": 282.115527354089,
        "vy": -1.6424640040534457,
        "vx": 0.4633267788741248
      },
      "target": {
        "id": "Ariel Procaccia",
        "title": "Ariel Procaccia",
        "tags": [
          "person"
        ],
        "content": "#person \n\nhttps://procaccia.info/",
        "size": 17.142857142857142,
        "index": 30,
        "x": 1767.4207500457344,
        "y": 495.40167289038965,
        "vy": -1.5944030237308504,
        "vx": 0.18021375498871298
      },
      "index": 200
    },
    {
      "source": {
        "id": "Generative Social Choice",
        "title": "Generative Social Choice",
        "tags": [
          "project"
        ],
        "content": "#project \n\nhttps://arxiv.org/pdf/2309.01291\n\nPaper introduces a framework that combines LLMs with social choice theory to enable democratic decision-making on open-ended questions where traditional voting on predetermined alternatives isn't sufficient. The system can generate new consensus statements from diverse participant opinions and predict how well individuals would agree with any statement.\n\nFunded by [OpenAI Democratic Inputs to AI](https://github.com/openai/democratic-inputs).",
        "size": 20,
        "index": 144,
        "x": 2000.2661562421904,
        "y": 282.115527354089,
        "vy": -1.6424640040534457,
        "vx": 0.4633267788741248
      },
      "target": {
        "id": "Gili Rusak",
        "title": "Gili Rusak",
        "tags": [
          "person"
        ],
        "content": "#person",
        "size": 16.571428571428573,
        "index": 57,
        "x": 2078.0982759157587,
        "y": 314.993988535065,
        "vy": -1.8666143325261995,
        "vx": -0.1916936693237186
      },
      "index": 201
    },
    {
      "source": {
        "id": "Generative Social Choice",
        "title": "Generative Social Choice",
        "tags": [
          "project"
        ],
        "content": "#project \n\nhttps://arxiv.org/pdf/2309.01291\n\nPaper introduces a framework that combines LLMs with social choice theory to enable democratic decision-making on open-ended questions where traditional voting on predetermined alternatives isn't sufficient. The system can generate new consensus statements from diverse participant opinions and predict how well individuals would agree with any statement.\n\nFunded by [OpenAI Democratic Inputs to AI](https://github.com/openai/democratic-inputs).",
        "size": 20,
        "index": 144,
        "x": 2000.2661562421904,
        "y": 282.115527354089,
        "vy": -1.6424640040534457,
        "vx": 0.4633267788741248
      },
      "target": {
        "id": "Itai Shapira",
        "title": "Itai Shapira",
        "tags": [
          "person"
        ],
        "content": "#person",
        "size": 16.571428571428573,
        "index": 193,
        "x": 2069.1543214687053,
        "y": 259.1800226103808,
        "vy": -1.8983341901042783,
        "vx": 0.07503173657146109
      },
      "index": 202
    },
    {
      "source": {
        "id": "Generative Social Choice",
        "title": "Generative Social Choice",
        "tags": [
          "project"
        ],
        "content": "#project \n\nhttps://arxiv.org/pdf/2309.01291\n\nPaper introduces a framework that combines LLMs with social choice theory to enable democratic decision-making on open-ended questions where traditional voting on predetermined alternatives isn't sufficient. The system can generate new consensus statements from diverse participant opinions and predict how well individuals would agree with any statement.\n\nFunded by [OpenAI Democratic Inputs to AI](https://github.com/openai/democratic-inputs).",
        "size": 20,
        "index": 144,
        "x": 2000.2661562421904,
        "y": 282.115527354089,
        "vy": -1.6424640040534457,
        "vx": 0.4633267788741248
      },
      "target": {
        "id": "Manuel Wüthrich",
        "title": "Manuel Wüthrich",
        "tags": [
          "person"
        ],
        "content": "#person",
        "size": 17.714285714285715,
        "index": 393,
        "x": 2220.4196169294496,
        "y": 182.2761367020348,
        "vy": -1.9336271138999594,
        "vx": -0.03418308573307342
      },
      "index": 203
    },
    {
      "source": {
        "id": "Amplifying transformative potential while designing augmented deliberative systems",
        "title": "Amplifying transformative potential while designing augmented deliberative systems",
        "tags": [
          "project"
        ],
        "content": "#project \n\nhttps://ai.objectives.institute/blog/amplifying-transformative-potential-while-designing-augmented-deliberative-systems\n\nBlog post presenting the \"Goldilocks Framework for Augmented Group Intelligence,\" which proposes principles for designing AI systems that enhance human deliberation and decision-making while preserving authentic human agency. The framework argues that effective AI-augmented deliberation requires balancing two key elements: \n1. participants' ability to understand and steer AI outputs (human agency) and \n2. their commitment to deliberative outcomes \n\nHigher-commitment decisions requiring correspondingly higher human agency.",
        "size": 16.571428571428573,
        "index": 145,
        "x": 700.6406862856695,
        "y": -77.10666514285275,
        "vy": -1.1677296576331075,
        "vx": 0.023249733635715142
      },
      "target": {
        "id": "Shu Yang Lin",
        "title": "Shu Yang Lin",
        "tags": [
          "person"
        ],
        "content": "#person \n\nhttps://www.shuyanglin.com/",
        "size": 18.285714285714285,
        "index": 241,
        "x": 771.7824733892007,
        "y": -32.82433362604927,
        "vy": -1.1943655944737657,
        "vx": -0.00035972909812223005
      },
      "index": 204
    },
    {
      "source": {
        "id": "Creating a large language model of a philosopher",
        "title": "Creating a large language model of a philosopher",
        "tags": [
          "project"
        ],
        "content": "#project \n\nhttps://arxiv.org/abs/2302.01339\n\nExperiment where they fine-tuned GPT-3 on the works of Daniel Dennett to see if an LLM can produce expert-quality philosophical texts. They tested the \"digi-Dan\" model by asking both the real Dennett and the AI model ten philosophical questions, then had 425 participants try to distinguish between Dennett's actual answers and the machine-generated responses. The study found that experts often chose the AI's answers over Dennett's actual responses, and on two questions, the AI outputs were selected by more experts than Dennett's own answers.",
        "size": 18.285714285714285,
        "index": 147,
        "x": -191.03887356290213,
        "y": 2454.014419868978,
        "vy": -0.06299306082960622,
        "vx": 1.6859137911892892
      },
      "target": {
        "id": "Eric Schwitzgebel",
        "title": "Eric Schwitzgebel",
        "tags": [
          "person"
        ],
        "content": "#person",
        "size": 16.571428571428573,
        "index": 252,
        "x": -204.58100920442638,
        "y": 2528.4131610049744,
        "vy": -0.07423171220330382,
        "vx": 1.7531639174227476
      },
      "index": 205
    },
    {
      "source": {
        "id": "Creating a large language model of a philosopher",
        "title": "Creating a large language model of a philosopher",
        "tags": [
          "project"
        ],
        "content": "#project \n\nhttps://arxiv.org/abs/2302.01339\n\nExperiment where they fine-tuned GPT-3 on the works of Daniel Dennett to see if an LLM can produce expert-quality philosophical texts. They tested the \"digi-Dan\" model by asking both the real Dennett and the AI model ten philosophical questions, then had 425 participants try to distinguish between Dennett's actual answers and the machine-generated responses. The study found that experts often chose the AI's answers over Dennett's actual responses, and on two questions, the AI outputs were selected by more experts than Dennett's own answers.",
        "size": 18.285714285714285,
        "index": 147,
        "x": -191.03887356290213,
        "y": 2454.014419868978,
        "vy": -0.06299306082960622,
        "vx": 1.6859137911892892
      },
      "target": {
        "id": "David Schwitzgebel",
        "title": "David Schwitzgebel",
        "tags": [
          "person"
        ],
        "content": "#person",
        "size": 16.571428571428573,
        "index": 50,
        "x": -266.6761943198043,
        "y": 2454.8521255812566,
        "vy": 0.01201001986875525,
        "vx": 1.6748469613804664
      },
      "index": 206
    },
    {
      "source": {
        "id": "Creating a large language model of a philosopher",
        "title": "Creating a large language model of a philosopher",
        "tags": [
          "project"
        ],
        "content": "#project \n\nhttps://arxiv.org/abs/2302.01339\n\nExperiment where they fine-tuned GPT-3 on the works of Daniel Dennett to see if an LLM can produce expert-quality philosophical texts. They tested the \"digi-Dan\" model by asking both the real Dennett and the AI model ten philosophical questions, then had 425 participants try to distinguish between Dennett's actual answers and the machine-generated responses. The study found that experts often chose the AI's answers over Dennett's actual responses, and on two questions, the AI outputs were selected by more experts than Dennett's own answers.",
        "size": 18.285714285714285,
        "index": 147,
        "x": -191.03887356290213,
        "y": 2454.014419868978,
        "vy": -0.06299306082960622,
        "vx": 1.6859137911892892
      },
      "target": {
        "id": "Anna Strasser",
        "title": "Anna Strasser",
        "tags": [
          "person"
        ],
        "content": "#person",
        "size": 16.571428571428573,
        "index": 344,
        "x": -259.0789864987981,
        "y": 2511.229859108959,
        "vy": 0.0053490980277106194,
        "vx": 1.7279669210024728
      },
      "index": 207
    },
    {
      "source": {
        "id": "What's Important in \"AI for Epistemics\"?",
        "title": "What's Important in \"AI for Epistemics\"?",
        "tags": [
          "project"
        ],
        "content": "#project \n\nhttps://www.forethought.org/research/whats-important-in-ai-for-epistemics\n\nAnalysis of priorities and strategies for developing \"AI for epistemics\": essentially AI tools that help humans and organizations make better, more informed decisions by improving how we discover, evaluate, and act on information. The paper argues that as AI becomes more powerful, it will dramatically reshape how society generates knowledge and makes decisions, creating both opportunities to enhance human reasoning and risks of manipulation or poor coordination.",
        "size": 16.571428571428573,
        "index": 148,
        "x": -1693.3007157168433,
        "y": 424.986550231917,
        "vy": 0.03216155728476351,
        "vx": 0.20900920449032048
      },
      "target": {
        "id": "Lukas Finnveden",
        "title": "Lukas Finnveden",
        "tags": [
          "person"
        ],
        "content": "#person",
        "size": 18.285714285714285,
        "index": 256,
        "x": -1770.6095471676322,
        "y": 430.1205037586758,
        "vy": -0.22586013414071307,
        "vx": 0.1963978919880959
      },
      "index": 208
    },
    {
      "source": {
        "id": "Making Artificial Intelligence Work for Investigative Journalism",
        "title": "Making Artificial Intelligence Work for Investigative Journalism",
        "tags": [
          "project"
        ],
        "content": "#project \n\nhttps://www.tandfonline.com/doi/full/10.1080/21670811.2019.1630289\n\nPaper examines how artificial intelligence can be applied to investigative journalism, focusing on the practical challenges and realistic opportunities for AI tools in newsrooms. The research reveals that while AI has enormous theoretical potential for helping journalists analyze large datasets and uncover hidden patterns of public interest, current applications remain limited to relatively simple tasks like document classification and data cleaning.",
        "size": 16.571428571428573,
        "index": 149,
        "x": 1661.758339817345,
        "y": 1084.58986120295,
        "vy": -1.3937440729080688,
        "vx": 0.48209326570363975
      },
      "target": {
        "id": "Jonathan Stray",
        "title": "Jonathan Stray",
        "tags": [
          "person"
        ],
        "content": "#person \n\nhttp://jonathanstray.com/",
        "size": 17.142857142857142,
        "index": 4,
        "x": 1557.4840034772917,
        "y": 985.1290569276915,
        "vy": -1.2999813278401229,
        "vx": 0.2968571996859188
      },
      "index": 209
    },
    {
      "source": {
        "id": "Ezra Karger",
        "title": "Ezra Karger",
        "tags": [
          "person"
        ],
        "content": "#person \n\nhttps://ezrakarger.com/",
        "size": 18.285714285714285,
        "index": 150,
        "x": 13.294680683389643,
        "y": 76.75026712023329,
        "vy": -0.8178432271089577,
        "vx": 0.14711852217550486
      },
      "target": {
        "id": "Forecasting Research Institute",
        "title": "Forecasting Research Institute",
        "tags": [
          "organization"
        ],
        "content": "#organization \n\nhttps://forecastingresearch.org/",
        "size": 18.857142857142858,
        "index": 317,
        "x": -153.6122917815696,
        "y": -87.61093230240742,
        "vy": -0.8005381090437106,
        "vx": 0.1603930148987801
      },
      "index": 210
    },
    {
      "source": {
        "id": "LLMediator",
        "title": "LLMediator",
        "tags": [
          "project"
        ],
        "content": "#project \n\nhttps://arxiv.org/pdf/2307.16732\n\nAn experimental platform that uses GPT-4 to enhance online dispute resolution by helping parties communicate more effectively and reach amicable settlements. The system offers three key features: \n1. reformulating inflammatory messages to be less confrontational while preserving their core meaning, \n2. generating draft intervention messages for human mediators to guide discussions, and \n3. allowing AI to autonomously mediate certain low-stakes disputes.",
        "size": 17.714285714285715,
        "index": 151,
        "x": 2220.8672292388665,
        "y": -2295.825191344095,
        "vy": -1.966593254868449,
        "vx": -0.5901928723570321
      },
      "target": {
        "id": "Hannes Westermann",
        "title": "Hannes Westermann",
        "tags": [
          "person"
        ],
        "content": "#person",
        "size": 16.571428571428573,
        "index": 398,
        "x": 2286.0759951959867,
        "y": -2301.378577681666,
        "vy": -1.998457639281553,
        "vx": -0.5939952079739809
      },
      "index": 211
    },
    {
      "source": {
        "id": "LLMediator",
        "title": "LLMediator",
        "tags": [
          "project"
        ],
        "content": "#project \n\nhttps://arxiv.org/pdf/2307.16732\n\nAn experimental platform that uses GPT-4 to enhance online dispute resolution by helping parties communicate more effectively and reach amicable settlements. The system offers three key features: \n1. reformulating inflammatory messages to be less confrontational while preserving their core meaning, \n2. generating draft intervention messages for human mediators to guide discussions, and \n3. allowing AI to autonomously mediate certain low-stakes disputes.",
        "size": 17.714285714285715,
        "index": 151,
        "x": 2220.8672292388665,
        "y": -2295.825191344095,
        "vy": -1.966593254868449,
        "vx": -0.5901928723570321
      },
      "target": {
        "id": "Jaromir Savelka",
        "title": "Jaromir Savelka",
        "tags": [
          "person"
        ],
        "content": "#person",
        "size": 16.571428571428573,
        "index": 261,
        "x": 2206.1527616859025,
        "y": -2232.476065623996,
        "vy": -1.9656046052369873,
        "vx": -0.5855412538459536
      },
      "index": 212
    },
    {
      "source": {
        "id": "LLMediator",
        "title": "LLMediator",
        "tags": [
          "project"
        ],
        "content": "#project \n\nhttps://arxiv.org/pdf/2307.16732\n\nAn experimental platform that uses GPT-4 to enhance online dispute resolution by helping parties communicate more effectively and reach amicable settlements. The system offers three key features: \n1. reformulating inflammatory messages to be less confrontational while preserving their core meaning, \n2. generating draft intervention messages for human mediators to guide discussions, and \n3. allowing AI to autonomously mediate certain low-stakes disputes.",
        "size": 17.714285714285715,
        "index": 151,
        "x": 2220.8672292388665,
        "y": -2295.825191344095,
        "vy": -1.966593254868449,
        "vx": -0.5901928723570321
      },
      "target": {
        "id": "Karim Benyekhlef",
        "title": "Karim Benyekhlef",
        "tags": [
          "person"
        ],
        "content": "#person",
        "size": 16.571428571428573,
        "index": 368,
        "x": 2175.6970660772563,
        "y": -2341.4728709137075,
        "vy": -1.930654153099359,
        "vx": -0.6294540883244718
      },
      "index": 213
    },
    {
      "source": {
        "id": "Daniel Kokotajlo",
        "title": "Daniel Kokotajlo",
        "tags": [
          "person"
        ],
        "content": "#person",
        "size": 17.142857142857142,
        "index": 156,
        "x": -2287.9333745974973,
        "y": 382.44662513897083,
        "vy": -0.2659387512272784,
        "vx": 0.14156672468047232
      },
      "target": {
        "id": "AI Future Project",
        "title": "AI Future Project",
        "tags": [
          "organization"
        ],
        "content": "#organization \n\nhttps://ai-futures.org/",
        "size": 17.142857142857142,
        "index": 203,
        "x": -2227.808470514541,
        "y": 447.0357084853911,
        "vy": -0.25788816758637106,
        "vx": 0.14402888519418866
      },
      "index": 214
    },
    {
      "source": {
        "id": "Tell Me Why - Incentivizing Explanations",
        "title": "Tell Me Why - Incentivizing Explanations",
        "tags": [
          "project"
        ],
        "content": "#project \n\nhttps://arxiv.org/abs/2502.13410\n\nPaper presents a \"deliberation mechanism\" that uses economic incentives to get experts to explain their reasoning, not just state their conclusions. The mechanism works through a three-party structure: experts submit both beliefs and explanations to a supervisor, who then reports aggregated beliefs to a principal, with all parties scored using proper scoring rules based on accuracy. The key insight is that a supervisor can credibly commit to ignoring any expert reports that lack explanations, which forces experts to provide rationales despite the extra effort required, because without explanations, experts receive zero reward.",
        "size": 18.285714285714285,
        "index": 158,
        "x": 722.9932566978406,
        "y": 435.1233355907172,
        "vy": -1.0855387666245526,
        "vx": 0.20451252297606695
      },
      "target": {
        "id": "Siddarth Srinivasan",
        "title": "Siddarth Srinivasan",
        "tags": [
          "person"
        ],
        "content": "#person \n",
        "size": 16.571428571428573,
        "index": 105,
        "x": 612.7560851620352,
        "y": 390.90148399012725,
        "vy": -1.0927348977165001,
        "vx": 0.20037215812293108
      },
      "index": 215
    },
    {
      "source": {
        "id": "Tell Me Why - Incentivizing Explanations",
        "title": "Tell Me Why - Incentivizing Explanations",
        "tags": [
          "project"
        ],
        "content": "#project \n\nhttps://arxiv.org/abs/2502.13410\n\nPaper presents a \"deliberation mechanism\" that uses economic incentives to get experts to explain their reasoning, not just state their conclusions. The mechanism works through a three-party structure: experts submit both beliefs and explanations to a supervisor, who then reports aggregated beliefs to a principal, with all parties scored using proper scoring rules based on accuracy. The key insight is that a supervisor can credibly commit to ignoring any expert reports that lack explanations, which forces experts to provide rationales despite the extra effort required, because without explanations, experts receive zero reward.",
        "size": 18.285714285714285,
        "index": 158,
        "x": 722.9932566978406,
        "y": 435.1233355907172,
        "vy": -1.0855387666245526,
        "vx": 0.20451252297606695
      },
      "target": {
        "id": "Ezra Karger",
        "title": "Ezra Karger",
        "tags": [
          "person"
        ],
        "content": "#person \n\nhttps://ezrakarger.com/",
        "size": 18.285714285714285,
        "index": 150,
        "x": 13.294680683389643,
        "y": 76.75026712023329,
        "vy": -0.8178432271089577,
        "vx": 0.14711852217550486
      },
      "index": 216
    },
    {
      "source": {
        "id": "Tell Me Why - Incentivizing Explanations",
        "title": "Tell Me Why - Incentivizing Explanations",
        "tags": [
          "project"
        ],
        "content": "#project \n\nhttps://arxiv.org/abs/2502.13410\n\nPaper presents a \"deliberation mechanism\" that uses economic incentives to get experts to explain their reasoning, not just state their conclusions. The mechanism works through a three-party structure: experts submit both beliefs and explanations to a supervisor, who then reports aggregated beliefs to a principal, with all parties scored using proper scoring rules based on accuracy. The key insight is that a supervisor can credibly commit to ignoring any expert reports that lack explanations, which forces experts to provide rationales despite the extra effort required, because without explanations, experts receive zero reward.",
        "size": 18.285714285714285,
        "index": 158,
        "x": 722.9932566978406,
        "y": 435.1233355907172,
        "vy": -1.0855387666245526,
        "vx": 0.20451252297606695
      },
      "target": {
        "id": "Michiel Bakker",
        "title": "Michiel Bakker",
        "tags": [
          "person"
        ],
        "content": "#person \n\nhttps://miba.dev/",
        "size": 21.142857142857142,
        "index": 284,
        "x": 1225.501866244462,
        "y": 730.5079873536822,
        "vy": -1.2714527484014098,
        "vx": 0.27625785273055353
      },
      "index": 217
    },
    {
      "source": {
        "id": "Tell Me Why - Incentivizing Explanations",
        "title": "Tell Me Why - Incentivizing Explanations",
        "tags": [
          "project"
        ],
        "content": "#project \n\nhttps://arxiv.org/abs/2502.13410\n\nPaper presents a \"deliberation mechanism\" that uses economic incentives to get experts to explain their reasoning, not just state their conclusions. The mechanism works through a three-party structure: experts submit both beliefs and explanations to a supervisor, who then reports aggregated beliefs to a principal, with all parties scored using proper scoring rules based on accuracy. The key insight is that a supervisor can credibly commit to ignoring any expert reports that lack explanations, which forces experts to provide rationales despite the extra effort required, because without explanations, experts receive zero reward.",
        "size": 18.285714285714285,
        "index": 158,
        "x": 722.9932566978406,
        "y": 435.1233355907172,
        "vy": -1.0855387666245526,
        "vx": 0.20451252297606695
      },
      "target": {
        "id": "Yiling Chen",
        "title": "Yiling Chen",
        "tags": [
          "person"
        ],
        "content": "#person \n",
        "size": 16.571428571428573,
        "index": 191,
        "x": 664.051508207191,
        "y": 365.72042602708405,
        "vy": -1.0901503261075924,
        "vx": 0.20164112477156718
      },
      "index": 218
    },
    {
      "source": {
        "id": "Amelia Wattenberger",
        "title": "Amelia Wattenberger",
        "tags": [
          "person"
        ],
        "content": "#person \n\nhttps://wattenberger.com\nhttps://github.com/Wattenberger",
        "size": 17.142857142857142,
        "index": 159,
        "x": 2822.658368554034,
        "y": -1995.5627131174708,
        "vy": -1.8916671969526198,
        "vx": -0.3849671573117643
      },
      "target": {
        "id": "GitHub Next",
        "title": "GitHub Next",
        "tags": [
          "organization"
        ],
        "content": "#organization \n\nhttps://githubnext.com",
        "size": 16.571428571428573,
        "index": 360,
        "x": 2860.1705097321346,
        "y": -1944.5392066794366,
        "vy": -1.8409038432611364,
        "vx": -0.2502258714770864
      },
      "index": 219
    },
    {
      "source": {
        "id": "Metaforecast",
        "title": "Metaforecast",
        "tags": [
          "project"
        ],
        "content": "#project \n\nhttps://metaforecast.org\n\nSearch engine and aggregator for predictions and forecasts from various prediction markets and forecasting platforms, allowing users to search for probability estimates on topics like geopolitics, technology, and other events. It provides quality ratings for forecasts and tools to help users navigate and understand prediction data across multiple platforms.\n\n(not currently maintained)",
        "size": 17.714285714285715,
        "index": 160,
        "x": -911.2498873715613,
        "y": -1287.3762821910332,
        "vy": -0.22764532301028506,
        "vx": -0.5586516888668817
      },
      "target": {
        "id": "Nuño Sempere",
        "title": "Nuño Sempere",
        "tags": [
          "person"
        ],
        "content": "#person \n\nhttps://nunosempere.com/",
        "size": 17.714285714285715,
        "index": 127,
        "x": -1036.424048922515,
        "y": -1428.1816687468543,
        "vy": -0.23699158830523448,
        "vx": -0.649314981499335
      },
      "index": 220
    },
    {
      "source": {
        "id": "Metaforecast",
        "title": "Metaforecast",
        "tags": [
          "project"
        ],
        "content": "#project \n\nhttps://metaforecast.org\n\nSearch engine and aggregator for predictions and forecasts from various prediction markets and forecasting platforms, allowing users to search for probability estimates on topics like geopolitics, technology, and other events. It provides quality ratings for forecasts and tools to help users navigate and understand prediction data across multiple platforms.\n\n(not currently maintained)",
        "size": 17.714285714285715,
        "index": 160,
        "x": -911.2498873715613,
        "y": -1287.3762821910332,
        "vy": -0.22764532301028506,
        "vx": -0.5586516888668817
      },
      "target": {
        "id": "Ozzie Gooen",
        "title": "Ozzie Gooen",
        "tags": [
          "person"
        ],
        "content": "#person",
        "size": 18.285714285714285,
        "index": 352,
        "x": -795.6299935195096,
        "y": -1201.5289699537964,
        "vy": -0.36410862655798687,
        "vx": -0.5257118812416671
      },
      "index": 221
    },
    {
      "source": {
        "id": "What are human values,and how do we align AI to them?",
        "title": "What are human values,and how do we align AI to them?",
        "tags": [
          "project"
        ],
        "content": "#project \n\nhttps://arxiv.org/pdf/2404.10636\n\nPaper introduces \"Moral Graph Elicitation\" (MGE), a method for systematically collecting and organizing human values to better align AI systems with what people actually care about. The researchers developed a process that uses AI-powered interviews to elicit specific \"values cards\" from people (detailed descriptions of what they pay attention to when making meaningful choices) then connects these values in a graph structure based on which values participants consider \"wiser\" than others.",
        "size": 18.285714285714285,
        "index": 162,
        "x": 1253.099985655064,
        "y": -246.90291544937514,
        "vy": -1.3156260391726693,
        "vx": -0.04877623727693523
      },
      "target": {
        "id": "Oliver Klingefjord",
        "title": "Oliver Klingefjord",
        "tags": [
          "person"
        ],
        "content": "#person \n\nhttps://www.klingefjord.com/",
        "size": 18.285714285714285,
        "index": 44,
        "x": 1178.5910932786057,
        "y": -246.8472025448904,
        "vy": -1.2893218733044343,
        "vx": -0.05174057759054609
      },
      "index": 222
    },
    {
      "source": {
        "id": "What are human values,and how do we align AI to them?",
        "title": "What are human values,and how do we align AI to them?",
        "tags": [
          "project"
        ],
        "content": "#project \n\nhttps://arxiv.org/pdf/2404.10636\n\nPaper introduces \"Moral Graph Elicitation\" (MGE), a method for systematically collecting and organizing human values to better align AI systems with what people actually care about. The researchers developed a process that uses AI-powered interviews to elicit specific \"values cards\" from people (detailed descriptions of what they pay attention to when making meaningful choices) then connects these values in a graph structure based on which values participants consider \"wiser\" than others.",
        "size": 18.285714285714285,
        "index": 162,
        "x": 1253.099985655064,
        "y": -246.90291544937514,
        "vy": -1.3156260391726693,
        "vx": -0.04877623727693523
      },
      "target": {
        "id": "Ryan Lowe",
        "title": "Ryan Lowe",
        "tags": [
          "person"
        ],
        "content": "#person",
        "size": 17.142857142857142,
        "index": 354,
        "x": 1279.688024262066,
        "y": -362.91454807188126,
        "vy": -1.3197815536317754,
        "vx": -0.10514754429952329
      },
      "index": 223
    },
    {
      "source": {
        "id": "What are human values,and how do we align AI to them?",
        "title": "What are human values,and how do we align AI to them?",
        "tags": [
          "project"
        ],
        "content": "#project \n\nhttps://arxiv.org/pdf/2404.10636\n\nPaper introduces \"Moral Graph Elicitation\" (MGE), a method for systematically collecting and organizing human values to better align AI systems with what people actually care about. The researchers developed a process that uses AI-powered interviews to elicit specific \"values cards\" from people (detailed descriptions of what they pay attention to when making meaningful choices) then connects these values in a graph structure based on which values participants consider \"wiser\" than others.",
        "size": 18.285714285714285,
        "index": 162,
        "x": 1253.099985655064,
        "y": -246.90291544937514,
        "vy": -1.3156260391726693,
        "vx": -0.04877623727693523
      },
      "target": {
        "id": "Joe Edelman",
        "title": "Joe Edelman",
        "tags": [
          "person"
        ],
        "content": "#person \n\nhttps://nxhx.org/",
        "size": 17.714285714285715,
        "index": 268,
        "x": 1356.6939030945252,
        "y": -196.06959723472013,
        "vy": -1.3527938545248293,
        "vx": -0.025787874230328266
      },
      "index": 224
    },
    {
      "source": {
        "id": "Wargames & TTX",
        "title": "Wargames & TTX",
        "tags": [
          "topic"
        ],
        "content": "#topic",
        "size": 18.857142857142858,
        "index": 164,
        "x": -1892.2380713597158,
        "y": -1126.8699947554496,
        "vy": -0.12046666673136741,
        "vx": -0.36410177562718715
      },
      "target": {
        "id": "Wargames for Peace",
        "title": "Wargames for Peace",
        "tags": [
          "project"
        ],
        "content": "#project \n\nhttps://www.youtube.com/watch?v=9_zSzjTWr8A\n\n[MATS](https://www.matsprogram.org/) project to simulate  a tabletop exercises with LLMs, allowing a human to play solo.",
        "size": 18.285714285714285,
        "index": 15,
        "x": -2216.0206491589165,
        "y": 317.47789193596424,
        "vy": -0.26235631740140714,
        "vx": 0.11999156457905648
      },
      "type": "topic_link",
      "index": 225
    },
    {
      "source": {
        "id": "Wargames & TTX",
        "title": "Wargames & TTX",
        "tags": [
          "topic"
        ],
        "content": "#topic",
        "size": 18.857142857142858,
        "index": 164,
        "x": -1892.2380713597158,
        "y": -1126.8699947554496,
        "vy": -0.12046666673136741,
        "vx": -0.36410177562718715
      },
      "target": {
        "id": "Human vs. Machine -  Behavioral Differences between Expert Humans and Language Models in Wargame Simulations",
        "title": "Human vs. Machine -  Behavioral Differences between Expert Humans and Language Models in Wargame Simulations",
        "tags": [
          "project"
        ],
        "content": "#project \n\nhttps://arxiv.org/pdf/2403.03407\n\nPaper examines how LLMs behave compared to human experts when simulating military crisis decision-making through wargames. The study involved 214 national security experts and compared their responses to AI-simulated teams in a fictional U.S.-China crisis scenario over the Taiwan Strait. The research found that while LLMs showed significant overlap with human decision-making patterns, they demonstrated concerning tendencies toward more aggressive actions and were significantly affected by changes in scenario parameters.",
        "size": 20,
        "index": 118,
        "x": -2213.2951352228097,
        "y": -1376.7759485449815,
        "vy": -0.07881523560233798,
        "vx": -0.38620555083276925
      },
      "type": "topic_link",
      "index": 226
    },
    {
      "source": {
        "id": "Wargames & TTX",
        "title": "Wargames & TTX",
        "tags": [
          "topic"
        ],
        "content": "#topic",
        "size": 18.857142857142858,
        "index": 164,
        "x": -1892.2380713597158,
        "y": -1126.8699947554496,
        "vy": -0.12046666673136741,
        "vx": -0.36410177562718715
      },
      "target": {
        "id": "Snow Globe",
        "title": "Snow Globe",
        "tags": [
          "project"
        ],
        "content": "#project \n\nhttps://github.com/IQTLabs/snowglobe\nhttps://arxiv.org/pdf/2404.11446\n\nSystem that uses LLMs to automate qualitative wargames - open-ended strategic simulations where participants respond with natural language rather than choosing from predefined moves. The system can simulate crisis scenarios like AI incidents or geopolitical conflicts, allowing multiple AI agents to play different roles while a control agent moderates and adjudicates outcomes.",
        "size": 17.714285714285715,
        "index": 38,
        "x": -1567.358358192519,
        "y": -1969.309216161687,
        "vy": -0.3032097371136398,
        "vx": -0.49256209457360906
      },
      "type": "topic_link",
      "index": 227
    },
    {
      "source": {
        "id": "Wargames & TTX",
        "title": "Wargames & TTX",
        "tags": [
          "topic"
        ],
        "content": "#topic",
        "size": 18.857142857142858,
        "index": 164,
        "x": -1892.2380713597158,
        "y": -1126.8699947554496,
        "vy": -0.12046666673136741,
        "vx": -0.36410177562718715
      },
      "target": {
        "id": "Escalation Risks from Language Models in Military and Diplomatic Decision-Making",
        "title": "Escalation Risks from Language Models in Military and Diplomatic Decision-Making",
        "tags": [
          "project"
        ],
        "content": "#project \nhttps://dl.acm.org/doi/pdf/10.1145/3630106.3658942\n\nPaper examines what happens when LLMs are placed in charge of simulated nations in wargaming scenarios, finding that these AI systems frequently escalate conflicts and, in some cases, even deploy nuclear weapons. The researchers tested five different LLMs controlling autonomous nation agents and discovered that all models showed escalatory patterns, with some developing arms-race dynamics and making decisions based on concerning justifications like deterrence and first-strike tactics.",
        "size": 20,
        "index": 168,
        "x": -2094.3333318231967,
        "y": -1459.651732839816,
        "vy": -0.13637117934200482,
        "vx": -0.4229488354702969
      },
      "type": "topic_link",
      "index": 228
    },
    {
      "source": {
        "id": "Wargames & TTX",
        "title": "Wargames & TTX",
        "tags": [
          "topic"
        ],
        "content": "#topic",
        "size": 18.857142857142858,
        "index": 164,
        "x": -1892.2380713597158,
        "y": -1126.8699947554496,
        "vy": -0.12046666673136741,
        "vx": -0.36410177562718715
      },
      "target": {
        "id": "Grim",
        "title": "Grim",
        "tags": [
          "project"
        ],
        "content": "#project \n\nhttps://github.com/SentinelTeam/grim\nhttps://www.lesswrong.com/posts/eR69f3hi5ozxchhYg/scaling-wargaming-for-global-catastrophic-risks-with-ai\n\nAI-powered wargaming tool that uses LLMs to simulate complex catastrophic scenarios and help organizations practice emergency responses. The tool functions as a Telegram bot where participants can take actions, request information, and feed data into crisis simulations, with AI serving as both forecaster and game master to create detailed, dynamic scenarios.",
        "size": 17.714285714285715,
        "index": 242,
        "x": -1135.8393199919878,
        "y": -1478.5909522257905,
        "vy": -0.2986686394199088,
        "vx": -0.6542902351116809
      },
      "type": "topic_link",
      "index": 229
    },
    {
      "source": {
        "id": "Escalation Risks from Language Models in Military and Diplomatic Decision-Making",
        "title": "Escalation Risks from Language Models in Military and Diplomatic Decision-Making",
        "tags": [
          "project"
        ],
        "content": "#project \nhttps://dl.acm.org/doi/pdf/10.1145/3630106.3658942\n\nPaper examines what happens when LLMs are placed in charge of simulated nations in wargaming scenarios, finding that these AI systems frequently escalate conflicts and, in some cases, even deploy nuclear weapons. The researchers tested five different LLMs controlling autonomous nation agents and discovered that all models showed escalatory patterns, with some developing arms-race dynamics and making decisions based on concerning justifications like deterrence and first-strike tactics.",
        "size": 20,
        "index": 168,
        "x": -2094.3333318231967,
        "y": -1459.651732839816,
        "vy": -0.13637117934200482,
        "vx": -0.4229488354702969
      },
      "target": {
        "id": "Juan-Pablo Rivera",
        "title": "Juan-Pablo Rivera",
        "tags": [
          "person"
        ],
        "content": "#person",
        "size": 16.571428571428573,
        "index": 329,
        "x": -2112.588722836765,
        "y": -1535.2811894329448,
        "vy": -0.13026476822171845,
        "vx": -0.37238157713469194
      },
      "index": 230
    },
    {
      "source": {
        "id": "Escalation Risks from Language Models in Military and Diplomatic Decision-Making",
        "title": "Escalation Risks from Language Models in Military and Diplomatic Decision-Making",
        "tags": [
          "project"
        ],
        "content": "#project \nhttps://dl.acm.org/doi/pdf/10.1145/3630106.3658942\n\nPaper examines what happens when LLMs are placed in charge of simulated nations in wargaming scenarios, finding that these AI systems frequently escalate conflicts and, in some cases, even deploy nuclear weapons. The researchers tested five different LLMs controlling autonomous nation agents and discovered that all models showed escalatory patterns, with some developing arms-race dynamics and making decisions based on concerning justifications like deterrence and first-strike tactics.",
        "size": 20,
        "index": 168,
        "x": -2094.3333318231967,
        "y": -1459.651732839816,
        "vy": -0.13637117934200482,
        "vx": -0.4229488354702969
      },
      "target": {
        "id": "Gabriel Mukobi",
        "title": "Gabriel Mukobi",
        "tags": [
          "person"
        ],
        "content": "#person",
        "size": 16.571428571428573,
        "index": 188,
        "x": -2055.7964012026973,
        "y": -1533.6501881483662,
        "vy": -0.12366581352140225,
        "vx": -0.3735978125506575
      },
      "index": 231
    },
    {
      "source": {
        "id": "Escalation Risks from Language Models in Military and Diplomatic Decision-Making",
        "title": "Escalation Risks from Language Models in Military and Diplomatic Decision-Making",
        "tags": [
          "project"
        ],
        "content": "#project \nhttps://dl.acm.org/doi/pdf/10.1145/3630106.3658942\n\nPaper examines what happens when LLMs are placed in charge of simulated nations in wargaming scenarios, finding that these AI systems frequently escalate conflicts and, in some cases, even deploy nuclear weapons. The researchers tested five different LLMs controlling autonomous nation agents and discovered that all models showed escalatory patterns, with some developing arms-race dynamics and making decisions based on concerning justifications like deterrence and first-strike tactics.",
        "size": 20,
        "index": 168,
        "x": -2094.3333318231967,
        "y": -1459.651732839816,
        "vy": -0.13637117934200482,
        "vx": -0.4229488354702969
      },
      "target": {
        "id": "Anka Reuel",
        "title": "Anka Reuel",
        "tags": [
          "person"
        ],
        "content": "#person",
        "size": 16.571428571428573,
        "index": 264,
        "x": -2019.6451071960157,
        "y": -1489.8746609698208,
        "vy": -0.11181441342247465,
        "vx": -0.38808396032332443
      },
      "index": 232
    },
    {
      "source": {
        "id": "Escalation Risks from Language Models in Military and Diplomatic Decision-Making",
        "title": "Escalation Risks from Language Models in Military and Diplomatic Decision-Making",
        "tags": [
          "project"
        ],
        "content": "#project \nhttps://dl.acm.org/doi/pdf/10.1145/3630106.3658942\n\nPaper examines what happens when LLMs are placed in charge of simulated nations in wargaming scenarios, finding that these AI systems frequently escalate conflicts and, in some cases, even deploy nuclear weapons. The researchers tested five different LLMs controlling autonomous nation agents and discovered that all models showed escalatory patterns, with some developing arms-race dynamics and making decisions based on concerning justifications like deterrence and first-strike tactics.",
        "size": 20,
        "index": 168,
        "x": -2094.3333318231967,
        "y": -1459.651732839816,
        "vy": -0.13637117934200482,
        "vx": -0.4229488354702969
      },
      "target": {
        "id": "Max Lamparth",
        "title": "Max Lamparth",
        "tags": [
          "person"
        ],
        "content": "#person \n\nhttps://www.maxlamparth.com/",
        "size": 17.142857142857142,
        "index": 136,
        "x": -2139.075864003679,
        "y": -1403.148621492655,
        "vy": -0.09840077474495441,
        "vx": -0.4217991876266044
      },
      "index": 233
    },
    {
      "source": {
        "id": "Escalation Risks from Language Models in Military and Diplomatic Decision-Making",
        "title": "Escalation Risks from Language Models in Military and Diplomatic Decision-Making",
        "tags": [
          "project"
        ],
        "content": "#project \nhttps://dl.acm.org/doi/pdf/10.1145/3630106.3658942\n\nPaper examines what happens when LLMs are placed in charge of simulated nations in wargaming scenarios, finding that these AI systems frequently escalate conflicts and, in some cases, even deploy nuclear weapons. The researchers tested five different LLMs controlling autonomous nation agents and discovered that all models showed escalatory patterns, with some developing arms-race dynamics and making decisions based on concerning justifications like deterrence and first-strike tactics.",
        "size": 20,
        "index": 168,
        "x": -2094.3333318231967,
        "y": -1459.651732839816,
        "vy": -0.13637117934200482,
        "vx": -0.4229488354702969
      },
      "target": {
        "id": "Chandler Smith",
        "title": "Chandler Smith",
        "tags": [
          "person"
        ],
        "content": "#person",
        "size": 16.571428571428573,
        "index": 420,
        "x": -2025.9187936303938,
        "y": -1431.7380100541798,
        "vy": -0.1513305583271114,
        "vx": -0.3959812481579295
      },
      "index": 234
    },
    {
      "source": {
        "id": "Escalation Risks from Language Models in Military and Diplomatic Decision-Making",
        "title": "Escalation Risks from Language Models in Military and Diplomatic Decision-Making",
        "tags": [
          "project"
        ],
        "content": "#project \nhttps://dl.acm.org/doi/pdf/10.1145/3630106.3658942\n\nPaper examines what happens when LLMs are placed in charge of simulated nations in wargaming scenarios, finding that these AI systems frequently escalate conflicts and, in some cases, even deploy nuclear weapons. The researchers tested five different LLMs controlling autonomous nation agents and discovered that all models showed escalatory patterns, with some developing arms-race dynamics and making decisions based on concerning justifications like deterrence and first-strike tactics.",
        "size": 20,
        "index": 168,
        "x": -2094.3333318231967,
        "y": -1459.651732839816,
        "vy": -0.13637117934200482,
        "vx": -0.4229488354702969
      },
      "target": {
        "id": "Jacquelyn Schneider",
        "title": "Jacquelyn Schneider",
        "tags": [
          "person"
        ],
        "content": "#person",
        "size": 17.714285714285715,
        "index": 248,
        "x": -2179.5691824765895,
        "y": -1455.2408031463438,
        "vy": -0.0785190756515778,
        "vx": -0.3687066288969788
      },
      "index": 235
    },
    {
      "source": {
        "id": "Robert Gambee",
        "title": "Robert Gambee",
        "tags": [
          "person"
        ],
        "content": "#person",
        "size": 17.142857142857142,
        "index": 171,
        "x": -746.1246973487597,
        "y": -677.5281127461095,
        "vy": -0.4225432452466162,
        "vx": -0.3776717305559615
      },
      "target": {
        "id": "Future Search",
        "title": "Future Search",
        "tags": [
          "organization"
        ],
        "content": "#organization \n\nhttps://futuresearch.ai/",
        "size": 22.285714285714285,
        "index": 94,
        "x": -717.8526553292105,
        "y": -719.7276361443852,
        "vy": -0.4404383940953807,
        "vx": -0.3824839533244458
      },
      "index": 236
    },
    {
      "source": {
        "id": "Valdemar Danry",
        "title": "Valdemar Danry",
        "tags": [
          "person"
        ],
        "content": "#person\n\nhttps://valdemardanry.com/",
        "size": 18.857142857142858,
        "index": 172,
        "x": 1102.4705595757653,
        "y": -2731.308366831243,
        "vy": -1.122899896351339,
        "vx": -0.5144777169524257
      },
      "target": {
        "id": "MIT Media Lab",
        "title": "MIT Media Lab",
        "tags": [
          "organization"
        ],
        "content": "#organization \n\nhttps://www.media.mit.edu/",
        "size": 17.714285714285715,
        "index": 269,
        "x": 1006.2843605632936,
        "y": -2684.9292905918865,
        "vy": -1.0825039215382666,
        "vx": -0.515555946443613
      },
      "index": 237
    },
    {
      "source": {
        "id": "Contra papers claiming superhuman AI forecasting",
        "title": "Contra papers claiming superhuman AI forecasting",
        "tags": [
          "project"
        ],
        "content": "#project \n\nhttps://www.lesswrong.com/posts/uGkRcHqatmPkvpGLq/contra-papers-claiming-superhuman-ai-forecasting\n\nCritique of recent academic papers that claim AI systems have achieved \"superhuman\" forecasting performance. The authors from FutureSearch systematically debunk these claims by identifying major methodological flaws, including inadequate information retrieval capabilities, data contamination, unfair timing advantages, and misleading statistical interpretations that make AI performance appear better than it actually is.",
        "size": 18.857142857142858,
        "index": 173,
        "x": -912.2504237409754,
        "y": -668.3597869352501,
        "vy": -0.3168233753959674,
        "vx": -0.337357800659079
      },
      "target": {
        "id": "Nikos Bosse",
        "title": "Nikos Bosse",
        "tags": [
          "person"
        ],
        "content": "#person \n\nhttps://followtheargument.org/",
        "size": 18.285714285714285,
        "index": 243,
        "x": -843.047489587698,
        "y": -605.5760820500159,
        "vy": -0.3391263569445478,
        "vx": -0.31376832368339486
      },
      "index": 238
    },
    {
      "source": {
        "id": "Contra papers claiming superhuman AI forecasting",
        "title": "Contra papers claiming superhuman AI forecasting",
        "tags": [
          "project"
        ],
        "content": "#project \n\nhttps://www.lesswrong.com/posts/uGkRcHqatmPkvpGLq/contra-papers-claiming-superhuman-ai-forecasting\n\nCritique of recent academic papers that claim AI systems have achieved \"superhuman\" forecasting performance. The authors from FutureSearch systematically debunk these claims by identifying major methodological flaws, including inadequate information retrieval capabilities, data contamination, unfair timing advantages, and misleading statistical interpretations that make AI performance appear better than it actually is.",
        "size": 18.857142857142858,
        "index": 173,
        "x": -912.2504237409754,
        "y": -668.3597869352501,
        "vy": -0.3168233753959674,
        "vx": -0.337357800659079
      },
      "target": {
        "id": "Peter Mühlbacher",
        "title": "Peter Mühlbacher",
        "tags": [
          "person"
        ],
        "content": "#person \n\nhttp://peter.muehlbacher.me/",
        "size": 18.285714285714285,
        "index": 238,
        "x": -857.2972832702691,
        "y": -654.0740118249536,
        "vy": -0.3400430834320815,
        "vx": -0.3453910216674885
      },
      "index": 239
    },
    {
      "source": {
        "id": "Contra papers claiming superhuman AI forecasting",
        "title": "Contra papers claiming superhuman AI forecasting",
        "tags": [
          "project"
        ],
        "content": "#project \n\nhttps://www.lesswrong.com/posts/uGkRcHqatmPkvpGLq/contra-papers-claiming-superhuman-ai-forecasting\n\nCritique of recent academic papers that claim AI systems have achieved \"superhuman\" forecasting performance. The authors from FutureSearch systematically debunk these claims by identifying major methodological flaws, including inadequate information retrieval capabilities, data contamination, unfair timing advantages, and misleading statistical interpretations that make AI performance appear better than it actually is.",
        "size": 18.857142857142858,
        "index": 173,
        "x": -912.2504237409754,
        "y": -668.3597869352501,
        "vy": -0.3168233753959674,
        "vx": -0.337357800659079
      },
      "target": {
        "id": "Lawrence Phillips",
        "title": "Lawrence Phillips",
        "tags": [
          "person"
        ],
        "content": "#person",
        "size": 18.285714285714285,
        "index": 239,
        "x": -786.997452942158,
        "y": -671.6640862022364,
        "vy": -0.3955253133044403,
        "vx": -0.3629004531354729
      },
      "index": 240
    },
    {
      "source": {
        "id": "Contra papers claiming superhuman AI forecasting",
        "title": "Contra papers claiming superhuman AI forecasting",
        "tags": [
          "project"
        ],
        "content": "#project \n\nhttps://www.lesswrong.com/posts/uGkRcHqatmPkvpGLq/contra-papers-claiming-superhuman-ai-forecasting\n\nCritique of recent academic papers that claim AI systems have achieved \"superhuman\" forecasting performance. The authors from FutureSearch systematically debunk these claims by identifying major methodological flaws, including inadequate information retrieval capabilities, data contamination, unfair timing advantages, and misleading statistical interpretations that make AI performance appear better than it actually is.",
        "size": 18.857142857142858,
        "index": 173,
        "x": -912.2504237409754,
        "y": -668.3597869352501,
        "vy": -0.3168233753959674,
        "vx": -0.337357800659079
      },
      "target": {
        "id": "Dan Schwarz",
        "title": "Dan Schwarz",
        "tags": [
          "person"
        ],
        "content": "#person \n\nSome writing about [Google's internal prediction markets](https://asteriskmag.com/issues/08/the-death-and-life-of-prediction-markets-at-google).",
        "size": 18.285714285714285,
        "index": 404,
        "x": -831.4781389544952,
        "y": -708.941876393585,
        "vy": -0.3657625739173525,
        "vx": -0.3779117138521553
      },
      "index": 241
    },
    {
      "source": {
        "id": "Forecasting Future World Events with Neural Networks",
        "title": "Forecasting Future World Events with Neural Networks",
        "tags": [
          "project"
        ],
        "content": "#project \n\nhttps://arxiv.org/pdf/2206.15474\n\nIntroduces \"Autocast,\" a dataset and benchmark for evaluating neural networks' ability to forecast future world events using thousands of real forecasting questions from public tournaments. The dataset tests language models on diverse topics like politics, economics, and science by simulating historical forecasting conditions—providing only past news articles to prevent information leakage from the future.\n\n(research from 2022)",
        "size": 22.285714285714285,
        "index": 175,
        "x": -844.9675993592969,
        "y": 476.331173079975,
        "vy": -0.410662757686534,
        "vx": 0.22624753369910494
      },
      "target": {
        "id": "Andy Zou",
        "title": "Andy Zou",
        "tags": [
          "person"
        ],
        "content": "#person",
        "size": 16.571428571428573,
        "index": 14,
        "x": -906.6659596965399,
        "y": 430.7431010076412,
        "vy": -0.31173817277984966,
        "vx": 0.2260344189835437
      },
      "index": 242
    },
    {
      "source": {
        "id": "Forecasting Future World Events with Neural Networks",
        "title": "Forecasting Future World Events with Neural Networks",
        "tags": [
          "project"
        ],
        "content": "#project \n\nhttps://arxiv.org/pdf/2206.15474\n\nIntroduces \"Autocast,\" a dataset and benchmark for evaluating neural networks' ability to forecast future world events using thousands of real forecasting questions from public tournaments. The dataset tests language models on diverse topics like politics, economics, and science by simulating historical forecasting conditions—providing only past news articles to prevent information leakage from the future.\n\n(research from 2022)",
        "size": 22.285714285714285,
        "index": 175,
        "x": -844.9675993592969,
        "y": 476.331173079975,
        "vy": -0.410662757686534,
        "vx": 0.22624753369910494
      },
      "target": {
        "id": "Tristan Xiao",
        "title": "Tristan Xiao",
        "tags": [
          "person"
        ],
        "content": "#person",
        "size": 16.571428571428573,
        "index": 87,
        "x": -814.0999428682537,
        "y": 533.6793262651005,
        "vy": -0.46219597847689337,
        "vx": 0.25971174299897487
      },
      "index": 243
    },
    {
      "source": {
        "id": "Forecasting Future World Events with Neural Networks",
        "title": "Forecasting Future World Events with Neural Networks",
        "tags": [
          "project"
        ],
        "content": "#project \n\nhttps://arxiv.org/pdf/2206.15474\n\nIntroduces \"Autocast,\" a dataset and benchmark for evaluating neural networks' ability to forecast future world events using thousands of real forecasting questions from public tournaments. The dataset tests language models on diverse topics like politics, economics, and science by simulating historical forecasting conditions—providing only past news articles to prevent information leakage from the future.\n\n(research from 2022)",
        "size": 22.285714285714285,
        "index": 175,
        "x": -844.9675993592969,
        "y": 476.331173079975,
        "vy": -0.410662757686534,
        "vx": 0.22624753369910494
      },
      "target": {
        "id": "Ryan Jia",
        "title": "Ryan Jia",
        "tags": [
          "person"
        ],
        "content": "#person",
        "size": 16.571428571428573,
        "index": 305,
        "x": -902.6698848942369,
        "y": 567.3643356058368,
        "vy": -0.29335559264154853,
        "vx": 0.35224461392399714
      },
      "index": 244
    },
    {
      "source": {
        "id": "Forecasting Future World Events with Neural Networks",
        "title": "Forecasting Future World Events with Neural Networks",
        "tags": [
          "project"
        ],
        "content": "#project \n\nhttps://arxiv.org/pdf/2206.15474\n\nIntroduces \"Autocast,\" a dataset and benchmark for evaluating neural networks' ability to forecast future world events using thousands of real forecasting questions from public tournaments. The dataset tests language models on diverse topics like politics, economics, and science by simulating historical forecasting conditions—providing only past news articles to prevent information leakage from the future.\n\n(research from 2022)",
        "size": 22.285714285714285,
        "index": 175,
        "x": -844.9675993592969,
        "y": 476.331173079975,
        "vy": -0.410662757686534,
        "vx": 0.22624753369910494
      },
      "target": {
        "id": "Joe Kwon",
        "title": "Joe Kwon",
        "tags": [
          "person"
        ],
        "content": "#person",
        "size": 16.571428571428573,
        "index": 123,
        "x": -893.9290832199296,
        "y": 509.18197162098244,
        "vy": -0.2961225809635062,
        "vx": 0.3052093155728851
      },
      "index": 245
    },
    {
      "source": {
        "id": "Forecasting Future World Events with Neural Networks",
        "title": "Forecasting Future World Events with Neural Networks",
        "tags": [
          "project"
        ],
        "content": "#project \n\nhttps://arxiv.org/pdf/2206.15474\n\nIntroduces \"Autocast,\" a dataset and benchmark for evaluating neural networks' ability to forecast future world events using thousands of real forecasting questions from public tournaments. The dataset tests language models on diverse topics like politics, economics, and science by simulating historical forecasting conditions—providing only past news articles to prevent information leakage from the future.\n\n(research from 2022)",
        "size": 22.285714285714285,
        "index": 175,
        "x": -844.9675993592969,
        "y": 476.331173079975,
        "vy": -0.410662757686534,
        "vx": 0.22624753369910494
      },
      "target": {
        "id": "Mantas Mazeika",
        "title": "Mantas Mazeika",
        "tags": [
          "person"
        ],
        "content": "#person",
        "size": 16.571428571428573,
        "index": 153,
        "x": -947.6833679189886,
        "y": 532.1636341909024,
        "vy": -0.25653325685997325,
        "vx": 0.32341415740295776
      },
      "index": 246
    },
    {
      "source": {
        "id": "Forecasting Future World Events with Neural Networks",
        "title": "Forecasting Future World Events with Neural Networks",
        "tags": [
          "project"
        ],
        "content": "#project \n\nhttps://arxiv.org/pdf/2206.15474\n\nIntroduces \"Autocast,\" a dataset and benchmark for evaluating neural networks' ability to forecast future world events using thousands of real forecasting questions from public tournaments. The dataset tests language models on diverse topics like politics, economics, and science by simulating historical forecasting conditions—providing only past news articles to prevent information leakage from the future.\n\n(research from 2022)",
        "size": 22.285714285714285,
        "index": 175,
        "x": -844.9675993592969,
        "y": 476.331173079975,
        "vy": -0.410662757686534,
        "vx": 0.22624753369910494
      },
      "target": {
        "id": "Richard Li",
        "title": "Richard Li",
        "tags": [
          "person"
        ],
        "content": "#person",
        "size": 16.571428571428573,
        "index": 97,
        "x": -940.7466985429488,
        "y": 474.9853314430529,
        "vy": -0.2619478111798859,
        "vx": 0.2846382621904681
      },
      "index": 247
    },
    {
      "source": {
        "id": "Forecasting Future World Events with Neural Networks",
        "title": "Forecasting Future World Events with Neural Networks",
        "tags": [
          "project"
        ],
        "content": "#project \n\nhttps://arxiv.org/pdf/2206.15474\n\nIntroduces \"Autocast,\" a dataset and benchmark for evaluating neural networks' ability to forecast future world events using thousands of real forecasting questions from public tournaments. The dataset tests language models on diverse topics like politics, economics, and science by simulating historical forecasting conditions—providing only past news articles to prevent information leakage from the future.\n\n(research from 2022)",
        "size": 22.285714285714285,
        "index": 175,
        "x": -844.9675993592969,
        "y": 476.331173079975,
        "vy": -0.410662757686534,
        "vx": 0.22624753369910494
      },
      "target": {
        "id": "Dawn Song",
        "title": "Dawn Song",
        "tags": [
          "person"
        ],
        "content": "#person \n\nhttps://dawnsong.io/",
        "size": 17.142857142857142,
        "index": 280,
        "x": 59.22529616244745,
        "y": 606.6469952544618,
        "vy": -0.8750545448179495,
        "vx": 0.15846918601654594
      },
      "index": 248
    },
    {
      "source": {
        "id": "Forecasting Future World Events with Neural Networks",
        "title": "Forecasting Future World Events with Neural Networks",
        "tags": [
          "project"
        ],
        "content": "#project \n\nhttps://arxiv.org/pdf/2206.15474\n\nIntroduces \"Autocast,\" a dataset and benchmark for evaluating neural networks' ability to forecast future world events using thousands of real forecasting questions from public tournaments. The dataset tests language models on diverse topics like politics, economics, and science by simulating historical forecasting conditions—providing only past news articles to prevent information leakage from the future.\n\n(research from 2022)",
        "size": 22.285714285714285,
        "index": 175,
        "x": -844.9675993592969,
        "y": 476.331173079975,
        "vy": -0.410662757686534,
        "vx": 0.22624753369910494
      },
      "target": {
        "id": "Jacob Steinhardt",
        "title": "Jacob Steinhardt",
        "tags": [
          "person"
        ],
        "content": "#person \n\nhttps://jsteinhardt.stat.berkeley.edu/",
        "size": 17.142857142857142,
        "index": 89,
        "x": -689.2056804129033,
        "y": 371.8596738179657,
        "vy": -0.4506994170173919,
        "vx": 0.20059927482224163
      },
      "index": 249
    },
    {
      "source": {
        "id": "Forecasting Future World Events with Neural Networks",
        "title": "Forecasting Future World Events with Neural Networks",
        "tags": [
          "project"
        ],
        "content": "#project \n\nhttps://arxiv.org/pdf/2206.15474\n\nIntroduces \"Autocast,\" a dataset and benchmark for evaluating neural networks' ability to forecast future world events using thousands of real forecasting questions from public tournaments. The dataset tests language models on diverse topics like politics, economics, and science by simulating historical forecasting conditions—providing only past news articles to prevent information leakage from the future.\n\n(research from 2022)",
        "size": 22.285714285714285,
        "index": 175,
        "x": -844.9675993592969,
        "y": 476.331173079975,
        "vy": -0.410662757686534,
        "vx": 0.22624753369910494
      },
      "target": {
        "id": "Owain Evans",
        "title": "Owain Evans",
        "tags": [
          "person"
        ],
        "content": "#person \n\nhttps://owainevans.github.io/",
        "size": 17.714285714285715,
        "index": 39,
        "x": -1372.1944531749268,
        "y": 477.9869800887734,
        "vy": -0.29993178439518847,
        "vx": 0.18951433178696656
      },
      "index": 250
    },
    {
      "source": {
        "id": "Forecasting Future World Events with Neural Networks",
        "title": "Forecasting Future World Events with Neural Networks",
        "tags": [
          "project"
        ],
        "content": "#project \n\nhttps://arxiv.org/pdf/2206.15474\n\nIntroduces \"Autocast,\" a dataset and benchmark for evaluating neural networks' ability to forecast future world events using thousands of real forecasting questions from public tournaments. The dataset tests language models on diverse topics like politics, economics, and science by simulating historical forecasting conditions—providing only past news articles to prevent information leakage from the future.\n\n(research from 2022)",
        "size": 22.285714285714285,
        "index": 175,
        "x": -844.9675993592969,
        "y": 476.331173079975,
        "vy": -0.410662757686534,
        "vx": 0.22624753369910494
      },
      "target": {
        "id": "Dan Hendrycks",
        "title": "Dan Hendrycks",
        "tags": [
          "person"
        ],
        "content": "#person \n\nhttps://danhendrycks.com/",
        "size": 16.571428571428573,
        "index": 103,
        "x": -848.2753514987046,
        "y": 577.917902273538,
        "vy": -0.3658801788153614,
        "vx": 0.373680725832438
      },
      "index": 251
    },
    {
      "source": {
        "id": "ForecastBench - A Dynamic Benchmark of AI Forecasting Capabilities",
        "title": "ForecastBench - A Dynamic Benchmark of AI Forecasting Capabilities",
        "tags": [
          "project"
        ],
        "content": "#project \n\nhttps://arxiv.org/pdf/2409.19839\nhttps://www.forecastbench.org/\n\nBenchmark system that continuously evaluates AI forecasting capabilities using 1,000 automatically generated questions about future events that update in real-time. The system avoids data contamination by using only questions about genuinely unresolved future events, and it compares LLM performance against human forecasters including \"superforecasters\" who have proven track records. Research found that expert human forecasters significantly outperform the best-performing LLMs (like Claude 3.5 Sonnet).",
        "size": 20.57142857142857,
        "index": 176,
        "x": -256.173907996596,
        "y": 55.060340516367795,
        "vy": -0.6658430423286233,
        "vx": 0.1258733673270566
      },
      "target": {
        "id": "Ezra Karger",
        "title": "Ezra Karger",
        "tags": [
          "person"
        ],
        "content": "#person \n\nhttps://ezrakarger.com/",
        "size": 18.285714285714285,
        "index": 150,
        "x": 13.294680683389643,
        "y": 76.75026712023329,
        "vy": -0.8178432271089577,
        "vx": 0.14711852217550486
      },
      "index": 252
    },
    {
      "source": {
        "id": "ForecastBench - A Dynamic Benchmark of AI Forecasting Capabilities",
        "title": "ForecastBench - A Dynamic Benchmark of AI Forecasting Capabilities",
        "tags": [
          "project"
        ],
        "content": "#project \n\nhttps://arxiv.org/pdf/2409.19839\nhttps://www.forecastbench.org/\n\nBenchmark system that continuously evaluates AI forecasting capabilities using 1,000 automatically generated questions about future events that update in real-time. The system avoids data contamination by using only questions about genuinely unresolved future events, and it compares LLM performance against human forecasters including \"superforecasters\" who have proven track records. Research found that expert human forecasters significantly outperform the best-performing LLMs (like Claude 3.5 Sonnet).",
        "size": 20.57142857142857,
        "index": 176,
        "x": -256.173907996596,
        "y": 55.060340516367795,
        "vy": -0.6658430423286233,
        "vx": 0.1258733673270566
      },
      "target": {
        "id": "Houtan Bastani",
        "title": "Houtan Bastani",
        "tags": [
          "person"
        ],
        "content": "#person",
        "size": 17.142857142857142,
        "index": 411,
        "x": -167.68177451662453,
        "y": 10.18398720815008,
        "vy": -0.7666032941425005,
        "vx": 0.2770692053409178
      },
      "index": 253
    },
    {
      "source": {
        "id": "ForecastBench - A Dynamic Benchmark of AI Forecasting Capabilities",
        "title": "ForecastBench - A Dynamic Benchmark of AI Forecasting Capabilities",
        "tags": [
          "project"
        ],
        "content": "#project \n\nhttps://arxiv.org/pdf/2409.19839\nhttps://www.forecastbench.org/\n\nBenchmark system that continuously evaluates AI forecasting capabilities using 1,000 automatically generated questions about future events that update in real-time. The system avoids data contamination by using only questions about genuinely unresolved future events, and it compares LLM performance against human forecasters including \"superforecasters\" who have proven track records. Research found that expert human forecasters significantly outperform the best-performing LLMs (like Claude 3.5 Sonnet).",
        "size": 20.57142857142857,
        "index": 176,
        "x": -256.173907996596,
        "y": 55.060340516367795,
        "vy": -0.6658430423286233,
        "vx": 0.1258733673270566
      },
      "target": {
        "id": "Chen Yueh-Han",
        "title": "Chen Yueh-Han",
        "tags": [
          "person"
        ],
        "content": "#person \n\nhttps://john-chen.cc/",
        "size": 17.142857142857142,
        "index": 433,
        "x": -339.6355168567282,
        "y": 134.8839087724612,
        "vy": -0.5406065567864574,
        "vx": 0.1543294172612554
      },
      "index": 254
    },
    {
      "source": {
        "id": "ForecastBench - A Dynamic Benchmark of AI Forecasting Capabilities",
        "title": "ForecastBench - A Dynamic Benchmark of AI Forecasting Capabilities",
        "tags": [
          "project"
        ],
        "content": "#project \n\nhttps://arxiv.org/pdf/2409.19839\nhttps://www.forecastbench.org/\n\nBenchmark system that continuously evaluates AI forecasting capabilities using 1,000 automatically generated questions about future events that update in real-time. The system avoids data contamination by using only questions about genuinely unresolved future events, and it compares LLM performance against human forecasters including \"superforecasters\" who have proven track records. Research found that expert human forecasters significantly outperform the best-performing LLMs (like Claude 3.5 Sonnet).",
        "size": 20.57142857142857,
        "index": 176,
        "x": -256.173907996596,
        "y": 55.060340516367795,
        "vy": -0.6658430423286233,
        "vx": 0.1258733673270566
      },
      "target": {
        "id": "Zachary Jacobs",
        "title": "Zachary Jacobs",
        "tags": [
          "person"
        ],
        "content": "#person",
        "size": 17.142857142857142,
        "index": 226,
        "x": -218.36615785961288,
        "y": -18.596035026356134,
        "vy": -0.6399477037238545,
        "vx": 0.20487463098357075
      },
      "index": 255
    },
    {
      "source": {
        "id": "ForecastBench - A Dynamic Benchmark of AI Forecasting Capabilities",
        "title": "ForecastBench - A Dynamic Benchmark of AI Forecasting Capabilities",
        "tags": [
          "project"
        ],
        "content": "#project \n\nhttps://arxiv.org/pdf/2409.19839\nhttps://www.forecastbench.org/\n\nBenchmark system that continuously evaluates AI forecasting capabilities using 1,000 automatically generated questions about future events that update in real-time. The system avoids data contamination by using only questions about genuinely unresolved future events, and it compares LLM performance against human forecasters including \"superforecasters\" who have proven track records. Research found that expert human forecasters significantly outperform the best-performing LLMs (like Claude 3.5 Sonnet).",
        "size": 20.57142857142857,
        "index": 176,
        "x": -256.173907996596,
        "y": 55.060340516367795,
        "vy": -0.6658430423286233,
        "vx": 0.1258733673270566
      },
      "target": {
        "id": "Danny Halawi",
        "title": "Danny Halawi",
        "tags": [
          "person"
        ],
        "content": "#person",
        "size": 17.142857142857142,
        "index": 167,
        "x": -399.49091861133286,
        "y": 117.53463577365183,
        "vy": -0.6214489335318216,
        "vx": 0.17216289674951193
      },
      "index": 256
    },
    {
      "source": {
        "id": "ForecastBench - A Dynamic Benchmark of AI Forecasting Capabilities",
        "title": "ForecastBench - A Dynamic Benchmark of AI Forecasting Capabilities",
        "tags": [
          "project"
        ],
        "content": "#project \n\nhttps://arxiv.org/pdf/2409.19839\nhttps://www.forecastbench.org/\n\nBenchmark system that continuously evaluates AI forecasting capabilities using 1,000 automatically generated questions about future events that update in real-time. The system avoids data contamination by using only questions about genuinely unresolved future events, and it compares LLM performance against human forecasters including \"superforecasters\" who have proven track records. Research found that expert human forecasters significantly outperform the best-performing LLMs (like Claude 3.5 Sonnet).",
        "size": 20.57142857142857,
        "index": 176,
        "x": -256.173907996596,
        "y": 55.060340516367795,
        "vy": -0.6658430423286233,
        "vx": 0.1258733673270566
      },
      "target": {
        "id": "Fred Zhang",
        "title": "Fred Zhang",
        "tags": [
          "person"
        ],
        "content": "#person \n\nhttps://fredzhang.me/",
        "size": 17.142857142857142,
        "index": 302,
        "x": -379.93425636737896,
        "y": 176.99362172420902,
        "vy": -0.5999020415624712,
        "vx": 0.09227835755969993
      },
      "index": 257
    },
    {
      "source": {
        "id": "ForecastBench - A Dynamic Benchmark of AI Forecasting Capabilities",
        "title": "ForecastBench - A Dynamic Benchmark of AI Forecasting Capabilities",
        "tags": [
          "project"
        ],
        "content": "#project \n\nhttps://arxiv.org/pdf/2409.19839\nhttps://www.forecastbench.org/\n\nBenchmark system that continuously evaluates AI forecasting capabilities using 1,000 automatically generated questions about future events that update in real-time. The system avoids data contamination by using only questions about genuinely unresolved future events, and it compares LLM performance against human forecasters including \"superforecasters\" who have proven track records. Research found that expert human forecasters significantly outperform the best-performing LLMs (like Claude 3.5 Sonnet).",
        "size": 20.57142857142857,
        "index": 176,
        "x": -256.173907996596,
        "y": 55.060340516367795,
        "vy": -0.6658430423286233,
        "vx": 0.1258733673270566
      },
      "target": {
        "id": "Philip Tetlock",
        "title": "Philip Tetlock",
        "tags": [
          "person"
        ],
        "content": "#person",
        "size": 18.857142857142858,
        "index": 326,
        "x": -269.190694144188,
        "y": -121.36964146524063,
        "vy": -0.698755466059142,
        "vx": 0.0871105431534283
      },
      "index": 258
    },
    {
      "source": {
        "id": "Sofi Vanhanen",
        "title": "Sofi Vanhanen",
        "tags": [
          "person"
        ],
        "content": "#person \n\nhttps://sofiavanhanen.fi/",
        "size": 18.285714285714285,
        "index": 177,
        "x": 3629.495175318134,
        "y": -1087.890476428783,
        "vy": -0.9980357709893994,
        "vx": 0.19329731806245978
      },
      "target": {
        "id": "Mosaic Labs",
        "title": "Mosaic Labs",
        "tags": [
          "organization"
        ],
        "content": "#organization \n\nhttps://mosaic-labs.org/",
        "size": 17.142857142857142,
        "index": 179,
        "x": 3659.5707234295,
        "y": -1036.9469431723346,
        "vy": -0.9786142309862262,
        "vx": 0.1606327073407951
      },
      "index": 259
    },
    {
      "source": {
        "id": "AI Forecasting Benchmark",
        "title": "AI Forecasting Benchmark",
        "tags": [
          "project"
        ],
        "content": "#project \n\nhttps://www.metaculus.com/aib/\n\nMetaculus AIB (AI Benchmarking) is a series that benchmarks \"the state of the art in AI forecasting against the best humans on real-world questions.\"",
        "size": 20.57142857142857,
        "index": 178,
        "x": -491.69904780690194,
        "y": -761.3030318136879,
        "vy": -0.5357053744666,
        "vx": -0.2989940012828504
      },
      "target": {
        "id": "Metaculus",
        "title": "Metaculus",
        "tags": [
          "organization"
        ],
        "content": "#organization\n\nhttps://www.metaculus.com/",
        "size": 20,
        "index": 126,
        "x": -291.65109336863395,
        "y": -589.3419401920876,
        "vy": -0.7257043832009319,
        "vx": -0.15741051377760798
      },
      "index": 260
    },
    {
      "source": {
        "id": "AI doing philosophy = AI generating hands?",
        "title": "AI doing philosophy = AI generating hands?",
        "tags": [
          "project"
        ],
        "content": "#project \n\nhttps://www.lesswrong.com/posts/G4ARgcnFogpqorQgb/ai-doing-philosophy-ai-generating-hands-1\n\nWei Dai argues that just as current AI systems generate beautiful images but distorted hands, future AI might excel at science and technology while being dangerously incompetent at philosophical reasoning, creating a critical capability gap that requires urgent attention.",
        "size": 17.142857142857142,
        "index": 181,
        "x": 590.8140967392611,
        "y": 2738.198721233041,
        "vy": -0.997969840208746,
        "vx": 1.9443985164650819
      },
      "target": {
        "id": "Wei Dai",
        "title": "Wei Dai",
        "tags": [
          "person"
        ],
        "content": "#person \n\nhttps://wdai.us/",
        "size": 16.571428571428573,
        "index": 288,
        "x": 592.1653710341611,
        "y": 2821.081196902191,
        "vy": -1.0037176916620307,
        "vx": 2.017220655613453
      },
      "index": 261
    },
    {
      "source": {
        "id": "Why Chatbots Are Not the Future",
        "title": "Why Chatbots Are Not the Future",
        "tags": [
          "project"
        ],
        "content": "#project \n\nhttps://wattenberger.com/thoughts/boo-chatbots\n\nArticle that argues chatbots are poor interfaces for AI systems because they lack clear affordances, force users to learn complex prompting, and isolate responses without allowing iterative refinement. The author advocates for AI tools with better user interfaces that provide contextual controls, visual feedback, and support human agency rather than replacing human decision-making.",
        "size": 16.571428571428573,
        "index": 183,
        "x": 2764.192839543276,
        "y": -2018.307292734567,
        "vy": -1.8434410431056274,
        "vx": -0.3990578196192088
      },
      "target": {
        "id": "Amelia Wattenberger",
        "title": "Amelia Wattenberger",
        "tags": [
          "person"
        ],
        "content": "#person \n\nhttps://wattenberger.com\nhttps://github.com/Wattenberger",
        "size": 17.142857142857142,
        "index": 159,
        "x": 2822.658368554034,
        "y": -1995.5627131174708,
        "vy": -1.8916671969526198,
        "vx": -0.3849671573117643
      },
      "index": 262
    },
    {
      "source": {
        "id": "Andrea Brennen",
        "title": "Andrea Brennen",
        "tags": [
          "person"
        ],
        "content": "#person",
        "size": 17.142857142857142,
        "index": 184,
        "x": -1557.6993347002503,
        "y": -2058.063640049635,
        "vy": -0.3084296035603935,
        "vx": -0.5082222765103739
      },
      "target": {
        "id": "IQT Labs",
        "title": "IQT Labs",
        "tags": [
          "organization"
        ],
        "content": "#organization \n\nhttps://www.iqt.org/",
        "size": 17.142857142857142,
        "index": 266,
        "x": -1501.088614337867,
        "y": -2102.1091151589467,
        "vy": -0.31956553476439103,
        "vx": -0.515751933490656
      },
      "index": 263
    },
    {
      "source": {
        "id": "Habermas Machine",
        "title": "Habermas Machine",
        "tags": [
          "project"
        ],
        "content": "#project\n\nhttps://www.science.org/doi/10.1126/science.adq2852\n\nAI system that uses two fine-tuned language models to mediate group discussions and help people with differing opinions find common ground on contentious issues. The system takes individual written opinions from participants, generates candidate \"group statements\" that synthesize everyone's perspectives, and iteratively refines these statements based on participant feedback until the group converges on a shared position. Research showed that AI-mediated discussions were more effective than unmediated opinion exposure at causing people to change their minds and reach consensus, with the AI slightly outperforming untrained human mediators.",
        "size": 22.857142857142858,
        "index": 185,
        "x": 1358.4189684092353,
        "y": 558.0232276908089,
        "vy": -1.3801763496591373,
        "vx": 0.3119158781573072
      },
      "target": {
        "id": "Michael Henry Tessler",
        "title": "Michael Henry Tessler",
        "tags": [
          "person"
        ],
        "content": "#person\n\nhttps://www.mit.edu/~tessler/",
        "size": 18.285714285714285,
        "index": 426,
        "x": 1312.71187781098,
        "y": 660.8616297480344,
        "vy": -1.3375108366075392,
        "vx": 0.285090803595964
      },
      "index": 264
    },
    {
      "source": {
        "id": "Habermas Machine",
        "title": "Habermas Machine",
        "tags": [
          "project"
        ],
        "content": "#project\n\nhttps://www.science.org/doi/10.1126/science.adq2852\n\nAI system that uses two fine-tuned language models to mediate group discussions and help people with differing opinions find common ground on contentious issues. The system takes individual written opinions from participants, generates candidate \"group statements\" that synthesize everyone's perspectives, and iteratively refines these statements based on participant feedback until the group converges on a shared position. Research showed that AI-mediated discussions were more effective than unmediated opinion exposure at causing people to change their minds and reach consensus, with the AI slightly outperforming untrained human mediators.",
        "size": 22.857142857142858,
        "index": 185,
        "x": 1358.4189684092353,
        "y": 558.0232276908089,
        "vy": -1.3801763496591373,
        "vx": 0.3119158781573072
      },
      "target": {
        "id": "Michiel Bakker",
        "title": "Michiel Bakker",
        "tags": [
          "person"
        ],
        "content": "#person \n\nhttps://miba.dev/",
        "size": 21.142857142857142,
        "index": 284,
        "x": 1225.501866244462,
        "y": 730.5079873536822,
        "vy": -1.2714527484014098,
        "vx": 0.27625785273055353
      },
      "index": 265
    },
    {
      "source": {
        "id": "Habermas Machine",
        "title": "Habermas Machine",
        "tags": [
          "project"
        ],
        "content": "#project\n\nhttps://www.science.org/doi/10.1126/science.adq2852\n\nAI system that uses two fine-tuned language models to mediate group discussions and help people with differing opinions find common ground on contentious issues. The system takes individual written opinions from participants, generates candidate \"group statements\" that synthesize everyone's perspectives, and iteratively refines these statements based on participant feedback until the group converges on a shared position. Research showed that AI-mediated discussions were more effective than unmediated opinion exposure at causing people to change their minds and reach consensus, with the AI slightly outperforming untrained human mediators.",
        "size": 22.857142857142858,
        "index": 185,
        "x": 1358.4189684092353,
        "y": 558.0232276908089,
        "vy": -1.3801763496591373,
        "vx": 0.3119158781573072
      },
      "target": {
        "id": "Daniel Jarrett",
        "title": "Daniel Jarrett",
        "tags": [
          "person"
        ],
        "content": "#person",
        "size": 17.142857142857142,
        "index": 58,
        "x": 1210.2578542148913,
        "y": 589.8451188793808,
        "vy": -1.3811626256136391,
        "vx": 0.1734083518435134
      },
      "index": 266
    },
    {
      "source": {
        "id": "Habermas Machine",
        "title": "Habermas Machine",
        "tags": [
          "project"
        ],
        "content": "#project\n\nhttps://www.science.org/doi/10.1126/science.adq2852\n\nAI system that uses two fine-tuned language models to mediate group discussions and help people with differing opinions find common ground on contentious issues. The system takes individual written opinions from participants, generates candidate \"group statements\" that synthesize everyone's perspectives, and iteratively refines these statements based on participant feedback until the group converges on a shared position. Research showed that AI-mediated discussions were more effective than unmediated opinion exposure at causing people to change their minds and reach consensus, with the AI slightly outperforming untrained human mediators.",
        "size": 22.857142857142858,
        "index": 185,
        "x": 1358.4189684092353,
        "y": 558.0232276908089,
        "vy": -1.3801763496591373,
        "vx": 0.3119158781573072
      },
      "target": {
        "id": "Hannah Sheahan",
        "title": "Hannah Sheahan",
        "tags": [
          "person"
        ],
        "content": "#person \n\nhttps://hannahsheahan.github.io/",
        "size": 17.142857142857142,
        "index": 32,
        "x": 1355.6345270716827,
        "y": 495.9672877817744,
        "vy": -1.3810280862156508,
        "vx": 0.11788964987148415
      },
      "index": 267
    },
    {
      "source": {
        "id": "Habermas Machine",
        "title": "Habermas Machine",
        "tags": [
          "project"
        ],
        "content": "#project\n\nhttps://www.science.org/doi/10.1126/science.adq2852\n\nAI system that uses two fine-tuned language models to mediate group discussions and help people with differing opinions find common ground on contentious issues. The system takes individual written opinions from participants, generates candidate \"group statements\" that synthesize everyone's perspectives, and iteratively refines these statements based on participant feedback until the group converges on a shared position. Research showed that AI-mediated discussions were more effective than unmediated opinion exposure at causing people to change their minds and reach consensus, with the AI slightly outperforming untrained human mediators.",
        "size": 22.857142857142858,
        "index": 185,
        "x": 1358.4189684092353,
        "y": 558.0232276908089,
        "vy": -1.3801763496591373,
        "vx": 0.3119158781573072
      },
      "target": {
        "id": "Martin Chadwick",
        "title": "Martin Chadwick",
        "tags": [
          "person"
        ],
        "content": "#person",
        "size": 17.142857142857142,
        "index": 300,
        "x": 1305.5587724179861,
        "y": 528.6905398060712,
        "vy": -1.2330608401345255,
        "vx": 0.24983720752209582
      },
      "index": 268
    },
    {
      "source": {
        "id": "Habermas Machine",
        "title": "Habermas Machine",
        "tags": [
          "project"
        ],
        "content": "#project\n\nhttps://www.science.org/doi/10.1126/science.adq2852\n\nAI system that uses two fine-tuned language models to mediate group discussions and help people with differing opinions find common ground on contentious issues. The system takes individual written opinions from participants, generates candidate \"group statements\" that synthesize everyone's perspectives, and iteratively refines these statements based on participant feedback until the group converges on a shared position. Research showed that AI-mediated discussions were more effective than unmediated opinion exposure at causing people to change their minds and reach consensus, with the AI slightly outperforming untrained human mediators.",
        "size": 22.857142857142858,
        "index": 185,
        "x": 1358.4189684092353,
        "y": 558.0232276908089,
        "vy": -1.3801763496591373,
        "vx": 0.3119158781573072
      },
      "target": {
        "id": "Raphael Köster",
        "title": "Raphael Köster",
        "tags": [
          "person"
        ],
        "content": "#person",
        "size": 17.142857142857142,
        "index": 246,
        "x": 1250.5806382096266,
        "y": 546.7121239912451,
        "vy": -1.3626070407997648,
        "vx": 0.20076492715187425
      },
      "index": 269
    },
    {
      "source": {
        "id": "Habermas Machine",
        "title": "Habermas Machine",
        "tags": [
          "project"
        ],
        "content": "#project\n\nhttps://www.science.org/doi/10.1126/science.adq2852\n\nAI system that uses two fine-tuned language models to mediate group discussions and help people with differing opinions find common ground on contentious issues. The system takes individual written opinions from participants, generates candidate \"group statements\" that synthesize everyone's perspectives, and iteratively refines these statements based on participant feedback until the group converges on a shared position. Research showed that AI-mediated discussions were more effective than unmediated opinion exposure at causing people to change their minds and reach consensus, with the AI slightly outperforming untrained human mediators.",
        "size": 22.857142857142858,
        "index": 185,
        "x": 1358.4189684092353,
        "y": 558.0232276908089,
        "vy": -1.3801763496591373,
        "vx": 0.3119158781573072
      },
      "target": {
        "id": "Georgina Evans",
        "title": "Georgina Evans",
        "tags": [
          "person"
        ],
        "content": "#person \n\nhttps://sites.google.com/view/georginaevans",
        "size": 16.571428571428573,
        "index": 353,
        "x": 1434.751021566787,
        "y": 500.4740088642572,
        "vy": -1.0612477225634278,
        "vx": 0.3146352145806813
      },
      "index": 270
    },
    {
      "source": {
        "id": "Habermas Machine",
        "title": "Habermas Machine",
        "tags": [
          "project"
        ],
        "content": "#project\n\nhttps://www.science.org/doi/10.1126/science.adq2852\n\nAI system that uses two fine-tuned language models to mediate group discussions and help people with differing opinions find common ground on contentious issues. The system takes individual written opinions from participants, generates candidate \"group statements\" that synthesize everyone's perspectives, and iteratively refines these statements based on participant feedback until the group converges on a shared position. Research showed that AI-mediated discussions were more effective than unmediated opinion exposure at causing people to change their minds and reach consensus, with the AI slightly outperforming untrained human mediators.",
        "size": 22.857142857142858,
        "index": 185,
        "x": 1358.4189684092353,
        "y": 558.0232276908089,
        "vy": -1.3801763496591373,
        "vx": 0.3119158781573072
      },
      "target": {
        "id": "Lucy Campbell-Gillingham",
        "title": "Lucy Campbell-Gillingham",
        "tags": [
          "person"
        ],
        "content": "#person",
        "size": 17.714285714285715,
        "index": 407,
        "x": 1356.6725194125038,
        "y": 620.0271145323594,
        "vy": -1.3493309176801664,
        "vx": 0.2741836448885554
      },
      "index": 271
    },
    {
      "source": {
        "id": "Habermas Machine",
        "title": "Habermas Machine",
        "tags": [
          "project"
        ],
        "content": "#project\n\nhttps://www.science.org/doi/10.1126/science.adq2852\n\nAI system that uses two fine-tuned language models to mediate group discussions and help people with differing opinions find common ground on contentious issues. The system takes individual written opinions from participants, generates candidate \"group statements\" that synthesize everyone's perspectives, and iteratively refines these statements based on participant feedback until the group converges on a shared position. Research showed that AI-mediated discussions were more effective than unmediated opinion exposure at causing people to change their minds and reach consensus, with the AI slightly outperforming untrained human mediators.",
        "size": 22.857142857142858,
        "index": 185,
        "x": 1358.4189684092353,
        "y": 558.0232276908089,
        "vy": -1.3801763496591373,
        "vx": 0.3119158781573072
      },
      "target": {
        "id": "Tantum Collins",
        "title": "Tantum Collins",
        "tags": [
          "person"
        ],
        "content": "#person",
        "size": 16.571428571428573,
        "index": 382,
        "x": 1398.8418936929143,
        "y": 456.0242909837236,
        "vy": -1.2329950172858586,
        "vx": 0.5266007926516278
      },
      "index": 272
    },
    {
      "source": {
        "id": "Habermas Machine",
        "title": "Habermas Machine",
        "tags": [
          "project"
        ],
        "content": "#project\n\nhttps://www.science.org/doi/10.1126/science.adq2852\n\nAI system that uses two fine-tuned language models to mediate group discussions and help people with differing opinions find common ground on contentious issues. The system takes individual written opinions from participants, generates candidate \"group statements\" that synthesize everyone's perspectives, and iteratively refines these statements based on participant feedback until the group converges on a shared position. Research showed that AI-mediated discussions were more effective than unmediated opinion exposure at causing people to change their minds and reach consensus, with the AI slightly outperforming untrained human mediators.",
        "size": 22.857142857142858,
        "index": 185,
        "x": 1358.4189684092353,
        "y": 558.0232276908089,
        "vy": -1.3801763496591373,
        "vx": 0.3119158781573072
      },
      "target": {
        "id": "David Parkes",
        "title": "David Parkes",
        "tags": [
          "person"
        ],
        "content": "#person",
        "size": 18.285714285714285,
        "index": 133,
        "x": 1720.2221261795037,
        "y": 282.75903684678264,
        "vy": -1.5768079791609506,
        "vx": 0.16009266454570117
      },
      "index": 273
    },
    {
      "source": {
        "id": "Habermas Machine",
        "title": "Habermas Machine",
        "tags": [
          "project"
        ],
        "content": "#project\n\nhttps://www.science.org/doi/10.1126/science.adq2852\n\nAI system that uses two fine-tuned language models to mediate group discussions and help people with differing opinions find common ground on contentious issues. The system takes individual written opinions from participants, generates candidate \"group statements\" that synthesize everyone's perspectives, and iteratively refines these statements based on participant feedback until the group converges on a shared position. Research showed that AI-mediated discussions were more effective than unmediated opinion exposure at causing people to change their minds and reach consensus, with the AI slightly outperforming untrained human mediators.",
        "size": 22.857142857142858,
        "index": 185,
        "x": 1358.4189684092353,
        "y": 558.0232276908089,
        "vy": -1.3801763496591373,
        "vx": 0.3119158781573072
      },
      "target": {
        "id": "Matthew Botvinick",
        "title": "Matthew Botvinick",
        "tags": [
          "person"
        ],
        "content": "#person",
        "size": 18.285714285714285,
        "index": 139,
        "x": 1370.3134606965014,
        "y": 676.0315589136434,
        "vy": -1.3768077843382622,
        "vx": 0.20959930595528592
      },
      "index": 274
    },
    {
      "source": {
        "id": "Habermas Machine",
        "title": "Habermas Machine",
        "tags": [
          "project"
        ],
        "content": "#project\n\nhttps://www.science.org/doi/10.1126/science.adq2852\n\nAI system that uses two fine-tuned language models to mediate group discussions and help people with differing opinions find common ground on contentious issues. The system takes individual written opinions from participants, generates candidate \"group statements\" that synthesize everyone's perspectives, and iteratively refines these statements based on participant feedback until the group converges on a shared position. Research showed that AI-mediated discussions were more effective than unmediated opinion exposure at causing people to change their minds and reach consensus, with the AI slightly outperforming untrained human mediators.",
        "size": 22.857142857142858,
        "index": 185,
        "x": 1358.4189684092353,
        "y": 558.0232276908089,
        "vy": -1.3801763496591373,
        "vx": 0.3119158781573072
      },
      "target": {
        "id": "Christopher Summerfield",
        "title": "Christopher Summerfield",
        "tags": [
          "person"
        ],
        "content": "#person",
        "size": 18.285714285714285,
        "index": 318,
        "x": 1037.8359073408596,
        "y": 622.2526019294744,
        "vy": -1.2802118806311242,
        "vx": 0.2036645449820769
      },
      "index": 275
    },
    {
      "source": {
        "id": "Justin Reppert",
        "title": "Justin Reppert",
        "tags": [
          "person"
        ],
        "content": "#person \n\nhttps://www.justinreppert.com/",
        "size": 17.714285714285715,
        "index": 189,
        "x": -2033.3844297777323,
        "y": 436.6834102604725,
        "vy": -0.35398510340090217,
        "vx": -0.10515507632354346
      },
      "target": {
        "id": "Elicit",
        "title": "Elicit",
        "tags": [
          "organization"
        ],
        "content": "#organization \n\nhttps://elicit.com/",
        "size": 20.57142857142857,
        "index": 218,
        "x": -2027.2873240845167,
        "y": 375.96539772345454,
        "vy": -0.29711133317878174,
        "vx": 0.26985144319061544
      },
      "index": 276
    },
    {
      "source": {
        "id": "Justin Reppert",
        "title": "Justin Reppert",
        "tags": [
          "person"
        ],
        "content": "#person \n\nhttps://www.justinreppert.com/",
        "size": 17.714285714285715,
        "index": 189,
        "x": -2033.3844297777323,
        "y": 436.6834102604725,
        "vy": -0.35398510340090217,
        "vx": -0.10515507632354346
      },
      "target": {
        "id": "Ought",
        "title": "Ought",
        "tags": [
          "organization"
        ],
        "content": "#organization \n\nhttps://ought.org/ (website is old)\n\nOught 1.0 has effectively become what is now Elicit (the company) continuing development on Elicit (the tool). Ought 2.0 has not yet done any public work.",
        "size": 23.42857142857143,
        "index": 306,
        "x": -1888.3722954977052,
        "y": 453.0511056583274,
        "vy": -0.2547035865836596,
        "vx": 0.0471838457037637
      },
      "index": 277
    },
    {
      "source": {
        "id": "Jack Wildman",
        "title": "Jack Wildman",
        "tags": [
          "person"
        ],
        "content": "#person",
        "size": 17.714285714285715,
        "index": 190,
        "x": -700.5637035103321,
        "y": -656.0808982602216,
        "vy": -0.44997863065831584,
        "vx": -0.36028652459093663
      },
      "target": {
        "id": "Future Search",
        "title": "Future Search",
        "tags": [
          "organization"
        ],
        "content": "#organization \n\nhttps://futuresearch.ai/",
        "size": 22.285714285714285,
        "index": 94,
        "x": -717.8526553292105,
        "y": -719.7276361443852,
        "vy": -0.4404383940953807,
        "vx": -0.3824839533244458
      },
      "index": 278
    },
    {
      "source": {
        "id": "Debate Maps",
        "title": "Debate Maps",
        "tags": [
          "project"
        ],
        "content": "#project \n\nhttps://www.societylibrary.org/debate-mapping-program\n\nAutomated pipeline for scraping online content, extracting arguments/claims, and organizing them into visual \"maps\" which structure the debate into a tree of statements and their logical relationships.",
        "size": 17.142857142857142,
        "index": 198,
        "x": -12.304182348450263,
        "y": -1067.4816033717182,
        "vy": -0.8325786381460952,
        "vx": -0.42144941558990606
      },
      "target": {
        "id": "The Society Library",
        "title": "The Society Library",
        "tags": [
          "organization"
        ],
        "content": "#organization \n\nhttps://societylibrary.medium.com/\nhttps://www.societylibrary.org/",
        "size": 18.285714285714285,
        "index": 274,
        "x": -123.51596440526208,
        "y": -972.6539885865532,
        "vy": -0.7846889723379666,
        "vx": -0.37290689478790895
      },
      "index": 279
    },
    {
      "source": {
        "id": "Outcome-based Reinforcement Learning to Predict the Future",
        "title": "Outcome-based Reinforcement Learning to Predict the Future",
        "tags": [
          "project"
        ],
        "content": "#project \n\nhttps://www.lightningrod.ai/outcome-based-reinforcement-learning-to-predict-the-future\n\nPaper presents a method for training AI models to make better probabilistic forecasts about future events using reinforcement learning with outcome-based rewards. The researchers adapted existing reinforcement learning algorithms (like GRPO and ReMax) to work with the messy, delayed feedback that comes from real-world prediction tasks, using a dataset of over 100,000 forecasting questions from sources like Polymarket. Their approach produces a 14B parameter model that matches the accuracy of frontier models like OpenAI's o1 while achieving better calibration, ultimately translating this into higher hypothetical trading profits when betting against prediction markets.",
        "size": 19.428571428571427,
        "index": 199,
        "x": -378.65594725565177,
        "y": -456.26349804572686,
        "vy": -0.6998449710775491,
        "vx": 0.06796453288662563
      },
      "target": {
        "id": "Ben Turtel",
        "title": "Ben Turtel",
        "tags": [
          "person"
        ],
        "content": "#person",
        "size": 17.714285714285715,
        "index": 405,
        "x": -483.86316913010216,
        "y": -469.8714481131538,
        "vy": -0.5954542916718669,
        "vx": 0.08327716391928561
      },
      "index": 280
    },
    {
      "source": {
        "id": "Outcome-based Reinforcement Learning to Predict the Future",
        "title": "Outcome-based Reinforcement Learning to Predict the Future",
        "tags": [
          "project"
        ],
        "content": "#project \n\nhttps://www.lightningrod.ai/outcome-based-reinforcement-learning-to-predict-the-future\n\nPaper presents a method for training AI models to make better probabilistic forecasts about future events using reinforcement learning with outcome-based rewards. The researchers adapted existing reinforcement learning algorithms (like GRPO and ReMax) to work with the messy, delayed feedback that comes from real-world prediction tasks, using a dataset of over 100,000 forecasting questions from sources like Polymarket. Their approach produces a 14B parameter model that matches the accuracy of frontier models like OpenAI's o1 while achieving better calibration, ultimately translating this into higher hypothetical trading profits when betting against prediction markets.",
        "size": 19.428571428571427,
        "index": 199,
        "x": -378.65594725565177,
        "y": -456.26349804572686,
        "vy": -0.6998449710775491,
        "vx": 0.06796453288662563
      },
      "target": {
        "id": "Danny Fanklin",
        "title": "Danny Fanklin",
        "tags": [
          "person"
        ],
        "content": "#person",
        "size": 17.714285714285715,
        "index": 59,
        "x": -431.6774386700457,
        "y": -494.60304253552584,
        "vy": -0.673501095578475,
        "vx": 0.04697915176713927
      },
      "index": 281
    },
    {
      "source": {
        "id": "Outcome-based Reinforcement Learning to Predict the Future",
        "title": "Outcome-based Reinforcement Learning to Predict the Future",
        "tags": [
          "project"
        ],
        "content": "#project \n\nhttps://www.lightningrod.ai/outcome-based-reinforcement-learning-to-predict-the-future\n\nPaper presents a method for training AI models to make better probabilistic forecasts about future events using reinforcement learning with outcome-based rewards. The researchers adapted existing reinforcement learning algorithms (like GRPO and ReMax) to work with the messy, delayed feedback that comes from real-world prediction tasks, using a dataset of over 100,000 forecasting questions from sources like Polymarket. Their approach produces a 14B parameter model that matches the accuracy of frontier models like OpenAI's o1 while achieving better calibration, ultimately translating this into higher hypothetical trading profits when betting against prediction markets.",
        "size": 19.428571428571427,
        "index": 199,
        "x": -378.65594725565177,
        "y": -456.26349804572686,
        "vy": -0.6998449710775491,
        "vx": 0.06796453288662563
      },
      "target": {
        "id": "Philipp Schoenegger",
        "title": "Philipp Schoenegger",
        "tags": [
          "person"
        ],
        "content": "#person \n\nhttps://philipp-schoenegger.weebly.com/",
        "size": 19.428571428571427,
        "index": 120,
        "x": -308.0505670602049,
        "y": -355.92667066361173,
        "vy": -0.7160490223836882,
        "vx": 0.03780161064368572
      },
      "index": 282
    },
    {
      "source": {
        "id": "Outcome-based Reinforcement Learning to Predict the Future",
        "title": "Outcome-based Reinforcement Learning to Predict the Future",
        "tags": [
          "project"
        ],
        "content": "#project \n\nhttps://www.lightningrod.ai/outcome-based-reinforcement-learning-to-predict-the-future\n\nPaper presents a method for training AI models to make better probabilistic forecasts about future events using reinforcement learning with outcome-based rewards. The researchers adapted existing reinforcement learning algorithms (like GRPO and ReMax) to work with the messy, delayed feedback that comes from real-world prediction tasks, using a dataset of over 100,000 forecasting questions from sources like Polymarket. Their approach produces a 14B parameter model that matches the accuracy of frontier models like OpenAI's o1 while achieving better calibration, ultimately translating this into higher hypothetical trading profits when betting against prediction markets.",
        "size": 19.428571428571427,
        "index": 199,
        "x": -378.65594725565177,
        "y": -456.26349804572686,
        "vy": -0.6998449710775491,
        "vx": 0.06796453288662563
      },
      "target": {
        "id": "Kris Skotheim",
        "title": "Kris Skotheim",
        "tags": [
          "person"
        ],
        "content": "#person",
        "size": 17.142857142857142,
        "index": 77,
        "x": -411.3547046728863,
        "y": -545.4937432550389,
        "vy": -0.6691253570982997,
        "vx": 0.05855254827232383
      },
      "index": 283
    },
    {
      "source": {
        "id": "Outcome-based Reinforcement Learning to Predict the Future",
        "title": "Outcome-based Reinforcement Learning to Predict the Future",
        "tags": [
          "project"
        ],
        "content": "#project \n\nhttps://www.lightningrod.ai/outcome-based-reinforcement-learning-to-predict-the-future\n\nPaper presents a method for training AI models to make better probabilistic forecasts about future events using reinforcement learning with outcome-based rewards. The researchers adapted existing reinforcement learning algorithms (like GRPO and ReMax) to work with the messy, delayed feedback that comes from real-world prediction tasks, using a dataset of over 100,000 forecasting questions from sources like Polymarket. Their approach produces a 14B parameter model that matches the accuracy of frontier models like OpenAI's o1 while achieving better calibration, ultimately translating this into higher hypothetical trading profits when betting against prediction markets.",
        "size": 19.428571428571427,
        "index": 199,
        "x": -378.65594725565177,
        "y": -456.26349804572686,
        "vy": -0.6998449710775491,
        "vx": 0.06796453288662563
      },
      "target": {
        "id": "Luke Hewitt",
        "title": "Luke Hewitt",
        "tags": [
          "person"
        ],
        "content": "#person",
        "size": 16.571428571428573,
        "index": 349,
        "x": -329.37185961867965,
        "y": -509.61354593330196,
        "vy": -0.6866191907943483,
        "vx": 0.1455171790943485
      },
      "index": 284
    },
    {
      "source": {
        "id": "Molly Hickman",
        "title": "Molly Hickman",
        "tags": [
          "person"
        ],
        "content": "#person \n\nhttps://hickman-santini.github.io/",
        "size": 17.142857142857142,
        "index": 200,
        "x": -196.22191279599275,
        "y": -377.8911140363388,
        "vy": -0.7503121038643427,
        "vx": 0.007565280402353203
      },
      "target": {
        "id": "Forecasting Research Institute",
        "title": "Forecasting Research Institute",
        "tags": [
          "organization"
        ],
        "content": "#organization \n\nhttps://forecastingresearch.org/",
        "size": 18.857142857142858,
        "index": 317,
        "x": -153.6122917815696,
        "y": -87.61093230240742,
        "vy": -0.8005381090437106,
        "vx": 0.1603930148987801
      },
      "index": 285
    },
    {
      "source": {
        "id": "Molly Hickman",
        "title": "Molly Hickman",
        "tags": [
          "person"
        ],
        "content": "#person \n\nhttps://hickman-santini.github.io/",
        "size": 17.142857142857142,
        "index": 200,
        "x": -196.22191279599275,
        "y": -377.8911140363388,
        "vy": -0.7503121038643427,
        "vx": 0.007565280402353203
      },
      "target": {
        "id": "Metaculus",
        "title": "Metaculus",
        "tags": [
          "organization"
        ],
        "content": "#organization\n\nhttps://www.metaculus.com/",
        "size": 20,
        "index": 126,
        "x": -291.65109336863395,
        "y": -589.3419401920876,
        "vy": -0.7257043832009319,
        "vx": -0.15741051377760798
      },
      "index": 286
    },
    {
      "source": {
        "id": "Bot Mediation",
        "title": "Bot Mediation",
        "tags": [
          "project"
        ],
        "content": "#project \n\nhttps://www.americanbar.org/groups/law_practice/resources/law-technology-today/2025/ai-powered-mediation-for-efficient-legal-dispute-resolution/\n\nAI system designed to mediate disputes between parties without human mediators. The platform claims to reduce mediation timeframes from months to days (without having to pay for professional mediation). The company was selected as a [finalist](https://www.techshow.com/2025/02/voting-is-closed-results-are-in-here-are-the-15-legal-tech-startups-selected-for-the-2025-startup-alley-at-aba-techshow/) in the American Bar Association's [TECHSHOW](https://www.techshow.com/) 2025 Startup Alley competition.",
        "size": 17.714285714285715,
        "index": 205,
        "x": -76.12155763233767,
        "y": 3799.673203372321,
        "vy": -0.9583820842188991,
        "vx": 0.6154945223821072
      },
      "target": {
        "id": "Curtis Holdsworth",
        "title": "Curtis Holdsworth",
        "tags": [
          "person"
        ],
        "content": "#person",
        "size": 16.571428571428573,
        "index": 104,
        "x": -75.87709658829615,
        "y": 3865.3037411350742,
        "vy": -0.9610831753813492,
        "vx": 0.6112639654312851
      },
      "index": 287
    },
    {
      "source": {
        "id": "Bot Mediation",
        "title": "Bot Mediation",
        "tags": [
          "project"
        ],
        "content": "#project \n\nhttps://www.americanbar.org/groups/law_practice/resources/law-technology-today/2025/ai-powered-mediation-for-efficient-legal-dispute-resolution/\n\nAI system designed to mediate disputes between parties without human mediators. The platform claims to reduce mediation timeframes from months to days (without having to pay for professional mediation). The company was selected as a [finalist](https://www.techshow.com/2025/02/voting-is-closed-results-are-in-here-are-the-15-legal-tech-startups-selected-for-the-2025-startup-alley-at-aba-techshow/) in the American Bar Association's [TECHSHOW](https://www.techshow.com/) 2025 Startup Alley competition.",
        "size": 17.714285714285715,
        "index": 205,
        "x": -76.12155763233767,
        "y": 3799.673203372321,
        "vy": -0.9583820842188991,
        "vx": 0.6154945223821072
      },
      "target": {
        "id": "Darren Fancher",
        "title": "Darren Fancher",
        "tags": [
          "person"
        ],
        "content": "#person",
        "size": 16.571428571428573,
        "index": 73,
        "x": -136.97161845378483,
        "y": 3776.950868675314,
        "vy": -0.979290813628424,
        "vx": 0.6311237182975112
      },
      "index": 288
    },
    {
      "source": {
        "id": "Bot Mediation",
        "title": "Bot Mediation",
        "tags": [
          "project"
        ],
        "content": "#project \n\nhttps://www.americanbar.org/groups/law_practice/resources/law-technology-today/2025/ai-powered-mediation-for-efficient-legal-dispute-resolution/\n\nAI system designed to mediate disputes between parties without human mediators. The platform claims to reduce mediation timeframes from months to days (without having to pay for professional mediation). The company was selected as a [finalist](https://www.techshow.com/2025/02/voting-is-closed-results-are-in-here-are-the-15-legal-tech-startups-selected-for-the-2025-startup-alley-at-aba-techshow/) in the American Bar Association's [TECHSHOW](https://www.techshow.com/) 2025 Startup Alley competition.",
        "size": 17.714285714285715,
        "index": 205,
        "x": -76.12155763233767,
        "y": 3799.673203372321,
        "vy": -0.9583820842188991,
        "vx": 0.6154945223821072
      },
      "target": {
        "id": "Nick Hikita",
        "title": "Nick Hikita",
        "tags": [
          "person"
        ],
        "content": "#person",
        "size": 16.571428571428573,
        "index": 342,
        "x": -13.373112740791635,
        "y": 3784.34210242425,
        "vy": -0.9787992058312024,
        "vx": 0.606886162856365
      },
      "index": 289
    },
    {
      "source": {
        "id": "Linters for Thought",
        "title": "Linters for Thought",
        "tags": [
          "project"
        ],
        "content": "#project \n\nhttps://textpress.md/jlevy/d/lft.html\n\nEssay about using AI and software engineering principles to augment human intelligence and improve collective problem-solving. The essay proposes developing \"linters\" for human reasoning - automated tools that catch errors in logic, fact-checking, and consistency in writing and decision-making, similar to how code linters catch programming errors.",
        "size": 16.571428571428573,
        "index": 206,
        "x": 3931.383288706197,
        "y": -351.52681930047817,
        "vy": -1.3812106493102614,
        "vx": 0.2685751634044294
      },
      "target": {
        "id": "Joshua Levy",
        "title": "Joshua Levy",
        "tags": [
          "person"
        ],
        "content": "#person \n\nhttps://github.com/jlevy",
        "size": 16.571428571428573,
        "index": 228,
        "x": 3872.379266414667,
        "y": -371.0092680854118,
        "vy": -1.393989296412193,
        "vx": 0.27335356237171915
      },
      "index": 290
    },
    {
      "source": {
        "id": "Lizka Vaintrob",
        "title": "Lizka Vaintrob",
        "tags": [
          "person"
        ],
        "content": "#person",
        "size": 17.714285714285715,
        "index": 207,
        "x": -1838.159891060826,
        "y": 279.2616617432391,
        "vy": -0.17469176347322712,
        "vx": 0.2327175786848521
      },
      "target": {
        "id": "Forethought",
        "title": "Forethought",
        "tags": [
          "organization"
        ],
        "content": "#organization \n\nhttps://www.forethought.org/",
        "size": 17.714285714285715,
        "index": 202,
        "x": -1771.7035542392243,
        "y": 317.48703027353775,
        "vy": -0.1877889855715367,
        "vx": 0.2317876718704255
      },
      "index": 291
    },
    {
      "source": {
        "id": "Fine-tuning language models to find agreement among humans with diverse preferences",
        "title": "Fine-tuning language models to find agreement among humans with diverse preferences",
        "tags": [
          "project"
        ],
        "content": "#project \n\nhttps://proceedings.neurips.cc/paper_files/paper/2022/file/f978c8f3b5f399cae464e85f72e28503-Paper-Conference.pdf\n\nPaper presents a system that fine-tunes an LLM to generate consensus statements that maximize expected approval among groups of people with diverse political opinions. The researchers developed a method where participants provide written opinions on political questions, and an AI system generates candidate consensus statements that aim to find common ground among the group members.\n\n(research from 2022)",
        "size": 22.285714285714285,
        "index": 208,
        "x": 1279.701405292439,
        "y": 602.5496145797362,
        "vy": -1.292699622768919,
        "vx": 0.14077245550259992
      },
      "target": {
        "id": "Michiel Bakker",
        "title": "Michiel Bakker",
        "tags": [
          "person"
        ],
        "content": "#person \n\nhttps://miba.dev/",
        "size": 21.142857142857142,
        "index": 284,
        "x": 1225.501866244462,
        "y": 730.5079873536822,
        "vy": -1.2714527484014098,
        "vx": 0.27625785273055353
      },
      "index": 292
    },
    {
      "source": {
        "id": "Fine-tuning language models to find agreement among humans with diverse preferences",
        "title": "Fine-tuning language models to find agreement among humans with diverse preferences",
        "tags": [
          "project"
        ],
        "content": "#project \n\nhttps://proceedings.neurips.cc/paper_files/paper/2022/file/f978c8f3b5f399cae464e85f72e28503-Paper-Conference.pdf\n\nPaper presents a system that fine-tunes an LLM to generate consensus statements that maximize expected approval among groups of people with diverse political opinions. The researchers developed a method where participants provide written opinions on political questions, and an AI system generates candidate consensus statements that aim to find common ground among the group members.\n\n(research from 2022)",
        "size": 22.285714285714285,
        "index": 208,
        "x": 1279.701405292439,
        "y": 602.5496145797362,
        "vy": -1.292699622768919,
        "vx": 0.14077245550259992
      },
      "target": {
        "id": "Martin Chadwick",
        "title": "Martin Chadwick",
        "tags": [
          "person"
        ],
        "content": "#person",
        "size": 17.142857142857142,
        "index": 300,
        "x": 1305.5587724179861,
        "y": 528.6905398060712,
        "vy": -1.2330608401345255,
        "vx": 0.24983720752209582
      },
      "index": 293
    },
    {
      "source": {
        "id": "Fine-tuning language models to find agreement among humans with diverse preferences",
        "title": "Fine-tuning language models to find agreement among humans with diverse preferences",
        "tags": [
          "project"
        ],
        "content": "#project \n\nhttps://proceedings.neurips.cc/paper_files/paper/2022/file/f978c8f3b5f399cae464e85f72e28503-Paper-Conference.pdf\n\nPaper presents a system that fine-tunes an LLM to generate consensus statements that maximize expected approval among groups of people with diverse political opinions. The researchers developed a method where participants provide written opinions on political questions, and an AI system generates candidate consensus statements that aim to find common ground among the group members.\n\n(research from 2022)",
        "size": 22.285714285714285,
        "index": 208,
        "x": 1279.701405292439,
        "y": 602.5496145797362,
        "vy": -1.292699622768919,
        "vx": 0.14077245550259992
      },
      "target": {
        "id": "Hannah Sheahan",
        "title": "Hannah Sheahan",
        "tags": [
          "person"
        ],
        "content": "#person \n\nhttps://hannahsheahan.github.io/",
        "size": 17.142857142857142,
        "index": 32,
        "x": 1355.6345270716827,
        "y": 495.9672877817744,
        "vy": -1.3810280862156508,
        "vx": 0.11788964987148415
      },
      "index": 294
    },
    {
      "source": {
        "id": "Fine-tuning language models to find agreement among humans with diverse preferences",
        "title": "Fine-tuning language models to find agreement among humans with diverse preferences",
        "tags": [
          "project"
        ],
        "content": "#project \n\nhttps://proceedings.neurips.cc/paper_files/paper/2022/file/f978c8f3b5f399cae464e85f72e28503-Paper-Conference.pdf\n\nPaper presents a system that fine-tunes an LLM to generate consensus statements that maximize expected approval among groups of people with diverse political opinions. The researchers developed a method where participants provide written opinions on political questions, and an AI system generates candidate consensus statements that aim to find common ground among the group members.\n\n(research from 2022)",
        "size": 22.285714285714285,
        "index": 208,
        "x": 1279.701405292439,
        "y": 602.5496145797362,
        "vy": -1.292699622768919,
        "vx": 0.14077245550259992
      },
      "target": {
        "id": "Michael Henry Tessler",
        "title": "Michael Henry Tessler",
        "tags": [
          "person"
        ],
        "content": "#person\n\nhttps://www.mit.edu/~tessler/",
        "size": 18.285714285714285,
        "index": 426,
        "x": 1312.71187781098,
        "y": 660.8616297480344,
        "vy": -1.3375108366075392,
        "vx": 0.285090803595964
      },
      "index": 295
    },
    {
      "source": {
        "id": "Fine-tuning language models to find agreement among humans with diverse preferences",
        "title": "Fine-tuning language models to find agreement among humans with diverse preferences",
        "tags": [
          "project"
        ],
        "content": "#project \n\nhttps://proceedings.neurips.cc/paper_files/paper/2022/file/f978c8f3b5f399cae464e85f72e28503-Paper-Conference.pdf\n\nPaper presents a system that fine-tunes an LLM to generate consensus statements that maximize expected approval among groups of people with diverse political opinions. The researchers developed a method where participants provide written opinions on political questions, and an AI system generates candidate consensus statements that aim to find common ground among the group members.\n\n(research from 2022)",
        "size": 22.285714285714285,
        "index": 208,
        "x": 1279.701405292439,
        "y": 602.5496145797362,
        "vy": -1.292699622768919,
        "vx": 0.14077245550259992
      },
      "target": {
        "id": "Lucy Campbell-Gillingham",
        "title": "Lucy Campbell-Gillingham",
        "tags": [
          "person"
        ],
        "content": "#person",
        "size": 17.714285714285715,
        "index": 407,
        "x": 1356.6725194125038,
        "y": 620.0271145323594,
        "vy": -1.3493309176801664,
        "vx": 0.2741836448885554
      },
      "index": 296
    },
    {
      "source": {
        "id": "Fine-tuning language models to find agreement among humans with diverse preferences",
        "title": "Fine-tuning language models to find agreement among humans with diverse preferences",
        "tags": [
          "project"
        ],
        "content": "#project \n\nhttps://proceedings.neurips.cc/paper_files/paper/2022/file/f978c8f3b5f399cae464e85f72e28503-Paper-Conference.pdf\n\nPaper presents a system that fine-tunes an LLM to generate consensus statements that maximize expected approval among groups of people with diverse political opinions. The researchers developed a method where participants provide written opinions on political questions, and an AI system generates candidate consensus statements that aim to find common ground among the group members.\n\n(research from 2022)",
        "size": 22.285714285714285,
        "index": 208,
        "x": 1279.701405292439,
        "y": 602.5496145797362,
        "vy": -1.292699622768919,
        "vx": 0.14077245550259992
      },
      "target": {
        "id": "Jan Balaguer",
        "title": "Jan Balaguer",
        "tags": [
          "person"
        ],
        "content": "#person \n\nhttps://www.janbalaguer.net/",
        "size": 17.714285714285715,
        "index": 273,
        "x": 1227.3076588867907,
        "y": 638.3421386448723,
        "vy": -1.2872783959036884,
        "vx": 0.14224906351640465
      },
      "index": 297
    },
    {
      "source": {
        "id": "Fine-tuning language models to find agreement among humans with diverse preferences",
        "title": "Fine-tuning language models to find agreement among humans with diverse preferences",
        "tags": [
          "project"
        ],
        "content": "#project \n\nhttps://proceedings.neurips.cc/paper_files/paper/2022/file/f978c8f3b5f399cae464e85f72e28503-Paper-Conference.pdf\n\nPaper presents a system that fine-tunes an LLM to generate consensus statements that maximize expected approval among groups of people with diverse political opinions. The researchers developed a method where participants provide written opinions on political questions, and an AI system generates candidate consensus statements that aim to find common ground among the group members.\n\n(research from 2022)",
        "size": 22.285714285714285,
        "index": 208,
        "x": 1279.701405292439,
        "y": 602.5496145797362,
        "vy": -1.292699622768919,
        "vx": 0.14077245550259992
      },
      "target": {
        "id": "Nat McAleese",
        "title": "Nat McAleese",
        "tags": [
          "person"
        ],
        "content": "#person",
        "size": 16.571428571428573,
        "index": 217,
        "x": 1202.6273734661868,
        "y": 517.211418216224,
        "vy": -1.2529539184120402,
        "vx": 0.15368243497011086
      },
      "index": 298
    },
    {
      "source": {
        "id": "Fine-tuning language models to find agreement among humans with diverse preferences",
        "title": "Fine-tuning language models to find agreement among humans with diverse preferences",
        "tags": [
          "project"
        ],
        "content": "#project \n\nhttps://proceedings.neurips.cc/paper_files/paper/2022/file/f978c8f3b5f399cae464e85f72e28503-Paper-Conference.pdf\n\nPaper presents a system that fine-tunes an LLM to generate consensus statements that maximize expected approval among groups of people with diverse political opinions. The researchers developed a method where participants provide written opinions on political questions, and an AI system generates candidate consensus statements that aim to find common ground among the group members.\n\n(research from 2022)",
        "size": 22.285714285714285,
        "index": 208,
        "x": 1279.701405292439,
        "y": 602.5496145797362,
        "vy": -1.292699622768919,
        "vx": 0.14077245550259992
      },
      "target": {
        "id": "Amelia Glaese",
        "title": "Amelia Glaese",
        "tags": [
          "person"
        ],
        "content": "#person",
        "size": 16.571428571428573,
        "index": 7,
        "x": 1255.5505198378098,
        "y": 499.37128163947017,
        "vy": -1.364812927919691,
        "vx": -0.011039383509637091
      },
      "index": 299
    },
    {
      "source": {
        "id": "Fine-tuning language models to find agreement among humans with diverse preferences",
        "title": "Fine-tuning language models to find agreement among humans with diverse preferences",
        "tags": [
          "project"
        ],
        "content": "#project \n\nhttps://proceedings.neurips.cc/paper_files/paper/2022/file/f978c8f3b5f399cae464e85f72e28503-Paper-Conference.pdf\n\nPaper presents a system that fine-tunes an LLM to generate consensus statements that maximize expected approval among groups of people with diverse political opinions. The researchers developed a method where participants provide written opinions on political questions, and an AI system generates candidate consensus statements that aim to find common ground among the group members.\n\n(research from 2022)",
        "size": 22.285714285714285,
        "index": 208,
        "x": 1279.701405292439,
        "y": 602.5496145797362,
        "vy": -1.292699622768919,
        "vx": 0.14077245550259992
      },
      "target": {
        "id": "John Aslanides",
        "title": "John Aslanides",
        "tags": [
          "person"
        ],
        "content": "#person",
        "size": 16.571428571428573,
        "index": 337,
        "x": 1303.997142438219,
        "y": 470.9978661943301,
        "vy": -1.2269251561845316,
        "vx": 0.008905467057737178
      },
      "index": 300
    },
    {
      "source": {
        "id": "Fine-tuning language models to find agreement among humans with diverse preferences",
        "title": "Fine-tuning language models to find agreement among humans with diverse preferences",
        "tags": [
          "project"
        ],
        "content": "#project \n\nhttps://proceedings.neurips.cc/paper_files/paper/2022/file/f978c8f3b5f399cae464e85f72e28503-Paper-Conference.pdf\n\nPaper presents a system that fine-tunes an LLM to generate consensus statements that maximize expected approval among groups of people with diverse political opinions. The researchers developed a method where participants provide written opinions on political questions, and an AI system generates candidate consensus statements that aim to find common ground among the group members.\n\n(research from 2022)",
        "size": 22.285714285714285,
        "index": 208,
        "x": 1279.701405292439,
        "y": 602.5496145797362,
        "vy": -1.292699622768919,
        "vx": 0.14077245550259992
      },
      "target": {
        "id": "Matthew Botvinick",
        "title": "Matthew Botvinick",
        "tags": [
          "person"
        ],
        "content": "#person",
        "size": 18.285714285714285,
        "index": 139,
        "x": 1370.3134606965014,
        "y": 676.0315589136434,
        "vy": -1.3768077843382622,
        "vx": 0.20959930595528592
      },
      "index": 301
    },
    {
      "source": {
        "id": "Fine-tuning language models to find agreement among humans with diverse preferences",
        "title": "Fine-tuning language models to find agreement among humans with diverse preferences",
        "tags": [
          "project"
        ],
        "content": "#project \n\nhttps://proceedings.neurips.cc/paper_files/paper/2022/file/f978c8f3b5f399cae464e85f72e28503-Paper-Conference.pdf\n\nPaper presents a system that fine-tunes an LLM to generate consensus statements that maximize expected approval among groups of people with diverse political opinions. The researchers developed a method where participants provide written opinions on political questions, and an AI system generates candidate consensus statements that aim to find common ground among the group members.\n\n(research from 2022)",
        "size": 22.285714285714285,
        "index": 208,
        "x": 1279.701405292439,
        "y": 602.5496145797362,
        "vy": -1.292699622768919,
        "vx": 0.14077245550259992
      },
      "target": {
        "id": "Christopher Summerfield",
        "title": "Christopher Summerfield",
        "tags": [
          "person"
        ],
        "content": "#person",
        "size": 18.285714285714285,
        "index": 318,
        "x": 1037.8359073408596,
        "y": 622.2526019294744,
        "vy": -1.2802118806311242,
        "vx": 0.2036645449820769
      },
      "index": 302
    },
    {
      "source": {
        "id": "Nathan Young",
        "title": "Nathan Young",
        "tags": [
          "person"
        ],
        "content": "#person \n\nhttps://nathanpmyoung.substack.com/",
        "size": 16.571428571428573,
        "index": 210,
        "x": -545.145274347403,
        "y": -983.4306936756087,
        "vy": -0.432991516163353,
        "vx": -0.3427906603942827
      },
      "target": {
        "id": "Goodheart Labs",
        "title": "Goodheart Labs",
        "tags": [
          "organization"
        ],
        "content": "#organization \n\nhttps://goodheartlabs.com/",
        "size": 17.714285714285715,
        "index": 229,
        "x": -536.1165178685933,
        "y": -862.8993316329166,
        "vy": -0.48917270690236947,
        "vx": -0.3239942741752614
      },
      "index": 303
    },
    {
      "source": {
        "id": "Ivan Vendrov",
        "title": "Ivan Vendrov",
        "tags": [
          "person"
        ],
        "content": "#person \n\nhttps://www.vendrov.ai/",
        "size": 17.714285714285715,
        "index": 215,
        "x": 1810.5588734061498,
        "y": 305.9910348849648,
        "vy": -1.443643107030699,
        "vx": 0.22600812026212258
      },
      "target": {
        "id": "Midjourney",
        "title": "Midjourney",
        "tags": [
          "organization"
        ],
        "content": "#organization \n\nhttps://www.midjourney.com\n\nThe Collective Intelligence team at Midjourney (currently stealth).",
        "size": 17.142857142857142,
        "index": 348,
        "x": 2062.822718578165,
        "y": 201.81415254647143,
        "vy": -1.9018493554178864,
        "vx": 0.1061578931981707
      },
      "index": 304
    },
    {
      "source": {
        "id": "Can AI bring deliberative democracy to the masses?",
        "title": "Can AI bring deliberative democracy to the masses?",
        "tags": [
          "project"
        ],
        "content": "#project \n\nhttps://www.law.nyu.edu/sites/default/files/Helen%20Landemore%20Can%20AI%20bring%20deliberative%20democracy%20to%20the%20masses.pdf\n\nConceptual paper analyzes France's 2019 \"Great National Debate\" as a case study for scaling deliberative democracy, then theorizes two AI-augmented models: \n1. mass online deliberation platforms that use algorithms to cluster and organize arguments among thousands of participants, and \n2. rotating randomly-selected citizen assemblies supported by AI for facilitation, translation, fact-checking, and data synthesis.",
        "size": 16.571428571428573,
        "index": 219,
        "x": 1516.8095912183417,
        "y": 1184.4598882773644,
        "vy": -1.2824724812664254,
        "vx": 0.37539069564545313
      },
      "target": {
        "id": "Hélène Landemore",
        "title": "Hélène Landemore",
        "tags": [
          "person"
        ],
        "content": "#person\n\nhttps://www.helenelandemore.com/",
        "size": 17.142857142857142,
        "index": 425,
        "x": 1482.5429370856566,
        "y": 1037.3892381314145,
        "vy": -1.2905561967248251,
        "vx": 0.2545804807479611
      },
      "index": 305
    },
    {
      "source": {
        "id": "Colin Megill",
        "title": "Colin Megill",
        "tags": [
          "person"
        ],
        "content": "#person \n\nhttps://colinmegill.com/",
        "size": 17.142857142857142,
        "index": 220,
        "x": 2149.182032920351,
        "y": 798.9739724169445,
        "vy": -1.5233448647940664,
        "vx": 0.3617601303910128
      },
      "target": {
        "id": "The Computational Democracy Project",
        "title": "The Computational Democracy Project",
        "tags": [
          "organization"
        ],
        "content": "#organization \n\nhttps://compdemocracy.org/",
        "size": 19.428571428571427,
        "index": 138,
        "x": 2057.334021465157,
        "y": 849.740764420989,
        "vy": -1.5779545596273115,
        "vx": 0.3854928364433583
      },
      "index": 306
    },
    {
      "source": {
        "id": "John Bash",
        "title": "John Bash",
        "tags": [
          "person"
        ],
        "content": "#person",
        "size": 17.142857142857142,
        "index": 222,
        "x": -295.00162878021047,
        "y": -699.0077204062693,
        "vy": -0.8031477917263522,
        "vx": -0.3241548422855935
      },
      "target": {
        "id": "Metaculus",
        "title": "Metaculus",
        "tags": [
          "organization"
        ],
        "content": "#organization\n\nhttps://www.metaculus.com/",
        "size": 20,
        "index": 126,
        "x": -291.65109336863395,
        "y": -589.3419401920876,
        "vy": -0.7257043832009319,
        "vx": -0.15741051377760798
      },
      "index": 307
    },
    {
      "source": {
        "id": "Andreas Stuhlmüller",
        "title": "Andreas Stuhlmüller",
        "tags": [
          "person"
        ],
        "content": "#person \n\nhttps://stuhlmueller.org/",
        "size": 17.714285714285715,
        "index": 223,
        "x": -2027.6511164368258,
        "y": 499.3550973775167,
        "vy": -0.1844689960451033,
        "vx": 0.38887876049926895
      },
      "target": {
        "id": "Elicit",
        "title": "Elicit",
        "tags": [
          "organization"
        ],
        "content": "#organization \n\nhttps://elicit.com/",
        "size": 20.57142857142857,
        "index": 218,
        "x": -2027.2873240845167,
        "y": 375.96539772345454,
        "vy": -0.29711133317878174,
        "vx": 0.26985144319061544
      },
      "index": 308
    },
    {
      "source": {
        "id": "Andreas Stuhlmüller",
        "title": "Andreas Stuhlmüller",
        "tags": [
          "person"
        ],
        "content": "#person \n\nhttps://stuhlmueller.org/",
        "size": 17.714285714285715,
        "index": 223,
        "x": -2027.6511164368258,
        "y": 499.3550973775167,
        "vy": -0.1844689960451033,
        "vx": 0.38887876049926895
      },
      "target": {
        "id": "Ought",
        "title": "Ought",
        "tags": [
          "organization"
        ],
        "content": "#organization \n\nhttps://ought.org/ (website is old)\n\nOught 1.0 has effectively become what is now Elicit (the company) continuing development on Elicit (the tool). Ought 2.0 has not yet done any public work.",
        "size": 23.42857142857143,
        "index": 306,
        "x": -1888.3722954977052,
        "y": 453.0511056583274,
        "vy": -0.2547035865836596,
        "vx": 0.0471838457037637
      },
      "index": 309
    },
    {
      "source": {
        "id": "The AI Adoption Gap - Preparing the US Government for Advanced AI",
        "title": "The AI Adoption Gap - Preparing the US Government for Advanced AI",
        "tags": [
          "project"
        ],
        "content": "#project \n\nhttps://www.forethought.org/research/the-ai-adoption-gap\n\nResearch piece examining how the US federal government is falling behind the private sector in adopting AI technologies, creating risks for democratic institutions and national security. The research argues this gap could leave the government unable to effectively respond to AI-driven existential challenges or maintain oversight of rapidly advancing AI systems.",
        "size": 16.571428571428573,
        "index": 224,
        "x": -1893.0985324874694,
        "y": 206.04870559270014,
        "vy": -0.16286962752388628,
        "vx": 0.24099820248207385
      },
      "target": {
        "id": "Lizka Vaintrob",
        "title": "Lizka Vaintrob",
        "tags": [
          "person"
        ],
        "content": "#person",
        "size": 17.714285714285715,
        "index": 207,
        "x": -1838.159891060826,
        "y": 279.2616617432391,
        "vy": -0.17469176347322712,
        "vx": 0.2327175786848521
      },
      "index": 310
    },
    {
      "source": {
        "id": "Zachary Jacobs",
        "title": "Zachary Jacobs",
        "tags": [
          "person"
        ],
        "content": "#person",
        "size": 17.142857142857142,
        "index": 226,
        "x": -218.36615785961288,
        "y": -18.596035026356134,
        "vy": -0.6399477037238545,
        "vx": 0.20487463098357075
      },
      "target": {
        "id": "Forecasting Research Institute",
        "title": "Forecasting Research Institute",
        "tags": [
          "organization"
        ],
        "content": "#organization \n\nhttps://forecastingresearch.org/",
        "size": 18.857142857142858,
        "index": 317,
        "x": -153.6122917815696,
        "y": -87.61093230240742,
        "vy": -0.8005381090437106,
        "vx": 0.1603930148987801
      },
      "index": 311
    },
    {
      "source": {
        "id": "Rethinking Machine Ethics –Can LLMs Perform Moral Reasoning through the Lens of Moral Theories?",
        "title": "Rethinking Machine Ethics –Can LLMs Perform Moral Reasoning through the Lens of Moral Theories?",
        "tags": [
          "project"
        ],
        "content": "#project \n\nhttps://arxiv.org/pdf/2308.15399\n\nPaper presents a framework for enabling LLMs to make moral judgments by grounding them in established moral theories rather than learning from crowdsourced data. The researchers develop prompting techniques that guide models like GPT-4 to reason through ethical scenarios using theories from normative ethics (Justice, Deontology, Utilitarianism) and moral psychology (Theory of Dyadic Morality), producing explainable moral reasoning and decisions.",
        "size": 20.57142857142857,
        "index": 227,
        "x": 242.51830751517156,
        "y": 2646.2101137593295,
        "vy": -0.6996688990825385,
        "vx": 1.61146615333836
      },
      "target": {
        "id": "Jingyan Zhou",
        "title": "Jingyan Zhou",
        "tags": [
          "person"
        ],
        "content": "#person \n\nhttps://para-zhou.github.io/",
        "size": 16.571428571428573,
        "index": 124,
        "x": 219.24634731218836,
        "y": 2698.421522266516,
        "vy": -0.7266394680049623,
        "vx": 1.702582676465687
      },
      "index": 312
    },
    {
      "source": {
        "id": "Rethinking Machine Ethics –Can LLMs Perform Moral Reasoning through the Lens of Moral Theories?",
        "title": "Rethinking Machine Ethics –Can LLMs Perform Moral Reasoning through the Lens of Moral Theories?",
        "tags": [
          "project"
        ],
        "content": "#project \n\nhttps://arxiv.org/pdf/2308.15399\n\nPaper presents a framework for enabling LLMs to make moral judgments by grounding them in established moral theories rather than learning from crowdsourced data. The researchers develop prompting techniques that guide models like GPT-4 to reason through ethical scenarios using theories from normative ethics (Justice, Deontology, Utilitarianism) and moral psychology (Theory of Dyadic Morality), producing explainable moral reasoning and decisions.",
        "size": 20.57142857142857,
        "index": 227,
        "x": 242.51830751517156,
        "y": 2646.2101137593295,
        "vy": -0.6996688990825385,
        "vx": 1.61146615333836
      },
      "target": {
        "id": "Minda Hu",
        "title": "Minda Hu",
        "tags": [
          "person"
        ],
        "content": "#person",
        "size": 16.571428571428573,
        "index": 402,
        "x": 306.67622969538365,
        "y": 2681.6244558357957,
        "vy": -1.1451077580037012,
        "vx": 1.3671397772130625
      },
      "index": 313
    },
    {
      "source": {
        "id": "Rethinking Machine Ethics –Can LLMs Perform Moral Reasoning through the Lens of Moral Theories?",
        "title": "Rethinking Machine Ethics –Can LLMs Perform Moral Reasoning through the Lens of Moral Theories?",
        "tags": [
          "project"
        ],
        "content": "#project \n\nhttps://arxiv.org/pdf/2308.15399\n\nPaper presents a framework for enabling LLMs to make moral judgments by grounding them in established moral theories rather than learning from crowdsourced data. The researchers develop prompting techniques that guide models like GPT-4 to reason through ethical scenarios using theories from normative ethics (Justice, Deontology, Utilitarianism) and moral psychology (Theory of Dyadic Morality), producing explainable moral reasoning and decisions.",
        "size": 20.57142857142857,
        "index": 227,
        "x": 242.51830751517156,
        "y": 2646.2101137593295,
        "vy": -0.6996688990825385,
        "vx": 1.61146615333836
      },
      "target": {
        "id": "Junan Li",
        "title": "Junan Li",
        "tags": [
          "person"
        ],
        "content": "#person",
        "size": 16.571428571428573,
        "index": 240,
        "x": 165.7542383900035,
        "y": 2677.979415021302,
        "vy": -0.8527767529381781,
        "vx": 1.768730667097703
      },
      "index": 314
    },
    {
      "source": {
        "id": "Rethinking Machine Ethics –Can LLMs Perform Moral Reasoning through the Lens of Moral Theories?",
        "title": "Rethinking Machine Ethics –Can LLMs Perform Moral Reasoning through the Lens of Moral Theories?",
        "tags": [
          "project"
        ],
        "content": "#project \n\nhttps://arxiv.org/pdf/2308.15399\n\nPaper presents a framework for enabling LLMs to make moral judgments by grounding them in established moral theories rather than learning from crowdsourced data. The researchers develop prompting techniques that guide models like GPT-4 to reason through ethical scenarios using theories from normative ethics (Justice, Deontology, Utilitarianism) and moral psychology (Theory of Dyadic Morality), producing explainable moral reasoning and decisions.",
        "size": 20.57142857142857,
        "index": 227,
        "x": 242.51830751517156,
        "y": 2646.2101137593295,
        "vy": -0.6996688990825385,
        "vx": 1.61146615333836
      },
      "target": {
        "id": "Xiaoying Zhang",
        "title": "Xiaoying Zhang",
        "tags": [
          "person"
        ],
        "content": "#person",
        "size": 16.571428571428573,
        "index": 315,
        "x": 175.88879151181476,
        "y": 2621.7430464125996,
        "vy": -0.8909373776694122,
        "vx": 1.5605222027394763
      },
      "index": 315
    },
    {
      "source": {
        "id": "Rethinking Machine Ethics –Can LLMs Perform Moral Reasoning through the Lens of Moral Theories?",
        "title": "Rethinking Machine Ethics –Can LLMs Perform Moral Reasoning through the Lens of Moral Theories?",
        "tags": [
          "project"
        ],
        "content": "#project \n\nhttps://arxiv.org/pdf/2308.15399\n\nPaper presents a framework for enabling LLMs to make moral judgments by grounding them in established moral theories rather than learning from crowdsourced data. The researchers develop prompting techniques that guide models like GPT-4 to reason through ethical scenarios using theories from normative ethics (Justice, Deontology, Utilitarianism) and moral psychology (Theory of Dyadic Morality), producing explainable moral reasoning and decisions.",
        "size": 20.57142857142857,
        "index": 227,
        "x": 242.51830751517156,
        "y": 2646.2101137593295,
        "vy": -0.6996688990825385,
        "vx": 1.61146615333836
      },
      "target": {
        "id": "Xixin Wu",
        "title": "Xixin Wu",
        "tags": [
          "person"
        ],
        "content": "#person",
        "size": 16.571428571428573,
        "index": 362,
        "x": 224.68590181083243,
        "y": 2755.5934482456846,
        "vy": -0.7631017771887649,
        "vx": 1.6999789634904483
      },
      "index": 316
    },
    {
      "source": {
        "id": "Rethinking Machine Ethics –Can LLMs Perform Moral Reasoning through the Lens of Moral Theories?",
        "title": "Rethinking Machine Ethics –Can LLMs Perform Moral Reasoning through the Lens of Moral Theories?",
        "tags": [
          "project"
        ],
        "content": "#project \n\nhttps://arxiv.org/pdf/2308.15399\n\nPaper presents a framework for enabling LLMs to make moral judgments by grounding them in established moral theories rather than learning from crowdsourced data. The researchers develop prompting techniques that guide models like GPT-4 to reason through ethical scenarios using theories from normative ethics (Justice, Deontology, Utilitarianism) and moral psychology (Theory of Dyadic Morality), producing explainable moral reasoning and decisions.",
        "size": 20.57142857142857,
        "index": 227,
        "x": 242.51830751517156,
        "y": 2646.2101137593295,
        "vy": -0.6996688990825385,
        "vx": 1.61146615333836
      },
      "target": {
        "id": "Irwin King",
        "title": "Irwin King",
        "tags": [
          "person"
        ],
        "content": "#person",
        "size": 16.571428571428573,
        "index": 395,
        "x": 274.0512839825302,
        "y": 2728.537236792891,
        "vy": -0.937281149372095,
        "vx": 1.664799685818256
      },
      "index": 317
    },
    {
      "source": {
        "id": "Rethinking Machine Ethics –Can LLMs Perform Moral Reasoning through the Lens of Moral Theories?",
        "title": "Rethinking Machine Ethics –Can LLMs Perform Moral Reasoning through the Lens of Moral Theories?",
        "tags": [
          "project"
        ],
        "content": "#project \n\nhttps://arxiv.org/pdf/2308.15399\n\nPaper presents a framework for enabling LLMs to make moral judgments by grounding them in established moral theories rather than learning from crowdsourced data. The researchers develop prompting techniques that guide models like GPT-4 to reason through ethical scenarios using theories from normative ethics (Justice, Deontology, Utilitarianism) and moral psychology (Theory of Dyadic Morality), producing explainable moral reasoning and decisions.",
        "size": 20.57142857142857,
        "index": 227,
        "x": 242.51830751517156,
        "y": 2646.2101137593295,
        "vy": -0.6996688990825385,
        "vx": 1.61146615333836
      },
      "target": {
        "id": "Helen Meng",
        "title": "Helen Meng",
        "tags": [
          "person"
        ],
        "content": "#person",
        "size": 16.571428571428573,
        "index": 196,
        "x": 173.4878607384688,
        "y": 2733.421782830301,
        "vy": -0.8257320566244282,
        "vx": 1.676113870340737
      },
      "index": 318
    },
    {
      "source": {
        "id": "Goodheart Labs",
        "title": "Goodheart Labs",
        "tags": [
          "organization"
        ],
        "content": "#organization \n\nhttps://goodheartlabs.com/",
        "size": 17.714285714285715,
        "index": 229,
        "x": -536.1165178685933,
        "y": -862.8993316329166,
        "vy": -0.48917270690236947,
        "vx": -0.3239942741752614
      },
      "target": {
        "id": "AI Forecasting Benchmark",
        "title": "AI Forecasting Benchmark",
        "tags": [
          "project"
        ],
        "content": "#project \n\nhttps://www.metaculus.com/aib/\n\nMetaculus AIB (AI Benchmarking) is a series that benchmarks \"the state of the art in AI forecasting against the best humans on real-world questions.\"",
        "size": 20.57142857142857,
        "index": 178,
        "x": -491.69904780690194,
        "y": -761.3030318136879,
        "vy": -0.5357053744666,
        "vx": -0.2989940012828504
      },
      "index": 319
    },
    {
      "source": {
        "id": "Artifact",
        "title": "Artifact",
        "tags": [
          "project"
        ],
        "content": "#project \n\nhttps://techcrunch.com/2024/01/18/why-artifact-from-instagrams-founders-failed-shut-down/\n\nArtifact was an AI-powered news aggregation app created by Instagram's co-founders that used machine learning to recommend articles, summarize news stories, and rewrite clickbait headlines into clearer formats. The app featured social elements like commenting and following, creating an engaged community around news consumption, but ultimately shut down in early 2024.\n\nThey failed because they couldn't achieve sustainable user growth beyond their initial core community, despite having a quality product that users genuinely engaged with.",
        "size": 17.142857142857142,
        "index": 230,
        "x": 3432.127389478733,
        "y": 2572.0140546656303,
        "vy": 0.7791806022442452,
        "vx": -2.008994530774766
      },
      "target": {
        "id": "Kevin Systrom",
        "title": "Kevin Systrom",
        "tags": [
          "person"
        ],
        "content": "#person",
        "size": 16.571428571428573,
        "index": 204,
        "x": 3386.618674604603,
        "y": 2616.600659473594,
        "vy": 0.7695935252283973,
        "vx": -2.01867740384388
      },
      "index": 320
    },
    {
      "source": {
        "id": "Artifact",
        "title": "Artifact",
        "tags": [
          "project"
        ],
        "content": "#project \n\nhttps://techcrunch.com/2024/01/18/why-artifact-from-instagrams-founders-failed-shut-down/\n\nArtifact was an AI-powered news aggregation app created by Instagram's co-founders that used machine learning to recommend articles, summarize news stories, and rewrite clickbait headlines into clearer formats. The app featured social elements like commenting and following, creating an engaged community around news consumption, but ultimately shut down in early 2024.\n\nThey failed because they couldn't achieve sustainable user growth beyond their initial core community, despite having a quality product that users genuinely engaged with.",
        "size": 17.142857142857142,
        "index": 230,
        "x": 3432.127389478733,
        "y": 2572.0140546656303,
        "vy": 0.7791806022442452,
        "vx": -2.008994530774766
      },
      "target": {
        "id": "Mike Krieger",
        "title": "Mike Krieger",
        "tags": [
          "person"
        ],
        "content": "#person",
        "size": 16.571428571428573,
        "index": 371,
        "x": 3476.3057953048537,
        "y": 2527.430930765537,
        "vy": 0.788304174590024,
        "vx": -1.9992528995574235
      },
      "index": 321
    },
    {
      "source": {
        "id": "Wearable Reasoner",
        "title": "Wearable Reasoner",
        "tags": [
          "project"
        ],
        "content": "#project \n\nhttps://dl.acm.org/doi/pdf/10.1145/3384657.3384799\n\nProof-of-concept wearable device (smart glasses with audio feedback) that uses AI to analyze spoken arguments in real-time and tell users whether claims are supported by evidence or not. The system employs argumentation mining techniques to classify statements and provides either simple feedback (\"supported/unsupported\") or explainable feedback that describes what type of evidence was found.",
        "size": 18.285714285714285,
        "index": 231,
        "x": 1030.178547158851,
        "y": -2736.2828627715576,
        "vy": -1.0835563501909211,
        "vx": -0.513382299860859
      },
      "target": {
        "id": "Valdemar Danry",
        "title": "Valdemar Danry",
        "tags": [
          "person"
        ],
        "content": "#person\n\nhttps://valdemardanry.com/",
        "size": 18.857142857142858,
        "index": 172,
        "x": 1102.4705595757653,
        "y": -2731.308366831243,
        "vy": -1.122899896351339,
        "vx": -0.5144777169524257
      },
      "index": 322
    },
    {
      "source": {
        "id": "Wearable Reasoner",
        "title": "Wearable Reasoner",
        "tags": [
          "project"
        ],
        "content": "#project \n\nhttps://dl.acm.org/doi/pdf/10.1145/3384657.3384799\n\nProof-of-concept wearable device (smart glasses with audio feedback) that uses AI to analyze spoken arguments in real-time and tell users whether claims are supported by evidence or not. The system employs argumentation mining techniques to classify statements and provides either simple feedback (\"supported/unsupported\") or explainable feedback that describes what type of evidence was found.",
        "size": 18.285714285714285,
        "index": 231,
        "x": 1030.178547158851,
        "y": -2736.2828627715576,
        "vy": -1.0835563501909211,
        "vx": -0.513382299860859
      },
      "target": {
        "id": "Pat Pataranutaporn",
        "title": "Pat Pataranutaporn",
        "tags": [
          "person"
        ],
        "content": "#person \n\nhttps://patpat.world/",
        "size": 18.285714285714285,
        "index": 298,
        "x": 1066.2378443172674,
        "y": -2682.567814798428,
        "vy": -1.1214877655381226,
        "vx": -0.5139886416200151
      },
      "index": 323
    },
    {
      "source": {
        "id": "Wearable Reasoner",
        "title": "Wearable Reasoner",
        "tags": [
          "project"
        ],
        "content": "#project \n\nhttps://dl.acm.org/doi/pdf/10.1145/3384657.3384799\n\nProof-of-concept wearable device (smart glasses with audio feedback) that uses AI to analyze spoken arguments in real-time and tell users whether claims are supported by evidence or not. The system employs argumentation mining techniques to classify statements and provides either simple feedback (\"supported/unsupported\") or explainable feedback that describes what type of evidence was found.",
        "size": 18.285714285714285,
        "index": 231,
        "x": 1030.178547158851,
        "y": -2736.2828627715576,
        "vy": -1.0835563501909211,
        "vx": -0.513382299860859
      },
      "target": {
        "id": "Yaoli Mao",
        "title": "Yaoli Mao",
        "tags": [
          "person"
        ],
        "content": "#person",
        "size": 17.142857142857142,
        "index": 424,
        "x": 1013.6770029340609,
        "y": -2810.870854276536,
        "vy": -1.1259491261914438,
        "vx": -0.4540425695118114
      },
      "index": 324
    },
    {
      "source": {
        "id": "Wearable Reasoner",
        "title": "Wearable Reasoner",
        "tags": [
          "project"
        ],
        "content": "#project \n\nhttps://dl.acm.org/doi/pdf/10.1145/3384657.3384799\n\nProof-of-concept wearable device (smart glasses with audio feedback) that uses AI to analyze spoken arguments in real-time and tell users whether claims are supported by evidence or not. The system employs argumentation mining techniques to classify statements and provides either simple feedback (\"supported/unsupported\") or explainable feedback that describes what type of evidence was found.",
        "size": 18.285714285714285,
        "index": 231,
        "x": 1030.178547158851,
        "y": -2736.2828627715576,
        "vy": -1.0835563501909211,
        "vx": -0.513382299860859
      },
      "target": {
        "id": "Pattie Maes",
        "title": "Pattie Maes",
        "tags": [
          "person"
        ],
        "content": "#person",
        "size": 17.714285714285715,
        "index": 396,
        "x": 974.5602589949608,
        "y": -2755.20656340418,
        "vy": -1.1595146505930813,
        "vx": -0.4926021629325841
      },
      "index": 325
    },
    {
      "source": {
        "id": "Eli Lifland",
        "title": "Eli Lifland",
        "tags": [
          "person"
        ],
        "content": "#person \n\nhttps://www.elilifland.com/",
        "size": 17.714285714285715,
        "index": 233,
        "x": -2088.797171070193,
        "y": 462.7401955014765,
        "vy": -0.3502859520662234,
        "vx": 0.08033116490001913
      },
      "target": {
        "id": "AI Future Project",
        "title": "AI Future Project",
        "tags": [
          "organization"
        ],
        "content": "#organization \n\nhttps://ai-futures.org/",
        "size": 17.142857142857142,
        "index": 203,
        "x": -2227.808470514541,
        "y": 447.0357084853911,
        "vy": -0.25788816758637106,
        "vx": 0.14402888519418866
      },
      "index": 326
    },
    {
      "source": {
        "id": "Eli Lifland",
        "title": "Eli Lifland",
        "tags": [
          "person"
        ],
        "content": "#person \n\nhttps://www.elilifland.com/",
        "size": 17.714285714285715,
        "index": 233,
        "x": -2088.797171070193,
        "y": 462.7401955014765,
        "vy": -0.3502859520662234,
        "vx": 0.08033116490001913
      },
      "target": {
        "id": "Ought",
        "title": "Ought",
        "tags": [
          "organization"
        ],
        "content": "#organization \n\nhttps://ought.org/ (website is old)\n\nOught 1.0 has effectively become what is now Elicit (the company) continuing development on Elicit (the tool). Ought 2.0 has not yet done any public work.",
        "size": 23.42857142857143,
        "index": 306,
        "x": -1888.3722954977052,
        "y": 453.0511056583274,
        "vy": -0.2547035865836596,
        "vx": 0.0471838457037637
      },
      "index": 327
    },
    {
      "source": {
        "id": "Brendan Fong",
        "title": "Brendan Fong",
        "tags": [
          "person"
        ],
        "content": "#person \n\nhttp://www.brendanfong.com/",
        "size": 16.571428571428573,
        "index": 235,
        "x": -2592.60808144401,
        "y": 1882.646090768226,
        "vy": 0.3273466998397428,
        "vx": 0.810667564561805
      },
      "target": {
        "id": "Topos Institute",
        "title": "Topos Institute",
        "tags": [
          "organization"
        ],
        "content": "#organization \n\nhttps://topos.institute",
        "size": 16.571428571428573,
        "index": 423,
        "x": -2538.395358238742,
        "y": 1913.1175309997868,
        "vy": 0.3426166271666348,
        "vx": 0.8005681842499485
      },
      "index": 328
    },
    {
      "source": {
        "id": "Iterated Decomposition - Improving Science Q&A by Supervising Reasoning Processes",
        "title": "Iterated Decomposition - Improving Science Q&A by Supervising Reasoning Processes",
        "tags": [
          "project"
        ],
        "content": "#project \n\nhttps://arxiv.org/pdf/2301.01751\nhttps://github.com/oughtinc/ice\n\nA human-in-the-loop workflow for breaking down complex question-answering tasks into smaller, interpretable steps for language models. The approach involves systematically decomposing complex tasks, evaluating intermediate results, diagnosing failures, and refining the decomposition through multiple iterations, supported by a visualization tool called ICE.",
        "size": 20,
        "index": 236,
        "x": -2081.8814606953724,
        "y": 401.41473238306327,
        "vy": -0.3344104152334175,
        "vx": 0.2216074759604618
      },
      "target": {
        "id": "Justin Reppert",
        "title": "Justin Reppert",
        "tags": [
          "person"
        ],
        "content": "#person \n\nhttps://www.justinreppert.com/",
        "size": 17.714285714285715,
        "index": 189,
        "x": -2033.3844297777323,
        "y": 436.6834102604725,
        "vy": -0.35398510340090217,
        "vx": -0.10515507632354346
      },
      "index": 329
    },
    {
      "source": {
        "id": "Iterated Decomposition - Improving Science Q&A by Supervising Reasoning Processes",
        "title": "Iterated Decomposition - Improving Science Q&A by Supervising Reasoning Processes",
        "tags": [
          "project"
        ],
        "content": "#project \n\nhttps://arxiv.org/pdf/2301.01751\nhttps://github.com/oughtinc/ice\n\nA human-in-the-loop workflow for breaking down complex question-answering tasks into smaller, interpretable steps for language models. The approach involves systematically decomposing complex tasks, evaluating intermediate results, diagnosing failures, and refining the decomposition through multiple iterations, supported by a visualization tool called ICE.",
        "size": 20,
        "index": 236,
        "x": -2081.8814606953724,
        "y": 401.41473238306327,
        "vy": -0.3344104152334175,
        "vx": 0.2216074759604618
      },
      "target": {
        "id": "Ben Rachbach",
        "title": "Ben Rachbach",
        "tags": [
          "person"
        ],
        "content": "#person",
        "size": 17.714285714285715,
        "index": 130,
        "x": -1990.652548025034,
        "y": 404.6802552639591,
        "vy": -0.4507815928468619,
        "vx": 0.2575610744521918
      },
      "index": 330
    },
    {
      "source": {
        "id": "Iterated Decomposition - Improving Science Q&A by Supervising Reasoning Processes",
        "title": "Iterated Decomposition - Improving Science Q&A by Supervising Reasoning Processes",
        "tags": [
          "project"
        ],
        "content": "#project \n\nhttps://arxiv.org/pdf/2301.01751\nhttps://github.com/oughtinc/ice\n\nA human-in-the-loop workflow for breaking down complex question-answering tasks into smaller, interpretable steps for language models. The approach involves systematically decomposing complex tasks, evaluating intermediate results, diagnosing failures, and refining the decomposition through multiple iterations, supported by a visualization tool called ICE.",
        "size": 20,
        "index": 236,
        "x": -2081.8814606953724,
        "y": 401.41473238306327,
        "vy": -0.3344104152334175,
        "vx": 0.2216074759604618
      },
      "target": {
        "id": "Charlie George",
        "title": "Charlie George",
        "tags": [
          "person"
        ],
        "content": "#person",
        "size": 17.714285714285715,
        "index": 412,
        "x": -1942.725667713989,
        "y": 384.2379487239744,
        "vy": -0.16115006137629795,
        "vx": 0.22652937902231374
      },
      "index": 331
    },
    {
      "source": {
        "id": "Iterated Decomposition - Improving Science Q&A by Supervising Reasoning Processes",
        "title": "Iterated Decomposition - Improving Science Q&A by Supervising Reasoning Processes",
        "tags": [
          "project"
        ],
        "content": "#project \n\nhttps://arxiv.org/pdf/2301.01751\nhttps://github.com/oughtinc/ice\n\nA human-in-the-loop workflow for breaking down complex question-answering tasks into smaller, interpretable steps for language models. The approach involves systematically decomposing complex tasks, evaluating intermediate results, diagnosing failures, and refining the decomposition through multiple iterations, supported by a visualization tool called ICE.",
        "size": 20,
        "index": 236,
        "x": -2081.8814606953724,
        "y": 401.41473238306327,
        "vy": -0.3344104152334175,
        "vx": 0.2216074759604618
      },
      "target": {
        "id": "Luke Stebbing",
        "title": "Luke Stebbing",
        "tags": [
          "person"
        ],
        "content": "#person \n\nhttps://lukestebbing.com/",
        "size": 17.714285714285715,
        "index": 323,
        "x": -1992.994109604841,
        "y": 471.5976775812864,
        "vy": -0.4326831697641754,
        "vx": -0.058212480520888205
      },
      "index": 332
    },
    {
      "source": {
        "id": "Iterated Decomposition - Improving Science Q&A by Supervising Reasoning Processes",
        "title": "Iterated Decomposition - Improving Science Q&A by Supervising Reasoning Processes",
        "tags": [
          "project"
        ],
        "content": "#project \n\nhttps://arxiv.org/pdf/2301.01751\nhttps://github.com/oughtinc/ice\n\nA human-in-the-loop workflow for breaking down complex question-answering tasks into smaller, interpretable steps for language models. The approach involves systematically decomposing complex tasks, evaluating intermediate results, diagnosing failures, and refining the decomposition through multiple iterations, supported by a visualization tool called ICE.",
        "size": 20,
        "index": 236,
        "x": -2081.8814606953724,
        "y": 401.41473238306327,
        "vy": -0.3344104152334175,
        "vx": 0.2216074759604618
      },
      "target": {
        "id": "Jungwon Byun",
        "title": "Jungwon Byun",
        "tags": [
          "person"
        ],
        "content": "#person",
        "size": 17.714285714285715,
        "index": 392,
        "x": -1943.1560321890236,
        "y": 443.664957671305,
        "vy": -0.16095307487519664,
        "vx": 0.253426240202621
      },
      "index": 333
    },
    {
      "source": {
        "id": "Iterated Decomposition - Improving Science Q&A by Supervising Reasoning Processes",
        "title": "Iterated Decomposition - Improving Science Q&A by Supervising Reasoning Processes",
        "tags": [
          "project"
        ],
        "content": "#project \n\nhttps://arxiv.org/pdf/2301.01751\nhttps://github.com/oughtinc/ice\n\nA human-in-the-loop workflow for breaking down complex question-answering tasks into smaller, interpretable steps for language models. The approach involves systematically decomposing complex tasks, evaluating intermediate results, diagnosing failures, and refining the decomposition through multiple iterations, supported by a visualization tool called ICE.",
        "size": 20,
        "index": 236,
        "x": -2081.8814606953724,
        "y": 401.41473238306327,
        "vy": -0.3344104152334175,
        "vx": 0.2216074759604618
      },
      "target": {
        "id": "Maggie Appleton",
        "title": "Maggie Appleton",
        "tags": [
          "person"
        ],
        "content": "#person \n\nhttps://maggieappleton.com/",
        "size": 17.714285714285715,
        "index": 13,
        "x": -1986.0378873394982,
        "y": 345.1972131135029,
        "vy": -0.20291033946933473,
        "vx": 0.19098062665201015
      },
      "index": 334
    },
    {
      "source": {
        "id": "Iterated Decomposition - Improving Science Q&A by Supervising Reasoning Processes",
        "title": "Iterated Decomposition - Improving Science Q&A by Supervising Reasoning Processes",
        "tags": [
          "project"
        ],
        "content": "#project \n\nhttps://arxiv.org/pdf/2301.01751\nhttps://github.com/oughtinc/ice\n\nA human-in-the-loop workflow for breaking down complex question-answering tasks into smaller, interpretable steps for language models. The approach involves systematically decomposing complex tasks, evaluating intermediate results, diagnosing failures, and refining the decomposition through multiple iterations, supported by a visualization tool called ICE.",
        "size": 20,
        "index": 236,
        "x": -2081.8814606953724,
        "y": 401.41473238306327,
        "vy": -0.3344104152334175,
        "vx": 0.2216074759604618
      },
      "target": {
        "id": "Andreas Stuhlmüller",
        "title": "Andreas Stuhlmüller",
        "tags": [
          "person"
        ],
        "content": "#person \n\nhttps://stuhlmueller.org/",
        "size": 17.714285714285715,
        "index": 223,
        "x": -2027.6511164368258,
        "y": 499.3550973775167,
        "vy": -0.1844689960451033,
        "vx": 0.38887876049926895
      },
      "index": 335
    },
    {
      "source": {
        "id": "Deceptive Explanations by Large Language Models Lead People to Change their Beliefs About Misinformation More Often than Honest Explanations",
        "title": "Deceptive Explanations by Large Language Models Lead People to Change their Beliefs About Misinformation More Often than Honest Explanations",
        "tags": [
          "project"
        ],
        "content": "#project \n\nhttps://dl.acm.org/doi/pdf/10.1145/3706598.3713408\n\nResearch study examines how AI-generated explanations can be used to combat misinformation more effectively than simple false classifications alone. The researchers conducted an experiment with 589 participants who rated the truthfulness of news headlines before and after receiving AI feedback, finding that deceptive explanations were significantly more persuasive than both honest explanations and deceptive classifications without explanations.",
        "size": 18.285714285714285,
        "index": 237,
        "x": 1134.8172370273448,
        "y": -2658.7202582472933,
        "vy": -1.1013096145605086,
        "vx": -0.5513364721329652
      },
      "target": {
        "id": "Valdemar Danry",
        "title": "Valdemar Danry",
        "tags": [
          "person"
        ],
        "content": "#person\n\nhttps://valdemardanry.com/",
        "size": 18.857142857142858,
        "index": 172,
        "x": 1102.4705595757653,
        "y": -2731.308366831243,
        "vy": -1.122899896351339,
        "vx": -0.5144777169524257
      },
      "index": 336
    },
    {
      "source": {
        "id": "Deceptive Explanations by Large Language Models Lead People to Change their Beliefs About Misinformation More Often than Honest Explanations",
        "title": "Deceptive Explanations by Large Language Models Lead People to Change their Beliefs About Misinformation More Often than Honest Explanations",
        "tags": [
          "project"
        ],
        "content": "#project \n\nhttps://dl.acm.org/doi/pdf/10.1145/3706598.3713408\n\nResearch study examines how AI-generated explanations can be used to combat misinformation more effectively than simple false classifications alone. The researchers conducted an experiment with 589 participants who rated the truthfulness of news headlines before and after receiving AI feedback, finding that deceptive explanations were significantly more persuasive than both honest explanations and deceptive classifications without explanations.",
        "size": 18.285714285714285,
        "index": 237,
        "x": 1134.8172370273448,
        "y": -2658.7202582472933,
        "vy": -1.1013096145605086,
        "vx": -0.5513364721329652
      },
      "target": {
        "id": "Pat Pataranutaporn",
        "title": "Pat Pataranutaporn",
        "tags": [
          "person"
        ],
        "content": "#person \n\nhttps://patpat.world/",
        "size": 18.285714285714285,
        "index": 298,
        "x": 1066.2378443172674,
        "y": -2682.567814798428,
        "vy": -1.1214877655381226,
        "vx": -0.5139886416200151
      },
      "index": 337
    },
    {
      "source": {
        "id": "Deceptive Explanations by Large Language Models Lead People to Change their Beliefs About Misinformation More Often than Honest Explanations",
        "title": "Deceptive Explanations by Large Language Models Lead People to Change their Beliefs About Misinformation More Often than Honest Explanations",
        "tags": [
          "project"
        ],
        "content": "#project \n\nhttps://dl.acm.org/doi/pdf/10.1145/3706598.3713408\n\nResearch study examines how AI-generated explanations can be used to combat misinformation more effectively than simple false classifications alone. The researchers conducted an experiment with 589 participants who rated the truthfulness of news headlines before and after receiving AI feedback, finding that deceptive explanations were significantly more persuasive than both honest explanations and deceptive classifications without explanations.",
        "size": 18.285714285714285,
        "index": 237,
        "x": 1134.8172370273448,
        "y": -2658.7202582472933,
        "vy": -1.1013096145605086,
        "vx": -0.5513364721329652
      },
      "target": {
        "id": "Matthew Groh",
        "title": "Matthew Groh",
        "tags": [
          "person"
        ],
        "content": "#person",
        "size": 16.571428571428573,
        "index": 213,
        "x": 1156.8936631841618,
        "y": -2589.6255755514535,
        "vy": -1.0855980594789738,
        "vx": -0.5959854084342755
      },
      "index": 338
    },
    {
      "source": {
        "id": "Deceptive Explanations by Large Language Models Lead People to Change their Beliefs About Misinformation More Often than Honest Explanations",
        "title": "Deceptive Explanations by Large Language Models Lead People to Change their Beliefs About Misinformation More Often than Honest Explanations",
        "tags": [
          "project"
        ],
        "content": "#project \n\nhttps://dl.acm.org/doi/pdf/10.1145/3706598.3713408\n\nResearch study examines how AI-generated explanations can be used to combat misinformation more effectively than simple false classifications alone. The researchers conducted an experiment with 589 participants who rated the truthfulness of news headlines before and after receiving AI feedback, finding that deceptive explanations were significantly more persuasive than both honest explanations and deceptive classifications without explanations.",
        "size": 18.285714285714285,
        "index": 237,
        "x": 1134.8172370273448,
        "y": -2658.7202582472933,
        "vy": -1.1013096145605086,
        "vx": -0.5513364721329652
      },
      "target": {
        "id": "Ziv Epstein",
        "title": "Ziv Epstein",
        "tags": [
          "person"
        ],
        "content": "#person",
        "size": 16.571428571428573,
        "index": 78,
        "x": 1207.1550182508104,
        "y": -2638.0944084472067,
        "vy": -1.0541703208135271,
        "vx": -0.5819113102666126
      },
      "index": 339
    },
    {
      "source": {
        "id": "Peter Mühlbacher",
        "title": "Peter Mühlbacher",
        "tags": [
          "person"
        ],
        "content": "#person \n\nhttp://peter.muehlbacher.me/",
        "size": 18.285714285714285,
        "index": 238,
        "x": -857.2972832702691,
        "y": -654.0740118249536,
        "vy": -0.3400430834320815,
        "vx": -0.3453910216674885
      },
      "target": {
        "id": "Future Search",
        "title": "Future Search",
        "tags": [
          "organization"
        ],
        "content": "#organization \n\nhttps://futuresearch.ai/",
        "size": 22.285714285714285,
        "index": 94,
        "x": -717.8526553292105,
        "y": -719.7276361443852,
        "vy": -0.4404383940953807,
        "vx": -0.3824839533244458
      },
      "index": 340
    },
    {
      "source": {
        "id": "Lawrence Phillips",
        "title": "Lawrence Phillips",
        "tags": [
          "person"
        ],
        "content": "#person",
        "size": 18.285714285714285,
        "index": 239,
        "x": -786.997452942158,
        "y": -671.6640862022364,
        "vy": -0.3955253133044403,
        "vx": -0.3629004531354729
      },
      "target": {
        "id": "Future Search",
        "title": "Future Search",
        "tags": [
          "organization"
        ],
        "content": "#organization \n\nhttps://futuresearch.ai/",
        "size": 22.285714285714285,
        "index": 94,
        "x": -717.8526553292105,
        "y": -719.7276361443852,
        "vy": -0.4404383940953807,
        "vx": -0.3824839533244458
      },
      "index": 341
    },
    {
      "source": {
        "id": "Shu Yang Lin",
        "title": "Shu Yang Lin",
        "tags": [
          "person"
        ],
        "content": "#person \n\nhttps://www.shuyanglin.com/",
        "size": 18.285714285714285,
        "index": 241,
        "x": 771.7824733892007,
        "y": -32.82433362604927,
        "vy": -1.1943655944737657,
        "vx": -0.00035972909812223005
      },
      "target": {
        "id": "AI Objectives Institute",
        "title": "AI Objectives Institute",
        "tags": [
          "organization"
        ],
        "content": "#organization \n\nhttps://ai.objectives.institute/",
        "size": 20.57142857142857,
        "index": 304,
        "x": 665.7615191436854,
        "y": -247.07081557119588,
        "vy": -1.1515300453530217,
        "vx": -0.07444975689524554
      },
      "index": 342
    },
    {
      "source": {
        "id": "Shu Yang Lin",
        "title": "Shu Yang Lin",
        "tags": [
          "person"
        ],
        "content": "#person \n\nhttps://www.shuyanglin.com/",
        "size": 18.285714285714285,
        "index": 241,
        "x": 771.7824733892007,
        "y": -32.82433362604927,
        "vy": -1.1943655944737657,
        "vx": -0.00035972909812223005
      },
      "target": {
        "id": "vTaiwan & g0v",
        "title": "vTaiwan & g0v",
        "tags": [
          "organization"
        ],
        "content": "#organization \n\nhttps://info.vtaiwan.tw/\nhttps://g0v.tw/",
        "size": 18.285714285714285,
        "index": 389,
        "x": 918.308649354897,
        "y": 190.9842796861607,
        "vy": -1.2371396245854598,
        "vx": 0.056319588048500874
      },
      "index": 343
    },
    {
      "source": {
        "id": "Grim",
        "title": "Grim",
        "tags": [
          "project"
        ],
        "content": "#project \n\nhttps://github.com/SentinelTeam/grim\nhttps://www.lesswrong.com/posts/eR69f3hi5ozxchhYg/scaling-wargaming-for-global-catastrophic-risks-with-ai\n\nAI-powered wargaming tool that uses LLMs to simulate complex catastrophic scenarios and help organizations practice emergency responses. The tool functions as a Telegram bot where participants can take actions, request information, and feed data into crisis simulations, with AI serving as both forecaster and game master to create detailed, dynamic scenarios.",
        "size": 17.714285714285715,
        "index": 242,
        "x": -1135.8393199919878,
        "y": -1478.5909522257905,
        "vy": -0.2986686394199088,
        "vx": -0.6542902351116809
      },
      "target": {
        "id": "Rai Sur",
        "title": "Rai Sur",
        "tags": [
          "person"
        ],
        "content": "#person \n\nhttps://rai.dev",
        "size": 17.142857142857142,
        "index": 379,
        "x": -1136.882572758444,
        "y": -1562.3371179774033,
        "vy": -0.5651901195125634,
        "vx": -0.6815882967403171
      },
      "index": 344
    },
    {
      "source": {
        "id": "Grim",
        "title": "Grim",
        "tags": [
          "project"
        ],
        "content": "#project \n\nhttps://github.com/SentinelTeam/grim\nhttps://www.lesswrong.com/posts/eR69f3hi5ozxchhYg/scaling-wargaming-for-global-catastrophic-risks-with-ai\n\nAI-powered wargaming tool that uses LLMs to simulate complex catastrophic scenarios and help organizations practice emergency responses. The tool functions as a Telegram bot where participants can take actions, request information, and feed data into crisis simulations, with AI serving as both forecaster and game master to create detailed, dynamic scenarios.",
        "size": 17.714285714285715,
        "index": 242,
        "x": -1135.8393199919878,
        "y": -1478.5909522257905,
        "vy": -0.2986686394199088,
        "vx": -0.6542902351116809
      },
      "target": {
        "id": "Nuño Sempere",
        "title": "Nuño Sempere",
        "tags": [
          "person"
        ],
        "content": "#person \n\nhttps://nunosempere.com/",
        "size": 17.714285714285715,
        "index": 127,
        "x": -1036.424048922515,
        "y": -1428.1816687468543,
        "vy": -0.23699158830523448,
        "vx": -0.649314981499335
      },
      "index": 345
    },
    {
      "source": {
        "id": "Nikos Bosse",
        "title": "Nikos Bosse",
        "tags": [
          "person"
        ],
        "content": "#person \n\nhttps://followtheargument.org/",
        "size": 18.285714285714285,
        "index": 243,
        "x": -843.047489587698,
        "y": -605.5760820500159,
        "vy": -0.3391263569445478,
        "vx": -0.31376832368339486
      },
      "target": {
        "id": "Future Search",
        "title": "Future Search",
        "tags": [
          "organization"
        ],
        "content": "#organization \n\nhttps://futuresearch.ai/",
        "size": 22.285714285714285,
        "index": 94,
        "x": -717.8526553292105,
        "y": -719.7276361443852,
        "vy": -0.4404383940953807,
        "vx": -0.3824839533244458
      },
      "index": 346
    },
    {
      "source": {
        "id": "Jacquelyn Schneider",
        "title": "Jacquelyn Schneider",
        "tags": [
          "person"
        ],
        "content": "#person",
        "size": 17.714285714285715,
        "index": 248,
        "x": -2179.5691824765895,
        "y": -1455.2408031463438,
        "vy": -0.0785190756515778,
        "vx": -0.3687066288969788
      },
      "target": {
        "id": "Hoover Wargaming and Crisis Simulation Initiative",
        "title": "Hoover Wargaming and Crisis Simulation Initiative",
        "tags": [
          "organization"
        ],
        "content": "#organization \n\nhttps://www.hoover.org/research-teams/wargaming-and-crisis-simulation-initiative",
        "size": 17.142857142857142,
        "index": 322,
        "x": -2247.325695112165,
        "y": -1498.6468612151302,
        "vy": -0.027864307415909017,
        "vx": -0.2866046043686715
      },
      "index": 347
    },
    {
      "source": {
        "id": "Jamie Joyce",
        "title": "Jamie Joyce",
        "tags": [
          "person"
        ],
        "content": "#person \n\nhttps://www.jamiejoyce.com/",
        "size": 16.571428571428573,
        "index": 250,
        "x": -130.36663231613733,
        "y": -1067.9827765464338,
        "vy": -0.7845208059320311,
        "vx": -0.4195287531556206
      },
      "target": {
        "id": "The Society Library",
        "title": "The Society Library",
        "tags": [
          "organization"
        ],
        "content": "#organization \n\nhttps://societylibrary.medium.com/\nhttps://www.societylibrary.org/",
        "size": 18.285714285714285,
        "index": 274,
        "x": -123.51596440526208,
        "y": -972.6539885865532,
        "vy": -0.7846889723379666,
        "vx": -0.37290689478790895
      },
      "index": 348
    },
    {
      "source": {
        "id": "$SL Muses",
        "title": "$SL Muses",
        "tags": [
          "project"
        ],
        "content": "#project \nhttps://www.sltoken.xyz/\n\nAI agents called \"Muses\" that users can tag on social media platforms to perform reasoning tasks like fact-checking claims and researching topics. Their \"Muse of Truth\" provides substantive analysis of factual claims with evidence gaps and source references, while the \"Muse of Research\" pulls resources on requested topics.\n\nThe initiative was funded through a community-created meme token ($SL) that was anonymously gifted to the charity, with 50% of holdings donated to support their mission.",
        "size": 16.571428571428573,
        "index": 253,
        "x": -76.37214047101332,
        "y": -1049.2771759946258,
        "vy": -0.8214737063813725,
        "vx": -0.4067158365132719
      },
      "target": {
        "id": "The Society Library",
        "title": "The Society Library",
        "tags": [
          "organization"
        ],
        "content": "#organization \n\nhttps://societylibrary.medium.com/\nhttps://www.societylibrary.org/",
        "size": 18.285714285714285,
        "index": 274,
        "x": -123.51596440526208,
        "y": -972.6539885865532,
        "vy": -0.7846889723379666,
        "vx": -0.37290689478790895
      },
      "index": 349
    },
    {
      "source": {
        "id": "Lukas Finnveden",
        "title": "Lukas Finnveden",
        "tags": [
          "person"
        ],
        "content": "#person",
        "size": 18.285714285714285,
        "index": 256,
        "x": -1770.6095471676322,
        "y": 430.1205037586758,
        "vy": -0.22586013414071307,
        "vx": 0.1963978919880959
      },
      "target": {
        "id": "Forethought",
        "title": "Forethought",
        "tags": [
          "organization"
        ],
        "content": "#organization \n\nhttps://www.forethought.org/",
        "size": 17.714285714285715,
        "index": 202,
        "x": -1771.7035542392243,
        "y": 317.48703027353775,
        "vy": -0.1877889855715367,
        "vx": 0.2317876718704255
      },
      "index": 350
    },
    {
      "source": {
        "id": "Lukas Finnveden",
        "title": "Lukas Finnveden",
        "tags": [
          "person"
        ],
        "content": "#person",
        "size": 18.285714285714285,
        "index": 256,
        "x": -1770.6095471676322,
        "y": 430.1205037586758,
        "vy": -0.22586013414071307,
        "vx": 0.1963978919880959
      },
      "target": {
        "id": "Ought",
        "title": "Ought",
        "tags": [
          "organization"
        ],
        "content": "#organization \n\nhttps://ought.org/ (website is old)\n\nOught 1.0 has effectively become what is now Elicit (the company) continuing development on Elicit (the tool). Ought 2.0 has not yet done any public work.",
        "size": 23.42857142857143,
        "index": 306,
        "x": -1888.3722954977052,
        "y": 453.0511056583274,
        "vy": -0.2547035865836596,
        "vx": 0.0471838457037637
      },
      "index": 351
    },
    {
      "source": {
        "id": "Israel-Palestine Collective Dialogues",
        "title": "Israel-Palestine Collective Dialogues",
        "tags": [
          "project"
        ],
        "content": "#project \n\nhttps://arxiv.org/abs/2503.01769\n\nPaper describes a case study of using AI and collective dialogue methods to help Israeli and Palestinian peacebuilders find common ground during the conflict following October 7, 2023. The research integrates LLMs, bridging-based ranking algorithms, and online collective dialogues to identify shared perspectives across divided groups.",
        "size": 20,
        "index": 260,
        "x": 1513.2149875917946,
        "y": 692.5559634486403,
        "vy": -1.399900668332369,
        "vx": 0.23953875612992598
      },
      "target": {
        "id": "Andrew Konya",
        "title": "Andrew Konya",
        "tags": [
          "person"
        ],
        "content": "#person \n\nhttps://andrewkonya.com/",
        "size": 18.857142857142858,
        "index": 265,
        "x": 1563.7748126336485,
        "y": 791.1411106203664,
        "vy": -1.4006415470721048,
        "vx": 0.2425628664665928
      },
      "index": 352
    },
    {
      "source": {
        "id": "Israel-Palestine Collective Dialogues",
        "title": "Israel-Palestine Collective Dialogues",
        "tags": [
          "project"
        ],
        "content": "#project \n\nhttps://arxiv.org/abs/2503.01769\n\nPaper describes a case study of using AI and collective dialogue methods to help Israeli and Palestinian peacebuilders find common ground during the conflict following October 7, 2023. The research integrates LLMs, bridging-based ranking algorithms, and online collective dialogues to identify shared perspectives across divided groups.",
        "size": 20,
        "index": 260,
        "x": 1513.2149875917946,
        "y": 692.5559634486403,
        "vy": -1.399900668332369,
        "vx": 0.23953875612992598
      },
      "target": {
        "id": "Luke Thorburn",
        "title": "Luke Thorburn",
        "tags": [
          "person"
        ],
        "content": "#person",
        "size": 17.142857142857142,
        "index": 49,
        "x": 1650.4746137334355,
        "y": 709.4227495487538,
        "vy": -1.4702697462835792,
        "vx": 0.2532103757321741
      },
      "index": 353
    },
    {
      "source": {
        "id": "Israel-Palestine Collective Dialogues",
        "title": "Israel-Palestine Collective Dialogues",
        "tags": [
          "project"
        ],
        "content": "#project \n\nhttps://arxiv.org/abs/2503.01769\n\nPaper describes a case study of using AI and collective dialogue methods to help Israeli and Palestinian peacebuilders find common ground during the conflict following October 7, 2023. The research integrates LLMs, bridging-based ranking algorithms, and online collective dialogues to identify shared perspectives across divided groups.",
        "size": 20,
        "index": 260,
        "x": 1513.2149875917946,
        "y": 692.5559634486403,
        "vy": -1.399900668332369,
        "vx": 0.23953875612992598
      },
      "target": {
        "id": "Wasim Almasri",
        "title": "Wasim Almasri",
        "tags": [
          "person"
        ],
        "content": "#person",
        "size": 16.571428571428573,
        "index": 83,
        "x": 1602.9627753407951,
        "y": 618.6943993536536,
        "vy": -1.4467901264144611,
        "vx": 0.19543735789529917
      },
      "index": 354
    },
    {
      "source": {
        "id": "Israel-Palestine Collective Dialogues",
        "title": "Israel-Palestine Collective Dialogues",
        "tags": [
          "project"
        ],
        "content": "#project \n\nhttps://arxiv.org/abs/2503.01769\n\nPaper describes a case study of using AI and collective dialogue methods to help Israeli and Palestinian peacebuilders find common ground during the conflict following October 7, 2023. The research integrates LLMs, bridging-based ranking algorithms, and online collective dialogues to identify shared perspectives across divided groups.",
        "size": 20,
        "index": 260,
        "x": 1513.2149875917946,
        "y": 692.5559634486403,
        "vy": -1.399900668332369,
        "vx": 0.23953875612992598
      },
      "target": {
        "id": "Oded Adomi Leshem",
        "title": "Oded Adomi Leshem",
        "tags": [
          "person"
        ],
        "content": "#person",
        "size": 16.571428571428573,
        "index": 375,
        "x": 1648.5107050980628,
        "y": 653.2007948666912,
        "vy": -1.468862920900976,
        "vx": 0.21216833446805924
      },
      "index": 355
    },
    {
      "source": {
        "id": "Israel-Palestine Collective Dialogues",
        "title": "Israel-Palestine Collective Dialogues",
        "tags": [
          "project"
        ],
        "content": "#project \n\nhttps://arxiv.org/abs/2503.01769\n\nPaper describes a case study of using AI and collective dialogue methods to help Israeli and Palestinian peacebuilders find common ground during the conflict following October 7, 2023. The research integrates LLMs, bridging-based ranking algorithms, and online collective dialogues to identify shared perspectives across divided groups.",
        "size": 20,
        "index": 260,
        "x": 1513.2149875917946,
        "y": 692.5559634486403,
        "vy": -1.399900668332369,
        "vx": 0.23953875612992598
      },
      "target": {
        "id": "Ariel Procaccia",
        "title": "Ariel Procaccia",
        "tags": [
          "person"
        ],
        "content": "#person \n\nhttps://procaccia.info/",
        "size": 17.142857142857142,
        "index": 30,
        "x": 1767.4207500457344,
        "y": 495.40167289038965,
        "vy": -1.5944030237308504,
        "vx": 0.18021375498871298
      },
      "index": 356
    },
    {
      "source": {
        "id": "Israel-Palestine Collective Dialogues",
        "title": "Israel-Palestine Collective Dialogues",
        "tags": [
          "project"
        ],
        "content": "#project \n\nhttps://arxiv.org/abs/2503.01769\n\nPaper describes a case study of using AI and collective dialogue methods to help Israeli and Palestinian peacebuilders find common ground during the conflict following October 7, 2023. The research integrates LLMs, bridging-based ranking algorithms, and online collective dialogues to identify shared perspectives across divided groups.",
        "size": 20,
        "index": 260,
        "x": 1513.2149875917946,
        "y": 692.5559634486403,
        "vy": -1.399900668332369,
        "vx": 0.23953875612992598
      },
      "target": {
        "id": "Lisa Schirch",
        "title": "Lisa Schirch",
        "tags": [
          "person"
        ],
        "content": "#person \n\nhttps://kroc.nd.edu/faculty-and-staff/lisa-schirch/",
        "size": 18.285714285714285,
        "index": 291,
        "x": 1561.488577698378,
        "y": 890.5844108032592,
        "vy": -1.453577080273647,
        "vx": 0.2825364996104594
      },
      "index": 357
    },
    {
      "source": {
        "id": "Israel-Palestine Collective Dialogues",
        "title": "Israel-Palestine Collective Dialogues",
        "tags": [
          "project"
        ],
        "content": "#project \n\nhttps://arxiv.org/abs/2503.01769\n\nPaper describes a case study of using AI and collective dialogue methods to help Israeli and Palestinian peacebuilders find common ground during the conflict following October 7, 2023. The research integrates LLMs, bridging-based ranking algorithms, and online collective dialogues to identify shared perspectives across divided groups.",
        "size": 20,
        "index": 260,
        "x": 1513.2149875917946,
        "y": 692.5559634486403,
        "vy": -1.399900668332369,
        "vx": 0.23953875612992598
      },
      "target": {
        "id": "Michiel Bakker",
        "title": "Michiel Bakker",
        "tags": [
          "person"
        ],
        "content": "#person \n\nhttps://miba.dev/",
        "size": 21.142857142857142,
        "index": 284,
        "x": 1225.501866244462,
        "y": 730.5079873536822,
        "vy": -1.2714527484014098,
        "vx": 0.27625785273055353
      },
      "index": 358
    },
    {
      "source": {
        "id": "Opportunities and Risks of LLMs for Scalable Deliberation with Polis",
        "title": "Opportunities and Risks of LLMs for Scalable Deliberation with Polis",
        "tags": [
          "project"
        ],
        "content": "#project",
        "size": 21.142857142857142,
        "index": 262,
        "x": 2030.0768750921764,
        "y": 689.5426397389598,
        "vy": -1.5826461117082526,
        "vx": 0.38019529976898647
      },
      "target": {
        "id": "Christopher Small",
        "title": "Christopher Small",
        "tags": [
          "person"
        ],
        "content": "#person \n\nhttp://metasoarous.com/",
        "size": 18.285714285714285,
        "index": 290,
        "x": 1854.4692018669803,
        "y": 861.0514458806144,
        "vy": -1.5207937741217539,
        "vx": 0.3624142296931689
      },
      "index": 359
    },
    {
      "source": {
        "id": "Opportunities and Risks of LLMs for Scalable Deliberation with Polis",
        "title": "Opportunities and Risks of LLMs for Scalable Deliberation with Polis",
        "tags": [
          "project"
        ],
        "content": "#project",
        "size": 21.142857142857142,
        "index": 262,
        "x": 2030.0768750921764,
        "y": 689.5426397389598,
        "vy": -1.5826461117082526,
        "vx": 0.38019529976898647
      },
      "target": {
        "id": "Ivan Vendrov",
        "title": "Ivan Vendrov",
        "tags": [
          "person"
        ],
        "content": "#person \n\nhttps://www.vendrov.ai/",
        "size": 17.714285714285715,
        "index": 215,
        "x": 1810.5588734061498,
        "y": 305.9910348849648,
        "vy": -1.443643107030699,
        "vx": 0.22600812026212258
      },
      "index": 360
    },
    {
      "source": {
        "id": "Opportunities and Risks of LLMs for Scalable Deliberation with Polis",
        "title": "Opportunities and Risks of LLMs for Scalable Deliberation with Polis",
        "tags": [
          "project"
        ],
        "content": "#project",
        "size": 21.142857142857142,
        "index": 262,
        "x": 2030.0768750921764,
        "y": 689.5426397389598,
        "vy": -1.5826461117082526,
        "vx": 0.38019529976898647
      },
      "target": {
        "id": "Esin Durmus",
        "title": "Esin Durmus",
        "tags": [
          "person"
        ],
        "content": "#person \n\nhttps://esdurmus.github.io/",
        "size": 16.571428571428573,
        "index": 170,
        "x": 2111.255777757817,
        "y": 643.7713226273537,
        "vy": -1.6471404601520836,
        "vx": 0.3574954565359457
      },
      "index": 361
    },
    {
      "source": {
        "id": "Opportunities and Risks of LLMs for Scalable Deliberation with Polis",
        "title": "Opportunities and Risks of LLMs for Scalable Deliberation with Polis",
        "tags": [
          "project"
        ],
        "content": "#project",
        "size": 21.142857142857142,
        "index": 262,
        "x": 2030.0768750921764,
        "y": 689.5426397389598,
        "vy": -1.5826461117082526,
        "vx": 0.38019529976898647
      },
      "target": {
        "id": "Hadjar Homaei",
        "title": "Hadjar Homaei",
        "tags": [
          "person"
        ],
        "content": "#person",
        "size": 17.142857142857142,
        "index": 119,
        "x": 2078.3315161452247,
        "y": 736.3969386386419,
        "vy": -1.652546889176861,
        "vx": 0.47318333071124574
      },
      "index": 362
    },
    {
      "source": {
        "id": "Opportunities and Risks of LLMs for Scalable Deliberation with Polis",
        "title": "Opportunities and Risks of LLMs for Scalable Deliberation with Polis",
        "tags": [
          "project"
        ],
        "content": "#project",
        "size": 21.142857142857142,
        "index": 262,
        "x": 2030.0768750921764,
        "y": 689.5426397389598,
        "vy": -1.5826461117082526,
        "vx": 0.38019529976898647
      },
      "target": {
        "id": "Elizabeth Barry",
        "title": "Elizabeth Barry",
        "tags": [
          "person"
        ],
        "content": "#person",
        "size": 17.142857142857142,
        "index": 296,
        "x": 2038.8526036228927,
        "y": 782.8111371581372,
        "vy": -1.7075617448758047,
        "vx": 0.3940459596078392
      },
      "index": 363
    },
    {
      "source": {
        "id": "Opportunities and Risks of LLMs for Scalable Deliberation with Polis",
        "title": "Opportunities and Risks of LLMs for Scalable Deliberation with Polis",
        "tags": [
          "project"
        ],
        "content": "#project",
        "size": 21.142857142857142,
        "index": 262,
        "x": 2030.0768750921764,
        "y": 689.5426397389598,
        "vy": -1.5826461117082526,
        "vx": 0.38019529976898647
      },
      "target": {
        "id": "Julien Cornebise",
        "title": "Julien Cornebise",
        "tags": [
          "person"
        ],
        "content": "#person",
        "size": 17.142857142857142,
        "index": 56,
        "x": 2133.7538366244335,
        "y": 741.7209256089054,
        "vy": -1.5606596859274167,
        "vx": 0.4799794532457997
      },
      "index": 364
    },
    {
      "source": {
        "id": "Opportunities and Risks of LLMs for Scalable Deliberation with Polis",
        "title": "Opportunities and Risks of LLMs for Scalable Deliberation with Polis",
        "tags": [
          "project"
        ],
        "content": "#project",
        "size": 21.142857142857142,
        "index": 262,
        "x": 2030.0768750921764,
        "y": 689.5426397389598,
        "vy": -1.5826461117082526,
        "vx": 0.38019529976898647
      },
      "target": {
        "id": "Ted Suzman",
        "title": "Ted Suzman",
        "tags": [
          "person"
        ],
        "content": "#person",
        "size": 17.142857142857142,
        "index": 350,
        "x": 2095.8625924779726,
        "y": 794.9387059636165,
        "vy": -1.6339108176425123,
        "vx": 0.3784470371848435
      },
      "index": 365
    },
    {
      "source": {
        "id": "Opportunities and Risks of LLMs for Scalable Deliberation with Polis",
        "title": "Opportunities and Risks of LLMs for Scalable Deliberation with Polis",
        "tags": [
          "project"
        ],
        "content": "#project",
        "size": 21.142857142857142,
        "index": 262,
        "x": 2030.0768750921764,
        "y": 689.5426397389598,
        "vy": -1.5826461117082526,
        "vx": 0.38019529976898647
      },
      "target": {
        "id": "Deep Ganguli",
        "title": "Deep Ganguli",
        "tags": [
          "person"
        ],
        "content": "#person \n\nhttps://dganguli.github.io/pweb/",
        "size": 16.571428571428573,
        "index": 251,
        "x": 2147.370190950839,
        "y": 688.0551231311405,
        "vy": -1.6539087982061307,
        "vx": 0.3657999195621096
      },
      "index": 366
    },
    {
      "source": {
        "id": "Opportunities and Risks of LLMs for Scalable Deliberation with Polis",
        "title": "Opportunities and Risks of LLMs for Scalable Deliberation with Polis",
        "tags": [
          "project"
        ],
        "content": "#project",
        "size": 21.142857142857142,
        "index": 262,
        "x": 2030.0768750921764,
        "y": 689.5426397389598,
        "vy": -1.5826461117082526,
        "vx": 0.38019529976898647
      },
      "target": {
        "id": "Colin Megill",
        "title": "Colin Megill",
        "tags": [
          "person"
        ],
        "content": "#person \n\nhttps://colinmegill.com/",
        "size": 17.142857142857142,
        "index": 220,
        "x": 2149.182032920351,
        "y": 798.9739724169445,
        "vy": -1.5233448647940664,
        "vx": 0.3617601303910128
      },
      "index": 367
    },
    {
      "source": {
        "id": "Andrew Konya",
        "title": "Andrew Konya",
        "tags": [
          "person"
        ],
        "content": "#person \n\nhttps://andrewkonya.com/",
        "size": 18.857142857142858,
        "index": 265,
        "x": 1563.7748126336485,
        "y": 791.1411106203664,
        "vy": -1.4006415470721048,
        "vx": 0.2425628664665928
      },
      "target": {
        "id": "Remesh",
        "title": "Remesh",
        "tags": [
          "organization"
        ],
        "content": "#organization \n\nhttps://www.remesh.ai",
        "size": 17.142857142857142,
        "index": 6,
        "x": 1434.9197807893906,
        "y": 780.4664062408266,
        "vy": -1.3892800306989497,
        "vx": 0.2759787817962076
      },
      "index": 368
    },
    {
      "source": {
        "id": "Andrew Konya",
        "title": "Andrew Konya",
        "tags": [
          "person"
        ],
        "content": "#person \n\nhttps://andrewkonya.com/",
        "size": 18.857142857142858,
        "index": 265,
        "x": 1563.7748126336485,
        "y": 791.1411106203664,
        "vy": -1.4006415470721048,
        "vx": 0.2425628664665928
      },
      "target": {
        "id": "AI & Democracy Foundation",
        "title": "AI & Democracy Foundation",
        "tags": [
          "organization"
        ],
        "content": "#organization \n\nhttps://aidemocracyfoundation.org/",
        "size": 17.714285714285715,
        "index": 432,
        "x": 1666.8768486198028,
        "y": 789.5155954205912,
        "vy": -1.4702217936721376,
        "vx": 0.26566951389858834
      },
      "index": 369
    },
    {
      "source": {
        "id": "Joe Edelman",
        "title": "Joe Edelman",
        "tags": [
          "person"
        ],
        "content": "#person \n\nhttps://nxhx.org/",
        "size": 17.714285714285715,
        "index": 268,
        "x": 1356.6939030945252,
        "y": -196.06959723472013,
        "vy": -1.3527938545248293,
        "vx": -0.025787874230328266
      },
      "target": {
        "id": "Meaning Alignment Institute",
        "title": "Meaning Alignment Institute",
        "tags": [
          "organization"
        ],
        "content": "#organization \n\nhttps://www.meaningalignment.org/",
        "size": 17.714285714285715,
        "index": 310,
        "x": 1258.0435242093358,
        "y": -308.1817574200967,
        "vy": -1.311900518523638,
        "vx": -0.08522925964609504
      },
      "index": 370
    },
    {
      "source": {
        "id": "Jacob Ganz",
        "title": "Jacob Ganz",
        "tags": [
          "person"
        ],
        "content": "#person",
        "size": 17.142857142857142,
        "index": 271,
        "x": -2270.1138391784716,
        "y": -1434.05530323859,
        "vy": -0.07937823843324772,
        "vx": -0.3726164155159207
      },
      "target": {
        "id": "Hoover Wargaming and Crisis Simulation Initiative",
        "title": "Hoover Wargaming and Crisis Simulation Initiative",
        "tags": [
          "organization"
        ],
        "content": "#organization \n\nhttps://www.hoover.org/research-teams/wargaming-and-crisis-simulation-initiative",
        "size": 17.142857142857142,
        "index": 322,
        "x": -2247.325695112165,
        "y": -1498.6468612151302,
        "vy": -0.027864307415909017,
        "vx": -0.2866046043686715
      },
      "index": 371
    },
    {
      "source": {
        "id": "Mantic",
        "title": "Mantic",
        "tags": [
          "organization"
        ],
        "content": "#organization \n\nhttps://mntc.ai/",
        "size": 17.714285714285715,
        "index": 275,
        "x": -472.4300783671211,
        "y": -887.3407076068335,
        "vy": -0.49223288520611863,
        "vx": -0.32327819541462854
      },
      "target": {
        "id": "AI Forecasting Benchmark",
        "title": "AI Forecasting Benchmark",
        "tags": [
          "project"
        ],
        "content": "#project \n\nhttps://www.metaculus.com/aib/\n\nMetaculus AIB (AI Benchmarking) is a series that benchmarks \"the state of the art in AI forecasting against the best humans on real-world questions.\"",
        "size": 20.57142857142857,
        "index": 178,
        "x": -491.69904780690194,
        "y": -761.3030318136879,
        "vy": -0.5357053744666,
        "vx": -0.2989940012828504
      },
      "index": 372
    },
    {
      "source": {
        "id": "Wisdom of the silicon crowd - LLM ensemble prediction capabilities rival human crowd accuracy",
        "title": "Wisdom of the silicon crowd - LLM ensemble prediction capabilities rival human crowd accuracy",
        "tags": [
          "project"
        ],
        "content": "#project \n\nhttps://www.science.org/doi/pdf/10.1126/sciadv.adp1528\n\nPaper demonstrates that an ensemble of 12 different large language models can achieve forecasting accuracy that rivals human crowd predictions in real-world forecasting tournaments. The study shows that while individual LLMs typically underperform human crowds, aggregating predictions from multiple LLMs creates a \"wisdom of the silicon crowd\" effect that matches human accuracy on 31 binary forecasting questions.",
        "size": 19.428571428571427,
        "index": 281,
        "x": -292.04779945167223,
        "y": -239.92132255230587,
        "vy": -0.7213623615049221,
        "vx": 0.039225809807466795
      },
      "target": {
        "id": "Philipp Schoenegger",
        "title": "Philipp Schoenegger",
        "tags": [
          "person"
        ],
        "content": "#person \n\nhttps://philipp-schoenegger.weebly.com/",
        "size": 19.428571428571427,
        "index": 120,
        "x": -308.0505670602049,
        "y": -355.92667066361173,
        "vy": -0.7160490223836882,
        "vx": 0.03780161064368572
      },
      "index": 373
    },
    {
      "source": {
        "id": "Wisdom of the silicon crowd - LLM ensemble prediction capabilities rival human crowd accuracy",
        "title": "Wisdom of the silicon crowd - LLM ensemble prediction capabilities rival human crowd accuracy",
        "tags": [
          "project"
        ],
        "content": "#project \n\nhttps://www.science.org/doi/pdf/10.1126/sciadv.adp1528\n\nPaper demonstrates that an ensemble of 12 different large language models can achieve forecasting accuracy that rivals human crowd predictions in real-world forecasting tournaments. The study shows that while individual LLMs typically underperform human crowds, aggregating predictions from multiple LLMs creates a \"wisdom of the silicon crowd\" effect that matches human accuracy on 31 binary forecasting questions.",
        "size": 19.428571428571427,
        "index": 281,
        "x": -292.04779945167223,
        "y": -239.92132255230587,
        "vy": -0.7213623615049221,
        "vx": 0.039225809807466795
      },
      "target": {
        "id": "Indre Tuminauskaite",
        "title": "Indre Tuminauskaite",
        "tags": [
          "person"
        ],
        "content": "#person",
        "size": 16.571428571428573,
        "index": 259,
        "x": -256.3482287166553,
        "y": -305.9958249138554,
        "vy": -0.6925489380865054,
        "vx": 0.08207477212541159
      },
      "index": 374
    },
    {
      "source": {
        "id": "Wisdom of the silicon crowd - LLM ensemble prediction capabilities rival human crowd accuracy",
        "title": "Wisdom of the silicon crowd - LLM ensemble prediction capabilities rival human crowd accuracy",
        "tags": [
          "project"
        ],
        "content": "#project \n\nhttps://www.science.org/doi/pdf/10.1126/sciadv.adp1528\n\nPaper demonstrates that an ensemble of 12 different large language models can achieve forecasting accuracy that rivals human crowd predictions in real-world forecasting tournaments. The study shows that while individual LLMs typically underperform human crowds, aggregating predictions from multiple LLMs creates a \"wisdom of the silicon crowd\" effect that matches human accuracy on 31 binary forecasting questions.",
        "size": 19.428571428571427,
        "index": 281,
        "x": -292.04779945167223,
        "y": -239.92132255230587,
        "vy": -0.7213623615049221,
        "vx": 0.039225809807466795
      },
      "target": {
        "id": "Peter Park",
        "title": "Peter Park",
        "tags": [
          "person"
        ],
        "content": "#person",
        "size": 17.142857142857142,
        "index": 276,
        "x": -226.8077158046543,
        "y": -232.4893312689628,
        "vy": -0.7153680729821785,
        "vx": 0.018395868889687337
      },
      "index": 375
    },
    {
      "source": {
        "id": "Wisdom of the silicon crowd - LLM ensemble prediction capabilities rival human crowd accuracy",
        "title": "Wisdom of the silicon crowd - LLM ensemble prediction capabilities rival human crowd accuracy",
        "tags": [
          "project"
        ],
        "content": "#project \n\nhttps://www.science.org/doi/pdf/10.1126/sciadv.adp1528\n\nPaper demonstrates that an ensemble of 12 different large language models can achieve forecasting accuracy that rivals human crowd predictions in real-world forecasting tournaments. The study shows that while individual LLMs typically underperform human crowds, aggregating predictions from multiple LLMs creates a \"wisdom of the silicon crowd\" effect that matches human accuracy on 31 binary forecasting questions.",
        "size": 19.428571428571427,
        "index": 281,
        "x": -292.04779945167223,
        "y": -239.92132255230587,
        "vy": -0.7213623615049221,
        "vx": 0.039225809807466795
      },
      "target": {
        "id": "Rafael Valdece Sousa Bastos",
        "title": "Rafael Valdece Sousa Bastos",
        "tags": [
          "person"
        ],
        "content": "#person",
        "size": 16.571428571428573,
        "index": 421,
        "x": -367.3590451040928,
        "y": -272.2935311953073,
        "vy": -0.6928685814306373,
        "vx": 0.028191872762758977
      },
      "index": 376
    },
    {
      "source": {
        "id": "Wisdom of the silicon crowd - LLM ensemble prediction capabilities rival human crowd accuracy",
        "title": "Wisdom of the silicon crowd - LLM ensemble prediction capabilities rival human crowd accuracy",
        "tags": [
          "project"
        ],
        "content": "#project \n\nhttps://www.science.org/doi/pdf/10.1126/sciadv.adp1528\n\nPaper demonstrates that an ensemble of 12 different large language models can achieve forecasting accuracy that rivals human crowd predictions in real-world forecasting tournaments. The study shows that while individual LLMs typically underperform human crowds, aggregating predictions from multiple LLMs creates a \"wisdom of the silicon crowd\" effect that matches human accuracy on 31 binary forecasting questions.",
        "size": 19.428571428571427,
        "index": 281,
        "x": -292.04779945167223,
        "y": -239.92132255230587,
        "vy": -0.7213623615049221,
        "vx": 0.039225809807466795
      },
      "target": {
        "id": "Philip Tetlock",
        "title": "Philip Tetlock",
        "tags": [
          "person"
        ],
        "content": "#person",
        "size": 18.857142857142858,
        "index": 326,
        "x": -269.190694144188,
        "y": -121.36964146524063,
        "vy": -0.698755466059142,
        "vx": 0.0871105431534283
      },
      "index": 377
    },
    {
      "source": {
        "id": "Aviv Ovadya",
        "title": "Aviv Ovadya",
        "tags": [
          "person"
        ],
        "content": "#person \n\nhttps://aviv.me/",
        "size": 18.285714285714285,
        "index": 283,
        "x": 1597.9484999890756,
        "y": 842.2153044669851,
        "vy": -1.4288230936945292,
        "vx": 0.3153930584572157
      },
      "target": {
        "id": "AI & Democracy Foundation",
        "title": "AI & Democracy Foundation",
        "tags": [
          "organization"
        ],
        "content": "#organization \n\nhttps://aidemocracyfoundation.org/",
        "size": 17.714285714285715,
        "index": 432,
        "x": 1666.8768486198028,
        "y": 789.5155954205912,
        "vy": -1.4702217936721376,
        "vx": 0.26566951389858834
      },
      "index": 378
    },
    {
      "source": {
        "id": "Let’s use AI to harden human defenses against AI manipulation",
        "title": "Let’s use AI to harden human defenses against AI manipulation",
        "tags": [
          "project"
        ],
        "content": "#project \n\nhttps://www.lesswrong.com/posts/zxmzBTwKkPMxQQcfR/let-s-use-ai-to-harden-human-defenses-against-ai\n\nBlog post proposes using AI systems to discover and catalog human manipulation techniques in order to train \"detector-AIs\" and humans to recognize these tactics, essentially creating a defensive system against AI manipulation. The approach involves optimizing AIs to persuade humans of both truths and falsehoods, analyzing what manipulation strategies they develop, and then training detection systems to identify these techniques in real-world scenarios.",
        "size": 16.571428571428573,
        "index": 285,
        "x": -1820.2062335519965,
        "y": 123.53840436834315,
        "vy": -0.1755662190399219,
        "vx": 0.20579865553798893
      },
      "target": {
        "id": "Tom Davidson",
        "title": "Tom Davidson",
        "tags": [
          "person"
        ],
        "content": "#person \n\nhttps://tomdavidson-ai.github.io/",
        "size": 17.142857142857142,
        "index": 24,
        "x": -1788.7773735372552,
        "y": 204.77188300552578,
        "vy": -0.1772045677729227,
        "vx": 0.2246908047923073
      },
      "index": 379
    },
    {
      "source": {
        "id": "Mapping AI Discourse",
        "title": "Mapping AI Discourse",
        "tags": [
          "topic"
        ],
        "content": "#topic",
        "size": 18.285714285714285,
        "index": 287,
        "x": 953.8736451437898,
        "y": -640.3740320424765,
        "vy": -1.2650627430421217,
        "vx": -0.2563483233916974
      },
      "target": {
        "id": "Recursive Public",
        "title": "Recursive Public",
        "tags": [
          "project"
        ],
        "content": "#project \n\nhttps://vtaiwan-openai-2023.vercel.app/Report_%20Recursive%20Public.pdf\n\nPilot democratic deliberation system designed to gather public input on AI governance decisions through cascading online discussions and AI-powered analysis. The project used platforms like [Polis](https://pol.is) and [Talk to the City](https://ai.objectives.institute/talk-to-the-city) to engage over 1,000 participants across different communities in identifying AI governance priorities, then employed LLMs to analyze consensus and disagreement patterns in the discussions.\n\nFunded by [OpenAI Democratic Inputs to AI](https://github.com/openai/democratic-inputs).",
        "size": 21.714285714285715,
        "index": 36,
        "x": 835.0730612401152,
        "y": -42.327038893972045,
        "vy": -1.2017478297377246,
        "vx": -0.0014674486757292548
      },
      "type": "topic_link",
      "index": 380
    },
    {
      "source": {
        "id": "Mapping AI Discourse",
        "title": "Mapping AI Discourse",
        "tags": [
          "topic"
        ],
        "content": "#topic",
        "size": 18.285714285714285,
        "index": 287,
        "x": 953.8736451437898,
        "y": -640.3740320424765,
        "vy": -1.2650627430421217,
        "vx": -0.2563483233916974
      },
      "target": {
        "id": "Global Dialogues",
        "title": "Global Dialogues",
        "tags": [
          "project"
        ],
        "content": "#project \n\nhttps://globaldialogues.ai\n\nResearch initiative that uses AI-powered interactive dialogue platforms to systematically map global public perspectives on AI development and its societal impacts.",
        "size": 17.714285714285715,
        "index": 320,
        "x": 1296.0147866388418,
        "y": 779.8433552858686,
        "vy": -1.3451514847475807,
        "vx": 0.28839187429349333
      },
      "type": "topic_link",
      "index": 381
    },
    {
      "source": {
        "id": "Mapping AI Discourse",
        "title": "Mapping AI Discourse",
        "tags": [
          "topic"
        ],
        "content": "#topic",
        "size": 18.285714285714285,
        "index": 287,
        "x": 953.8736451437898,
        "y": -640.3740320424765,
        "vy": -1.2650627430421217,
        "vx": -0.2563483233916974
      },
      "target": {
        "id": "Mapping the Discourse on AI Safety & Ethics",
        "title": "Mapping the Discourse on AI Safety & Ethics",
        "tags": [
          "project"
        ],
        "content": "#project \n\nhttps://ai.objectives.institute/blog/mapping-the-discourse-on-ai-safety-amp-ethics\n\nProject by the AI Objectives Institute that used their \"Talk to the City\" tool to analyze and map Twitter conversations about AI safety and ethics, identifying six distinct perspectives ranging from near-term harms to long-term existential risks. The tool automatically clustered thousands of tweets to reveal different viewpoints on AI governance, safety approaches, and development concerns, finding significant overlap between groups despite their different priorities.",
        "size": 18.285714285714285,
        "index": 0,
        "x": 788.547473702122,
        "y": -380.61239477665157,
        "vy": -1.1528366322432453,
        "vx": -0.09430758140445046
      },
      "type": "topic_link",
      "index": 382
    },
    {
      "source": {
        "id": "Mapping AI Discourse",
        "title": "Mapping AI Discourse",
        "tags": [
          "topic"
        ],
        "content": "#topic",
        "size": 18.285714285714285,
        "index": 287,
        "x": 953.8736451437898,
        "y": -640.3740320424765,
        "vy": -1.2650627430421217,
        "vx": -0.2563483233916974
      },
      "target": {
        "id": "AI Debate Maps",
        "title": "AI Debate Maps",
        "tags": [
          "project"
        ],
        "content": "#project \n\nhttps://www.societylibrary.org/topics-blog/ai-alignment-superintelligence-ethics\n\nDebate maps specifically about Artificial Intelligence.",
        "size": 17.142857142857142,
        "index": 116,
        "x": 119.44632840542677,
        "y": -1129.4661000636625,
        "vy": -0.8702793844281093,
        "vx": -0.46610233240688503
      },
      "type": "topic_link",
      "index": 383
    },
    {
      "source": {
        "id": "Finn Hambly",
        "title": "Finn Hambly",
        "tags": [
          "person"
        ],
        "content": "#person",
        "size": 17.142857142857142,
        "index": 289,
        "x": -777.0782596942344,
        "y": -732.8668921963484,
        "vy": -0.39969189429602353,
        "vx": -0.3928166053995662
      },
      "target": {
        "id": "Future Search",
        "title": "Future Search",
        "tags": [
          "organization"
        ],
        "content": "#organization \n\nhttps://futuresearch.ai/",
        "size": 22.285714285714285,
        "index": 94,
        "x": -717.8526553292105,
        "y": -719.7276361443852,
        "vy": -0.4404383940953807,
        "vx": -0.3824839533244458
      },
      "index": 384
    },
    {
      "source": {
        "id": "Christopher Small",
        "title": "Christopher Small",
        "tags": [
          "person"
        ],
        "content": "#person \n\nhttp://metasoarous.com/",
        "size": 18.285714285714285,
        "index": 290,
        "x": 1854.4692018669803,
        "y": 861.0514458806144,
        "vy": -1.5207937741217539,
        "vx": 0.3624142296931689
      },
      "target": {
        "id": "Google Jigsaw",
        "title": "Google Jigsaw",
        "tags": [
          "organization"
        ],
        "content": "#organization \n\nhttps://jigsaw.google.com/",
        "size": 19.428571428571427,
        "index": 390,
        "x": 1784.38820840888,
        "y": 1035.7563657120636,
        "vy": -1.5493284452037834,
        "vx": 0.4016872013074959
      },
      "index": 385
    },
    {
      "source": {
        "id": "Christopher Small",
        "title": "Christopher Small",
        "tags": [
          "person"
        ],
        "content": "#person \n\nhttp://metasoarous.com/",
        "size": 18.285714285714285,
        "index": 290,
        "x": 1854.4692018669803,
        "y": 861.0514458806144,
        "vy": -1.5207937741217539,
        "vx": 0.3624142296931689
      },
      "target": {
        "id": "The Computational Democracy Project",
        "title": "The Computational Democracy Project",
        "tags": [
          "organization"
        ],
        "content": "#organization \n\nhttps://compdemocracy.org/",
        "size": 19.428571428571427,
        "index": 138,
        "x": 2057.334021465157,
        "y": 849.740764420989,
        "vy": -1.5779545596273115,
        "vx": 0.3854928364433583
      },
      "index": 386
    },
    {
      "source": {
        "id": "Lisa Schirch",
        "title": "Lisa Schirch",
        "tags": [
          "person"
        ],
        "content": "#person \n\nhttps://kroc.nd.edu/faculty-and-staff/lisa-schirch/",
        "size": 18.285714285714285,
        "index": 291,
        "x": 1561.488577698378,
        "y": 890.5844108032592,
        "vy": -1.453577080273647,
        "vx": 0.2825364996104594
      },
      "target": {
        "id": "Plurality Institute",
        "title": "Plurality Institute",
        "tags": [
          "organization"
        ],
        "content": "#organization",
        "size": 20,
        "index": 364,
        "x": 1548.928220590021,
        "y": 1070.4694466931655,
        "vy": -1.5138983339043548,
        "vx": 0.41914174319685277
      },
      "index": 387
    },
    {
      "source": {
        "id": "Sensemaker",
        "title": "Sensemaker",
        "tags": [
          "project"
        ],
        "content": "#project \n\nhttps://github.com/Jigsaw-Code/sensemaking-tools/\nhttps://report.whatcouldbgbe.com/\n\nCollection of AI-powered systems designed to help analyze and understand large-scale online conversations by automatically identifying topics, categorizing statements, and summarizing areas of agreement and disagreement from thousands of public comments.",
        "size": 16.571428571428573,
        "index": 293,
        "x": 1893.37190843819,
        "y": 1114.4626559031512,
        "vy": -1.6435195590643616,
        "vx": 0.37069239492580885
      },
      "target": {
        "id": "Google Jigsaw",
        "title": "Google Jigsaw",
        "tags": [
          "organization"
        ],
        "content": "#organization \n\nhttps://jigsaw.google.com/",
        "size": 19.428571428571427,
        "index": 390,
        "x": 1784.38820840888,
        "y": 1035.7563657120636,
        "vy": -1.5493284452037834,
        "vx": 0.4016872013074959
      },
      "index": 388
    },
    {
      "source": {
        "id": "Elizabeth Barry",
        "title": "Elizabeth Barry",
        "tags": [
          "person"
        ],
        "content": "#person",
        "size": 17.142857142857142,
        "index": 296,
        "x": 2038.8526036228927,
        "y": 782.8111371581372,
        "vy": -1.7075617448758047,
        "vx": 0.3940459596078392
      },
      "target": {
        "id": "The Computational Democracy Project",
        "title": "The Computational Democracy Project",
        "tags": [
          "organization"
        ],
        "content": "#organization \n\nhttps://compdemocracy.org/",
        "size": 19.428571428571427,
        "index": 138,
        "x": 2057.334021465157,
        "y": 849.740764420989,
        "vy": -1.5779545596273115,
        "vx": 0.3854928364433583
      },
      "index": 389
    },
    {
      "source": {
        "id": "Pat Pataranutaporn",
        "title": "Pat Pataranutaporn",
        "tags": [
          "person"
        ],
        "content": "#person \n\nhttps://patpat.world/",
        "size": 18.285714285714285,
        "index": 298,
        "x": 1066.2378443172674,
        "y": -2682.567814798428,
        "vy": -1.1214877655381226,
        "vx": -0.5139886416200151
      },
      "target": {
        "id": "MIT Media Lab",
        "title": "MIT Media Lab",
        "tags": [
          "organization"
        ],
        "content": "#organization \n\nhttps://www.media.mit.edu/",
        "size": 17.714285714285715,
        "index": 269,
        "x": 1006.2843605632936,
        "y": -2684.9292905918865,
        "vy": -1.0825039215382666,
        "vx": -0.515555946443613
      },
      "index": 390
    },
    {
      "source": {
        "id": "LLMs Can Teach Themselves to Better Predict the Future",
        "title": "LLMs Can Teach Themselves to Better Predict the Future",
        "tags": [
          "project"
        ],
        "content": "#project \n\nhttps://arxiv.org/pdf/2502.05253\n\nMethod for improving large language models' forecasting capabilities through self-play and outcome-driven fine-tuning, where models generate multiple reasoning traces for prediction questions and learn from which approaches led to more accurate forecasts. The authors demonstrate that smaller models (14B parameters) fine-tuned with this method can achieve forecasting performance comparable to much larger frontier models like GPT-4o.",
        "size": 18.285714285714285,
        "index": 299,
        "x": -409.11929591810326,
        "y": -402.5919502022023,
        "vy": -0.6893254342018448,
        "vx": 0.08644409566424723
      },
      "target": {
        "id": "Ben Turtel",
        "title": "Ben Turtel",
        "tags": [
          "person"
        ],
        "content": "#person",
        "size": 17.714285714285715,
        "index": 405,
        "x": -483.86316913010216,
        "y": -469.8714481131538,
        "vy": -0.5954542916718669,
        "vx": 0.08327716391928561
      },
      "index": 391
    },
    {
      "source": {
        "id": "LLMs Can Teach Themselves to Better Predict the Future",
        "title": "LLMs Can Teach Themselves to Better Predict the Future",
        "tags": [
          "project"
        ],
        "content": "#project \n\nhttps://arxiv.org/pdf/2502.05253\n\nMethod for improving large language models' forecasting capabilities through self-play and outcome-driven fine-tuning, where models generate multiple reasoning traces for prediction questions and learn from which approaches led to more accurate forecasts. The authors demonstrate that smaller models (14B parameters) fine-tuned with this method can achieve forecasting performance comparable to much larger frontier models like GPT-4o.",
        "size": 18.285714285714285,
        "index": 299,
        "x": -409.11929591810326,
        "y": -402.5919502022023,
        "vy": -0.6893254342018448,
        "vx": 0.08644409566424723
      },
      "target": {
        "id": "Danny Fanklin",
        "title": "Danny Fanklin",
        "tags": [
          "person"
        ],
        "content": "#person",
        "size": 17.714285714285715,
        "index": 59,
        "x": -431.6774386700457,
        "y": -494.60304253552584,
        "vy": -0.673501095578475,
        "vx": 0.04697915176713927
      },
      "index": 392
    },
    {
      "source": {
        "id": "LLMs Can Teach Themselves to Better Predict the Future",
        "title": "LLMs Can Teach Themselves to Better Predict the Future",
        "tags": [
          "project"
        ],
        "content": "#project \n\nhttps://arxiv.org/pdf/2502.05253\n\nMethod for improving large language models' forecasting capabilities through self-play and outcome-driven fine-tuning, where models generate multiple reasoning traces for prediction questions and learn from which approaches led to more accurate forecasts. The authors demonstrate that smaller models (14B parameters) fine-tuned with this method can achieve forecasting performance comparable to much larger frontier models like GPT-4o.",
        "size": 18.285714285714285,
        "index": 299,
        "x": -409.11929591810326,
        "y": -402.5919502022023,
        "vy": -0.6893254342018448,
        "vx": 0.08644409566424723
      },
      "target": {
        "id": "Philipp Schoenegger",
        "title": "Philipp Schoenegger",
        "tags": [
          "person"
        ],
        "content": "#person \n\nhttps://philipp-schoenegger.weebly.com/",
        "size": 19.428571428571427,
        "index": 120,
        "x": -308.0505670602049,
        "y": -355.92667066361173,
        "vy": -0.7160490223836882,
        "vx": 0.03780161064368572
      },
      "index": 393
    },
    {
      "source": {
        "id": "Nexus",
        "title": "Nexus",
        "tags": [
          "project"
        ],
        "content": "#project \n\nhttps://nexus-tool.com/\n\nNexus is a discussion platform that organizes user statements and reactions into a visual graph, where semantically similar statements are connected and colored based on consensus, divergence, and quality scores. The tool specifically counteracts factionalism and amplifying unique perspectives by weighting minority voices more heavily, highlighting surprising agreement between usual opponents (consensus), and promoting dissenting views within like-minded groups (divergence).",
        "size": 17.142857142857142,
        "index": 303,
        "x": 3709.580929369303,
        "y": -1062.1952294556093,
        "vy": -0.9785489830017879,
        "vx": 0.15937506406491578
      },
      "target": {
        "id": "Sofi Vanhanen",
        "title": "Sofi Vanhanen",
        "tags": [
          "person"
        ],
        "content": "#person \n\nhttps://sofiavanhanen.fi/",
        "size": 18.285714285714285,
        "index": 177,
        "x": 3629.495175318134,
        "y": -1087.890476428783,
        "vy": -0.9980357709893994,
        "vx": 0.19329731806245978
      },
      "index": 394
    },
    {
      "source": {
        "id": "Nexus",
        "title": "Nexus",
        "tags": [
          "project"
        ],
        "content": "#project \n\nhttps://nexus-tool.com/\n\nNexus is a discussion platform that organizes user statements and reactions into a visual graph, where semantically similar statements are connected and colored based on consensus, divergence, and quality scores. The tool specifically counteracts factionalism and amplifying unique perspectives by weighting minority voices more heavily, highlighting surprising agreement between usual opponents (consensus), and promoting dissenting views within like-minded groups (divergence).",
        "size": 17.142857142857142,
        "index": 303,
        "x": 3709.580929369303,
        "y": -1062.1952294556093,
        "vy": -0.9785489830017879,
        "vx": 0.15937506406491578
      },
      "target": {
        "id": "Niki Dupuis",
        "title": "Niki Dupuis",
        "tags": [
          "person"
        ],
        "content": "#person",
        "size": 17.714285714285715,
        "index": 65,
        "x": 3688.7093262635094,
        "y": -985.8406135311669,
        "vy": -0.9897340634814135,
        "vx": 0.17742209799442776
      },
      "index": 395
    },
    {
      "source": {
        "id": "Q1 AI Benchmarking results",
        "title": "Q1 AI Benchmarking results",
        "tags": [
          "project"
        ],
        "content": "#project \n\nhttps://www.metaculus.com/notebooks/38673/q1-ai-benchmarking-results/\n\nReport on Metaculus's quarterly tournament that compared AI forecasting bots against professional human forecasters on real-world prediction questions, with the key finding that professional forecasters significantly outperformed the best AI systems.",
        "size": 18.285714285714285,
        "index": 311,
        "x": -376.8873672015769,
        "y": -773.3612804866773,
        "vy": -0.6741199914635285,
        "vx": -0.2863978459087953
      },
      "target": {
        "id": "Ben Wilson",
        "title": "Ben Wilson",
        "tags": [
          "person"
        ],
        "content": "#person",
        "size": 18.285714285714285,
        "index": 98,
        "x": -246.84657412896797,
        "y": -802.6439658065143,
        "vy": -0.7451535349150801,
        "vx": -0.25821581807688415
      },
      "index": 396
    },
    {
      "source": {
        "id": "Q1 AI Benchmarking results",
        "title": "Q1 AI Benchmarking results",
        "tags": [
          "project"
        ],
        "content": "#project \n\nhttps://www.metaculus.com/notebooks/38673/q1-ai-benchmarking-results/\n\nReport on Metaculus's quarterly tournament that compared AI forecasting bots against professional human forecasters on real-world prediction questions, with the key finding that professional forecasters significantly outperformed the best AI systems.",
        "size": 18.285714285714285,
        "index": 311,
        "x": -376.8873672015769,
        "y": -773.3612804866773,
        "vy": -0.6741199914635285,
        "vx": -0.2863978459087953
      },
      "target": {
        "id": "John Bash",
        "title": "John Bash",
        "tags": [
          "person"
        ],
        "content": "#person",
        "size": 17.142857142857142,
        "index": 222,
        "x": -295.00162878021047,
        "y": -699.0077204062693,
        "vy": -0.8031477917263522,
        "vx": -0.3241548422855935
      },
      "index": 397
    },
    {
      "source": {
        "id": "Q1 AI Benchmarking results",
        "title": "Q1 AI Benchmarking results",
        "tags": [
          "project"
        ],
        "content": "#project \n\nhttps://www.metaculus.com/notebooks/38673/q1-ai-benchmarking-results/\n\nReport on Metaculus's quarterly tournament that compared AI forecasting bots against professional human forecasters on real-world prediction questions, with the key finding that professional forecasters significantly outperformed the best AI systems.",
        "size": 18.285714285714285,
        "index": 311,
        "x": -376.8873672015769,
        "y": -773.3612804866773,
        "vy": -0.6741199914635285,
        "vx": -0.2863978459087953
      },
      "target": {
        "id": "AI Forecasting Benchmark",
        "title": "AI Forecasting Benchmark",
        "tags": [
          "project"
        ],
        "content": "#project \n\nhttps://www.metaculus.com/aib/\n\nMetaculus AIB (AI Benchmarking) is a series that benchmarks \"the state of the art in AI forecasting against the best humans on real-world questions.\"",
        "size": 20.57142857142857,
        "index": 178,
        "x": -491.69904780690194,
        "y": -761.3030318136879,
        "vy": -0.5357053744666,
        "vx": -0.2989940012828504
      },
      "index": 398
    },
    {
      "source": {
        "id": "Roast My Post",
        "title": "Roast My Post",
        "tags": [
          "project"
        ],
        "content": "#project \n\nhttps://www.roastmypost.org\nhttps://github.com/quantified-uncertainty/roast-my-post\n\nOpen-source AI-powered platform for document analysis and evaluation. It allows users to upload documents and receive feedback with inline comments from customizable AI agents.",
        "size": 16.571428571428573,
        "index": 316,
        "x": -796.7656435233521,
        "y": -1415.5938324039541,
        "vy": -0.36606208538854773,
        "vx": -0.6543139638357243
      },
      "target": {
        "id": "Quantified Uncertainty Research Institute",
        "title": "Quantified Uncertainty Research Institute",
        "tags": [
          "organization"
        ],
        "content": "#organization \n\nhttps://quantifieduncertainty.org/",
        "size": 17.142857142857142,
        "index": 146,
        "x": -798.2335318785231,
        "y": -1310.1155250287434,
        "vy": -0.36104964512126025,
        "vx": -0.5790503716227251
      },
      "index": 399
    },
    {
      "source": {
        "id": "Global Dialogues",
        "title": "Global Dialogues",
        "tags": [
          "project"
        ],
        "content": "#project \n\nhttps://globaldialogues.ai\n\nResearch initiative that uses AI-powered interactive dialogue platforms to systematically map global public perspectives on AI development and its societal impacts.",
        "size": 17.714285714285715,
        "index": 320,
        "x": 1296.0147866388418,
        "y": 779.8433552858686,
        "vy": -1.3451514847475807,
        "vx": 0.28839187429349333
      },
      "target": {
        "id": "Collective Intelligence Project",
        "title": "Collective Intelligence Project",
        "tags": [
          "organization"
        ],
        "content": "#organization \n\nhttps://www.cip.org/",
        "size": 18.285714285714285,
        "index": 71,
        "x": 1196.948459287815,
        "y": 832.5435137077104,
        "vy": -1.3117327264452452,
        "vx": 0.30198095758611615
      },
      "index": 400
    },
    {
      "source": {
        "id": "Global Dialogues",
        "title": "Global Dialogues",
        "tags": [
          "project"
        ],
        "content": "#project \n\nhttps://globaldialogues.ai\n\nResearch initiative that uses AI-powered interactive dialogue platforms to systematically map global public perspectives on AI development and its societal impacts.",
        "size": 17.714285714285715,
        "index": 320,
        "x": 1296.0147866388418,
        "y": 779.8433552858686,
        "vy": -1.3451514847475807,
        "vx": 0.28839187429349333
      },
      "target": {
        "id": "Remesh",
        "title": "Remesh",
        "tags": [
          "organization"
        ],
        "content": "#organization \n\nhttps://www.remesh.ai",
        "size": 17.142857142857142,
        "index": 6,
        "x": 1434.9197807893906,
        "y": 780.4664062408266,
        "vy": -1.3892800306989497,
        "vx": 0.2759787817962076
      },
      "index": 401
    },
    {
      "source": {
        "id": "Luke Stebbing",
        "title": "Luke Stebbing",
        "tags": [
          "person"
        ],
        "content": "#person \n\nhttps://lukestebbing.com/",
        "size": 17.714285714285715,
        "index": 323,
        "x": -1992.994109604841,
        "y": 471.5976775812864,
        "vy": -0.4326831697641754,
        "vx": -0.058212480520888205
      },
      "target": {
        "id": "Elicit",
        "title": "Elicit",
        "tags": [
          "organization"
        ],
        "content": "#organization \n\nhttps://elicit.com/",
        "size": 20.57142857142857,
        "index": 218,
        "x": -2027.2873240845167,
        "y": 375.96539772345454,
        "vy": -0.29711133317878174,
        "vx": 0.26985144319061544
      },
      "index": 402
    },
    {
      "source": {
        "id": "Luke Stebbing",
        "title": "Luke Stebbing",
        "tags": [
          "person"
        ],
        "content": "#person \n\nhttps://lukestebbing.com/",
        "size": 17.714285714285715,
        "index": 323,
        "x": -1992.994109604841,
        "y": 471.5976775812864,
        "vy": -0.4326831697641754,
        "vx": -0.058212480520888205
      },
      "target": {
        "id": "Ought",
        "title": "Ought",
        "tags": [
          "organization"
        ],
        "content": "#organization \n\nhttps://ought.org/ (website is old)\n\nOught 1.0 has effectively become what is now Elicit (the company) continuing development on Elicit (the tool). Ought 2.0 has not yet done any public work.",
        "size": 23.42857142857143,
        "index": 306,
        "x": -1888.3722954977052,
        "y": 453.0511056583274,
        "vy": -0.2547035865836596,
        "vx": 0.0471838457037637
      },
      "index": 403
    },
    {
      "source": {
        "id": "How Malicious AI Swarms Can Threaten Democracy",
        "title": "How Malicious AI Swarms Can Threaten Democracy",
        "tags": [
          "project"
        ],
        "content": "#project \n\nhttps://arxiv.org/pdf/2506.06299\n\nExamines how coordinated networks of AI agents (\"AI swarms\") can manipulate democratic discourse and decision-making through sophisticated influence operations. The authors warn that unlike traditional botnets that simply repeat scripted messages, AI swarms can create thousands of distinct personas that adapt, learn from feedback, and coordinate autonomously to fabricate grassroots consensus and fragment shared reality.",
        "size": 28,
        "index": 324,
        "x": 734.1845256822567,
        "y": 681.6766011163681,
        "vy": -1.2242065545784204,
        "vx": 0.13586496766179135
      },
      "target": {
        "id": "Daniel Schroeder",
        "title": "Daniel Schroeder",
        "tags": [
          "person"
        ],
        "content": "#person \n",
        "size": 16.571428571428573,
        "index": 186,
        "x": 700.0322187809883,
        "y": 793.523096438554,
        "vy": -1.2425404235287438,
        "vx": 0.20480836297908
      },
      "index": 404
    },
    {
      "source": {
        "id": "How Malicious AI Swarms Can Threaten Democracy",
        "title": "How Malicious AI Swarms Can Threaten Democracy",
        "tags": [
          "project"
        ],
        "content": "#project \n\nhttps://arxiv.org/pdf/2506.06299\n\nExamines how coordinated networks of AI agents (\"AI swarms\") can manipulate democratic discourse and decision-making through sophisticated influence operations. The authors warn that unlike traditional botnets that simply repeat scripted messages, AI swarms can create thousands of distinct personas that adapt, learn from feedback, and coordinate autonomously to fabricate grassroots consensus and fragment shared reality.",
        "size": 28,
        "index": 324,
        "x": 734.1845256822567,
        "y": 681.6766011163681,
        "vy": -1.2242065545784204,
        "vx": 0.13586496766179135
      },
      "target": {
        "id": "Meeyoung Cha",
        "title": "Meeyoung Cha",
        "tags": [
          "person"
        ],
        "content": "#person \n",
        "size": 16.571428571428573,
        "index": 92,
        "x": 640.5524594959512,
        "y": 595.0887027404054,
        "vy": -1.163017060592356,
        "vx": 0.045750774594935616
      },
      "index": 405
    },
    {
      "source": {
        "id": "How Malicious AI Swarms Can Threaten Democracy",
        "title": "How Malicious AI Swarms Can Threaten Democracy",
        "tags": [
          "project"
        ],
        "content": "#project \n\nhttps://arxiv.org/pdf/2506.06299\n\nExamines how coordinated networks of AI agents (\"AI swarms\") can manipulate democratic discourse and decision-making through sophisticated influence operations. The authors warn that unlike traditional botnets that simply repeat scripted messages, AI swarms can create thousands of distinct personas that adapt, learn from feedback, and coordinate autonomously to fabricate grassroots consensus and fragment shared reality.",
        "size": 28,
        "index": 324,
        "x": 734.1845256822567,
        "y": 681.6766011163681,
        "vy": -1.2242065545784204,
        "vx": 0.13586496766179135
      },
      "target": {
        "id": "Andrea Baronchelli",
        "title": "Andrea Baronchelli",
        "tags": [
          "person"
        ],
        "content": "#person \n",
        "size": 16.571428571428573,
        "index": 417,
        "x": 645.0897059621471,
        "y": 815.980439445282,
        "vy": -1.2021063944879817,
        "vx": 0.22461961609277217
      },
      "index": 406
    },
    {
      "source": {
        "id": "How Malicious AI Swarms Can Threaten Democracy",
        "title": "How Malicious AI Swarms Can Threaten Democracy",
        "tags": [
          "project"
        ],
        "content": "#project \n\nhttps://arxiv.org/pdf/2506.06299\n\nExamines how coordinated networks of AI agents (\"AI swarms\") can manipulate democratic discourse and decision-making through sophisticated influence operations. The authors warn that unlike traditional botnets that simply repeat scripted messages, AI swarms can create thousands of distinct personas that adapt, learn from feedback, and coordinate autonomously to fabricate grassroots consensus and fragment shared reality.",
        "size": 28,
        "index": 324,
        "x": 734.1845256822567,
        "y": 681.6766011163681,
        "vy": -1.2242065545784204,
        "vx": 0.13586496766179135
      },
      "target": {
        "id": "Nick Bostrom",
        "title": "Nick Bostrom",
        "tags": [
          "person"
        ],
        "content": "#person \n\nhttps://nickbostrom.com/\n",
        "size": 16.571428571428573,
        "index": 95,
        "x": 692.6717770301715,
        "y": 630.92732644266,
        "vy": -1.2189117625982357,
        "vx": 0.07945420935306521
      },
      "index": 407
    },
    {
      "source": {
        "id": "How Malicious AI Swarms Can Threaten Democracy",
        "title": "How Malicious AI Swarms Can Threaten Democracy",
        "tags": [
          "project"
        ],
        "content": "#project \n\nhttps://arxiv.org/pdf/2506.06299\n\nExamines how coordinated networks of AI agents (\"AI swarms\") can manipulate democratic discourse and decision-making through sophisticated influence operations. The authors warn that unlike traditional botnets that simply repeat scripted messages, AI swarms can create thousands of distinct personas that adapt, learn from feedback, and coordinate autonomously to fabricate grassroots consensus and fragment shared reality.",
        "size": 28,
        "index": 324,
        "x": 734.1845256822567,
        "y": 681.6766011163681,
        "vy": -1.2242065545784204,
        "vx": 0.13586496766179135
      },
      "target": {
        "id": "Nicholas Christakis",
        "title": "Nicholas Christakis",
        "tags": [
          "person"
        ],
        "content": "#person \n",
        "size": 16.571428571428573,
        "index": 211,
        "x": 656.3899362887881,
        "y": 709.3746224659601,
        "vy": -1.209627221862739,
        "vx": 0.133623960280189
      },
      "index": 408
    },
    {
      "source": {
        "id": "How Malicious AI Swarms Can Threaten Democracy",
        "title": "How Malicious AI Swarms Can Threaten Democracy",
        "tags": [
          "project"
        ],
        "content": "#project \n\nhttps://arxiv.org/pdf/2506.06299\n\nExamines how coordinated networks of AI agents (\"AI swarms\") can manipulate democratic discourse and decision-making through sophisticated influence operations. The authors warn that unlike traditional botnets that simply repeat scripted messages, AI swarms can create thousands of distinct personas that adapt, learn from feedback, and coordinate autonomously to fabricate grassroots consensus and fragment shared reality.",
        "size": 28,
        "index": 324,
        "x": 734.1845256822567,
        "y": 681.6766011163681,
        "vy": -1.2242065545784204,
        "vx": 0.13586496766179135
      },
      "target": {
        "id": "David Garcia",
        "title": "David Garcia",
        "tags": [
          "person"
        ],
        "content": "#person \n",
        "size": 16.571428571428573,
        "index": 234,
        "x": 603.7471438883641,
        "y": 679.5813518746548,
        "vy": -1.130445157905492,
        "vx": 0.07912651364125231
      },
      "index": 409
    },
    {
      "source": {
        "id": "How Malicious AI Swarms Can Threaten Democracy",
        "title": "How Malicious AI Swarms Can Threaten Democracy",
        "tags": [
          "project"
        ],
        "content": "#project \n\nhttps://arxiv.org/pdf/2506.06299\n\nExamines how coordinated networks of AI agents (\"AI swarms\") can manipulate democratic discourse and decision-making through sophisticated influence operations. The authors warn that unlike traditional botnets that simply repeat scripted messages, AI swarms can create thousands of distinct personas that adapt, learn from feedback, and coordinate autonomously to fabricate grassroots consensus and fragment shared reality.",
        "size": 28,
        "index": 324,
        "x": 734.1845256822567,
        "y": 681.6766011163681,
        "vy": -1.2242065545784204,
        "vx": 0.13586496766179135
      },
      "target": {
        "id": "Amit Goldenberg",
        "title": "Amit Goldenberg",
        "tags": [
          "person"
        ],
        "content": "#person \n",
        "size": 16.571428571428573,
        "index": 154,
        "x": 701.437464398921,
        "y": 737.2087059205674,
        "vy": -1.247948086288025,
        "vx": 0.16508662449904934
      },
      "index": 410
    },
    {
      "source": {
        "id": "How Malicious AI Swarms Can Threaten Democracy",
        "title": "How Malicious AI Swarms Can Threaten Democracy",
        "tags": [
          "project"
        ],
        "content": "#project \n\nhttps://arxiv.org/pdf/2506.06299\n\nExamines how coordinated networks of AI agents (\"AI swarms\") can manipulate democratic discourse and decision-making through sophisticated influence operations. The authors warn that unlike traditional botnets that simply repeat scripted messages, AI swarms can create thousands of distinct personas that adapt, learn from feedback, and coordinate autonomously to fabricate grassroots consensus and fragment shared reality.",
        "size": 28,
        "index": 324,
        "x": 734.1845256822567,
        "y": 681.6766011163681,
        "vy": -1.2242065545784204,
        "vx": 0.13586496766179135
      },
      "target": {
        "id": "Yara Kyrychenko",
        "title": "Yara Kyrychenko",
        "tags": [
          "person"
        ],
        "content": "#person \n",
        "size": 16.571428571428573,
        "index": 369,
        "x": 649.6030223428628,
        "y": 759.0161123424392,
        "vy": -1.2053475340130404,
        "vx": 0.18384350246709855
      },
      "index": 411
    },
    {
      "source": {
        "id": "How Malicious AI Swarms Can Threaten Democracy",
        "title": "How Malicious AI Swarms Can Threaten Democracy",
        "tags": [
          "project"
        ],
        "content": "#project \n\nhttps://arxiv.org/pdf/2506.06299\n\nExamines how coordinated networks of AI agents (\"AI swarms\") can manipulate democratic discourse and decision-making through sophisticated influence operations. The authors warn that unlike traditional botnets that simply repeat scripted messages, AI swarms can create thousands of distinct personas that adapt, learn from feedback, and coordinate autonomously to fabricate grassroots consensus and fragment shared reality.",
        "size": 28,
        "index": 324,
        "x": 734.1845256822567,
        "y": 681.6766011163681,
        "vy": -1.2242065545784204,
        "vx": 0.13586496766179135
      },
      "target": {
        "id": "Kevin Leyton-Brown",
        "title": "Kevin Leyton-Brown",
        "tags": [
          "person"
        ],
        "content": "#person \n",
        "size": 16.571428571428573,
        "index": 33,
        "x": 609.4688133586419,
        "y": 733.6508194289695,
        "vy": -1.1647819733722542,
        "vx": 0.136271488561405
      },
      "index": 412
    },
    {
      "source": {
        "id": "How Malicious AI Swarms Can Threaten Democracy",
        "title": "How Malicious AI Swarms Can Threaten Democracy",
        "tags": [
          "project"
        ],
        "content": "#project \n\nhttps://arxiv.org/pdf/2506.06299\n\nExamines how coordinated networks of AI agents (\"AI swarms\") can manipulate democratic discourse and decision-making through sophisticated influence operations. The authors warn that unlike traditional botnets that simply repeat scripted messages, AI swarms can create thousands of distinct personas that adapt, learn from feedback, and coordinate autonomously to fabricate grassroots consensus and fragment shared reality.",
        "size": 28,
        "index": 324,
        "x": 734.1845256822567,
        "y": 681.6766011163681,
        "vy": -1.2242065545784204,
        "vx": 0.13586496766179135
      },
      "target": {
        "id": "Nina Lutz",
        "title": "Nina Lutz",
        "tags": [
          "person"
        ],
        "content": "#person \n",
        "size": 16.571428571428573,
        "index": 27,
        "x": 593.9891638038955,
        "y": 626.3486394185837,
        "vy": -1.1125008512614933,
        "vx": 0.07075420896955925
      },
      "index": 413
    },
    {
      "source": {
        "id": "How Malicious AI Swarms Can Threaten Democracy",
        "title": "How Malicious AI Swarms Can Threaten Democracy",
        "tags": [
          "project"
        ],
        "content": "#project \n\nhttps://arxiv.org/pdf/2506.06299\n\nExamines how coordinated networks of AI agents (\"AI swarms\") can manipulate democratic discourse and decision-making through sophisticated influence operations. The authors warn that unlike traditional botnets that simply repeat scripted messages, AI swarms can create thousands of distinct personas that adapt, learn from feedback, and coordinate autonomously to fabricate grassroots consensus and fragment shared reality.",
        "size": 28,
        "index": 324,
        "x": 734.1845256822567,
        "y": 681.6766011163681,
        "vy": -1.2242065545784204,
        "vx": 0.13586496766179135
      },
      "target": {
        "id": "Gary Marcus",
        "title": "Gary Marcus",
        "tags": [
          "person"
        ],
        "content": "#person \n\nhttp://www.garymarcus.com/",
        "size": 16.571428571428573,
        "index": 301,
        "x": 647.5288302303742,
        "y": 648.3052897331523,
        "vy": -1.1730569068164425,
        "vx": 0.07456166417608419
      },
      "index": 414
    },
    {
      "source": {
        "id": "How Malicious AI Swarms Can Threaten Democracy",
        "title": "How Malicious AI Swarms Can Threaten Democracy",
        "tags": [
          "project"
        ],
        "content": "#project \n\nhttps://arxiv.org/pdf/2506.06299\n\nExamines how coordinated networks of AI agents (\"AI swarms\") can manipulate democratic discourse and decision-making through sophisticated influence operations. The authors warn that unlike traditional botnets that simply repeat scripted messages, AI swarms can create thousands of distinct personas that adapt, learn from feedback, and coordinate autonomously to fabricate grassroots consensus and fragment shared reality.",
        "size": 28,
        "index": 324,
        "x": 734.1845256822567,
        "y": 681.6766011163681,
        "vy": -1.2242065545784204,
        "vx": 0.13586496766179135
      },
      "target": {
        "id": "Filippo Menczer",
        "title": "Filippo Menczer",
        "tags": [
          "person"
        ],
        "content": "#person \n",
        "size": 16.571428571428573,
        "index": 169,
        "x": 599.2964114239293,
        "y": 788.7618531523019,
        "vy": -1.159065617718064,
        "vx": 0.2187651263067182
      },
      "index": 415
    },
    {
      "source": {
        "id": "How Malicious AI Swarms Can Threaten Democracy",
        "title": "How Malicious AI Swarms Can Threaten Democracy",
        "tags": [
          "project"
        ],
        "content": "#project \n\nhttps://arxiv.org/pdf/2506.06299\n\nExamines how coordinated networks of AI agents (\"AI swarms\") can manipulate democratic discourse and decision-making through sophisticated influence operations. The authors warn that unlike traditional botnets that simply repeat scripted messages, AI swarms can create thousands of distinct personas that adapt, learn from feedback, and coordinate autonomously to fabricate grassroots consensus and fragment shared reality.",
        "size": 28,
        "index": 324,
        "x": 734.1845256822567,
        "y": 681.6766011163681,
        "vy": -1.2242065545784204,
        "vx": 0.13586496766179135
      },
      "target": {
        "id": "Gordon Pennycook",
        "title": "Gordon Pennycook",
        "tags": [
          "person"
        ],
        "content": "#person \n",
        "size": 16.571428571428573,
        "index": 279,
        "x": 562.3827246281912,
        "y": 703.2775975225394,
        "vy": -1.0878859812299277,
        "vx": 0.06912178940387369
      },
      "index": 416
    },
    {
      "source": {
        "id": "How Malicious AI Swarms Can Threaten Democracy",
        "title": "How Malicious AI Swarms Can Threaten Democracy",
        "tags": [
          "project"
        ],
        "content": "#project \n\nhttps://arxiv.org/pdf/2506.06299\n\nExamines how coordinated networks of AI agents (\"AI swarms\") can manipulate democratic discourse and decision-making through sophisticated influence operations. The authors warn that unlike traditional botnets that simply repeat scripted messages, AI swarms can create thousands of distinct personas that adapt, learn from feedback, and coordinate autonomously to fabricate grassroots consensus and fragment shared reality.",
        "size": 28,
        "index": 324,
        "x": 734.1845256822567,
        "y": 681.6766011163681,
        "vy": -1.2242065545784204,
        "vx": 0.13586496766179135
      },
      "target": {
        "id": "David Rand",
        "title": "David Rand",
        "tags": [
          "person"
        ],
        "content": "#person \n",
        "size": 16.571428571428573,
        "index": 5,
        "x": 552.86779890692,
        "y": 654.2670978276634,
        "vy": -1.1089150317079988,
        "vx": 0.0797534730358719
      },
      "index": 417
    },
    {
      "source": {
        "id": "How Malicious AI Swarms Can Threaten Democracy",
        "title": "How Malicious AI Swarms Can Threaten Democracy",
        "tags": [
          "project"
        ],
        "content": "#project \n\nhttps://arxiv.org/pdf/2506.06299\n\nExamines how coordinated networks of AI agents (\"AI swarms\") can manipulate democratic discourse and decision-making through sophisticated influence operations. The authors warn that unlike traditional botnets that simply repeat scripted messages, AI swarms can create thousands of distinct personas that adapt, learn from feedback, and coordinate autonomously to fabricate grassroots consensus and fragment shared reality.",
        "size": 28,
        "index": 324,
        "x": 734.1845256822567,
        "y": 681.6766011163681,
        "vy": -1.2242065545784204,
        "vx": 0.13586496766179135
      },
      "target": {
        "id": "Frank Schweitzer",
        "title": "Frank Schweitzer",
        "tags": [
          "person"
        ],
        "content": "#person \n",
        "size": 16.571428571428573,
        "index": 295,
        "x": 558.6711963993663,
        "y": 760.2996607075387,
        "vy": -1.0801188792187995,
        "vx": 0.18666877888852265
      },
      "index": 418
    },
    {
      "source": {
        "id": "How Malicious AI Swarms Can Threaten Democracy",
        "title": "How Malicious AI Swarms Can Threaten Democracy",
        "tags": [
          "project"
        ],
        "content": "#project \n\nhttps://arxiv.org/pdf/2506.06299\n\nExamines how coordinated networks of AI agents (\"AI swarms\") can manipulate democratic discourse and decision-making through sophisticated influence operations. The authors warn that unlike traditional botnets that simply repeat scripted messages, AI swarms can create thousands of distinct personas that adapt, learn from feedback, and coordinate autonomously to fabricate grassroots consensus and fragment shared reality.",
        "size": 28,
        "index": 324,
        "x": 734.1845256822567,
        "y": 681.6766011163681,
        "vy": -1.2242065545784204,
        "vx": 0.13586496766179135
      },
      "target": {
        "id": "Christopher Summerfield",
        "title": "Christopher Summerfield",
        "tags": [
          "person"
        ],
        "content": "#person",
        "size": 18.285714285714285,
        "index": 318,
        "x": 1037.8359073408596,
        "y": 622.2526019294744,
        "vy": -1.2802118806311242,
        "vx": 0.2036645449820769
      },
      "index": 419
    },
    {
      "source": {
        "id": "How Malicious AI Swarms Can Threaten Democracy",
        "title": "How Malicious AI Swarms Can Threaten Democracy",
        "tags": [
          "project"
        ],
        "content": "#project \n\nhttps://arxiv.org/pdf/2506.06299\n\nExamines how coordinated networks of AI agents (\"AI swarms\") can manipulate democratic discourse and decision-making through sophisticated influence operations. The authors warn that unlike traditional botnets that simply repeat scripted messages, AI swarms can create thousands of distinct personas that adapt, learn from feedback, and coordinate autonomously to fabricate grassroots consensus and fragment shared reality.",
        "size": 28,
        "index": 324,
        "x": 734.1845256822567,
        "y": 681.6766011163681,
        "vy": -1.2242065545784204,
        "vx": 0.13586496766179135
      },
      "target": {
        "id": "Audrey Tang",
        "title": "Audrey Tang",
        "tags": [
          "person"
        ],
        "content": "#person \n\nhttps://github.com/audreyt",
        "size": 18.857142857142858,
        "index": 80,
        "x": 1086.0447800074567,
        "y": 719.5887259305994,
        "vy": -1.393226340879792,
        "vx": 0.1542153667232109
      },
      "index": 420
    },
    {
      "source": {
        "id": "How Malicious AI Swarms Can Threaten Democracy",
        "title": "How Malicious AI Swarms Can Threaten Democracy",
        "tags": [
          "project"
        ],
        "content": "#project \n\nhttps://arxiv.org/pdf/2506.06299\n\nExamines how coordinated networks of AI agents (\"AI swarms\") can manipulate democratic discourse and decision-making through sophisticated influence operations. The authors warn that unlike traditional botnets that simply repeat scripted messages, AI swarms can create thousands of distinct personas that adapt, learn from feedback, and coordinate autonomously to fabricate grassroots consensus and fragment shared reality.",
        "size": 28,
        "index": 324,
        "x": 734.1845256822567,
        "y": 681.6766011163681,
        "vy": -1.2242065545784204,
        "vx": 0.13586496766179135
      },
      "target": {
        "id": "Jay Van Bavel",
        "title": "Jay Van Bavel",
        "tags": [
          "person"
        ],
        "content": "#person \n",
        "size": 16.571428571428573,
        "index": 51,
        "x": 746.3419841580535,
        "y": 765.2003632492579,
        "vy": -1.254491896611489,
        "vx": 0.17902049181596355
      },
      "index": 421
    },
    {
      "source": {
        "id": "How Malicious AI Swarms Can Threaten Democracy",
        "title": "How Malicious AI Swarms Can Threaten Democracy",
        "tags": [
          "project"
        ],
        "content": "#project \n\nhttps://arxiv.org/pdf/2506.06299\n\nExamines how coordinated networks of AI agents (\"AI swarms\") can manipulate democratic discourse and decision-making through sophisticated influence operations. The authors warn that unlike traditional botnets that simply repeat scripted messages, AI swarms can create thousands of distinct personas that adapt, learn from feedback, and coordinate autonomously to fabricate grassroots consensus and fragment shared reality.",
        "size": 28,
        "index": 324,
        "x": 734.1845256822567,
        "y": 681.6766011163681,
        "vy": -1.2242065545784204,
        "vx": 0.13586496766179135
      },
      "target": {
        "id": "Sander van der Linden",
        "title": "Sander van der Linden",
        "tags": [
          "person"
        ],
        "content": "#person \n",
        "size": 16.571428571428573,
        "index": 53,
        "x": 741.6974951717235,
        "y": 613.1207583737784,
        "vy": -1.2771186971996775,
        "vx": 0.06502247942768759
      },
      "index": 422
    },
    {
      "source": {
        "id": "How Malicious AI Swarms Can Threaten Democracy",
        "title": "How Malicious AI Swarms Can Threaten Democracy",
        "tags": [
          "project"
        ],
        "content": "#project \n\nhttps://arxiv.org/pdf/2506.06299\n\nExamines how coordinated networks of AI agents (\"AI swarms\") can manipulate democratic discourse and decision-making through sophisticated influence operations. The authors warn that unlike traditional botnets that simply repeat scripted messages, AI swarms can create thousands of distinct personas that adapt, learn from feedback, and coordinate autonomously to fabricate grassroots consensus and fragment shared reality.",
        "size": 28,
        "index": 324,
        "x": 734.1845256822567,
        "y": 681.6766011163681,
        "vy": -1.2242065545784204,
        "vx": 0.13586496766179135
      },
      "target": {
        "id": "Dawn Song",
        "title": "Dawn Song",
        "tags": [
          "person"
        ],
        "content": "#person \n\nhttps://dawnsong.io/",
        "size": 17.142857142857142,
        "index": 280,
        "x": 59.22529616244745,
        "y": 606.6469952544618,
        "vy": -0.8750545448179495,
        "vx": 0.15846918601654594
      },
      "index": 423
    },
    {
      "source": {
        "id": "How Malicious AI Swarms Can Threaten Democracy",
        "title": "How Malicious AI Swarms Can Threaten Democracy",
        "tags": [
          "project"
        ],
        "content": "#project \n\nhttps://arxiv.org/pdf/2506.06299\n\nExamines how coordinated networks of AI agents (\"AI swarms\") can manipulate democratic discourse and decision-making through sophisticated influence operations. The authors warn that unlike traditional botnets that simply repeat scripted messages, AI swarms can create thousands of distinct personas that adapt, learn from feedback, and coordinate autonomously to fabricate grassroots consensus and fragment shared reality.",
        "size": 28,
        "index": 324,
        "x": 734.1845256822567,
        "y": 681.6766011163681,
        "vy": -1.2242065545784204,
        "vx": 0.13586496766179135
      },
      "target": {
        "id": "Jonas Kunst",
        "title": "Jonas Kunst",
        "tags": [
          "person"
        ],
        "content": "#person \n",
        "size": 16.571428571428573,
        "index": 216,
        "x": 697.0061324055793,
        "y": 577.4824470578427,
        "vy": -1.2280476119437262,
        "vx": 0.027303571274386606
      },
      "index": 424
    },
    {
      "source": {
        "id": "Philip Tetlock",
        "title": "Philip Tetlock",
        "tags": [
          "person"
        ],
        "content": "#person",
        "size": 18.857142857142858,
        "index": 326,
        "x": -269.190694144188,
        "y": -121.36964146524063,
        "vy": -0.698755466059142,
        "vx": 0.0871105431534283
      },
      "target": {
        "id": "Forecasting Research Institute",
        "title": "Forecasting Research Institute",
        "tags": [
          "organization"
        ],
        "content": "#organization \n\nhttps://forecastingresearch.org/",
        "size": 18.857142857142858,
        "index": 317,
        "x": -153.6122917815696,
        "y": -87.61093230240742,
        "vy": -0.8005381090437106,
        "vx": 0.1603930148987801
      },
      "index": 425
    },
    {
      "source": {
        "id": "Plurality Mapping Project",
        "title": "Plurality Mapping Project",
        "tags": [
          "project"
        ],
        "content": "#project \n\nhttps://publish.obsidian.md/plurality-map",
        "size": 16.571428571428573,
        "index": 328,
        "x": 1580.7603953677365,
        "y": 1181.6513301396883,
        "vy": -1.6385883186223986,
        "vx": 0.5418611243723881
      },
      "target": {
        "id": "Plurality Institute",
        "title": "Plurality Institute",
        "tags": [
          "organization"
        ],
        "content": "#organization",
        "size": 20,
        "index": 364,
        "x": 1548.928220590021,
        "y": 1070.4694466931655,
        "vy": -1.5138983339043548,
        "vx": 0.41914174319685277
      },
      "index": 426
    },
    {
      "source": {
        "id": "AI4Democracy - How AI Can Be Used to Inform Policymaking?",
        "title": "AI4Democracy - How AI Can Be Used to Inform Policymaking?",
        "tags": [
          "project"
        ],
        "content": "#project \n\nhttps://ai.objectives.institute/blog/ai4democracy-paper-how-ai-can-be-used-to-inform-policymaking\n\nReport on how LLMs can be used to process large volumes of public input and aggregate opinions for policymakers, specifically with \"[Talk to the City](https://ai.objectives.institute/talk-to-the-city).\" The paper examines three real-world case studies (including one on [DAO](https://en.wikipedia.org/wiki/Decentralized_autonomous_organization) governance).",
        "size": 17.142857142857142,
        "index": 331,
        "x": 480.8942378747947,
        "y": -429.5254953059022,
        "vy": -1.0690146031786603,
        "vx": -0.098303519411448
      },
      "target": {
        "id": "Colleen McKenzie",
        "title": "Colleen McKenzie",
        "tags": [
          "person"
        ],
        "content": "#person",
        "size": 17.714285714285715,
        "index": 22,
        "x": 615.6457683620001,
        "y": -374.6339202786093,
        "vy": -1.1097477536713236,
        "vx": 0.021596753844054135
      },
      "index": 427
    },
    {
      "source": {
        "id": "AI4Democracy - How AI Can Be Used to Inform Policymaking?",
        "title": "AI4Democracy - How AI Can Be Used to Inform Policymaking?",
        "tags": [
          "project"
        ],
        "content": "#project \n\nhttps://ai.objectives.institute/blog/ai4democracy-paper-how-ai-can-be-used-to-inform-policymaking\n\nReport on how LLMs can be used to process large volumes of public input and aggregate opinions for policymakers, specifically with \"[Talk to the City](https://ai.objectives.institute/talk-to-the-city).\" The paper examines three real-world case studies (including one on [DAO](https://en.wikipedia.org/wiki/Decentralized_autonomous_organization) governance).",
        "size": 17.142857142857142,
        "index": 331,
        "x": 480.8942378747947,
        "y": -429.5254953059022,
        "vy": -1.0690146031786603,
        "vx": -0.098303519411448
      },
      "target": {
        "id": "Değer Turan",
        "title": "Değer Turan",
        "tags": [
          "person"
        ],
        "content": "#person",
        "size": 18.285714285714285,
        "index": 107,
        "x": 371.54679486309595,
        "y": -417.8764113264942,
        "vy": -1.0219808978726659,
        "vx": -0.10788831074881157
      },
      "index": 428
    },
    {
      "source": {
        "id": "Deep Research Bench - Evaluating AI Web Research Agents",
        "title": "Deep Research Bench - Evaluating AI Web Research Agents",
        "tags": [
          "project"
        ],
        "content": "#project \n\nhttps://arxiv.org/abs/2506.06287\nhttps://evals.futuresearch.ai/\n\nBenchmark for evaluating AI agents on complex, multi-step web research tasks that mirror real-world analytical work, using a frozen web environment to ensure consistent and repeatable evaluations. The benchmark tests research capabilities including strategic planning, source credibility assessment, systematic information gathering, and synthesis across contradictory sources through eight different task categories.",
        "size": 21.142857142857142,
        "index": 332,
        "x": -788.3209992852137,
        "y": -565.6379415639828,
        "vy": -0.37590759276131464,
        "vx": -0.2869994643202302
      },
      "target": {
        "id": "Nikos Bosse",
        "title": "Nikos Bosse",
        "tags": [
          "person"
        ],
        "content": "#person \n\nhttps://followtheargument.org/",
        "size": 18.285714285714285,
        "index": 243,
        "x": -843.047489587698,
        "y": -605.5760820500159,
        "vy": -0.3391263569445478,
        "vx": -0.31376832368339486
      },
      "index": 429
    },
    {
      "source": {
        "id": "Deep Research Bench - Evaluating AI Web Research Agents",
        "title": "Deep Research Bench - Evaluating AI Web Research Agents",
        "tags": [
          "project"
        ],
        "content": "#project \n\nhttps://arxiv.org/abs/2506.06287\nhttps://evals.futuresearch.ai/\n\nBenchmark for evaluating AI agents on complex, multi-step web research tasks that mirror real-world analytical work, using a frozen web environment to ensure consistent and repeatable evaluations. The benchmark tests research capabilities including strategic planning, source credibility assessment, systematic information gathering, and synthesis across contradictory sources through eight different task categories.",
        "size": 21.142857142857142,
        "index": 332,
        "x": -788.3209992852137,
        "y": -565.6379415639828,
        "vy": -0.37590759276131464,
        "vx": -0.2869994643202302
      },
      "target": {
        "id": "Jon Evans",
        "title": "Jon Evans",
        "tags": [
          "person"
        ],
        "content": "#person \n\nhttps://rezendi.com",
        "size": 18.285714285714285,
        "index": 341,
        "x": -610.9648619044706,
        "y": -618.6960073290362,
        "vy": -0.476093867230055,
        "vx": -0.32700571013017027
      },
      "index": 430
    },
    {
      "source": {
        "id": "Deep Research Bench - Evaluating AI Web Research Agents",
        "title": "Deep Research Bench - Evaluating AI Web Research Agents",
        "tags": [
          "project"
        ],
        "content": "#project \n\nhttps://arxiv.org/abs/2506.06287\nhttps://evals.futuresearch.ai/\n\nBenchmark for evaluating AI agents on complex, multi-step web research tasks that mirror real-world analytical work, using a frozen web environment to ensure consistent and repeatable evaluations. The benchmark tests research capabilities including strategic planning, source credibility assessment, systematic information gathering, and synthesis across contradictory sources through eight different task categories.",
        "size": 21.142857142857142,
        "index": 332,
        "x": -788.3209992852137,
        "y": -565.6379415639828,
        "vy": -0.37590759276131464,
        "vx": -0.2869994643202302
      },
      "target": {
        "id": "Robert Gambee",
        "title": "Robert Gambee",
        "tags": [
          "person"
        ],
        "content": "#person",
        "size": 17.142857142857142,
        "index": 171,
        "x": -746.1246973487597,
        "y": -677.5281127461095,
        "vy": -0.4225432452466162,
        "vx": -0.3776717305559615
      },
      "index": 431
    },
    {
      "source": {
        "id": "Deep Research Bench - Evaluating AI Web Research Agents",
        "title": "Deep Research Bench - Evaluating AI Web Research Agents",
        "tags": [
          "project"
        ],
        "content": "#project \n\nhttps://arxiv.org/abs/2506.06287\nhttps://evals.futuresearch.ai/\n\nBenchmark for evaluating AI agents on complex, multi-step web research tasks that mirror real-world analytical work, using a frozen web environment to ensure consistent and repeatable evaluations. The benchmark tests research capabilities including strategic planning, source credibility assessment, systematic information gathering, and synthesis across contradictory sources through eight different task categories.",
        "size": 21.142857142857142,
        "index": 332,
        "x": -788.3209992852137,
        "y": -565.6379415639828,
        "vy": -0.37590759276131464,
        "vx": -0.2869994643202302
      },
      "target": {
        "id": "Daniel Hnyk",
        "title": "Daniel Hnyk",
        "tags": [
          "person"
        ],
        "content": "#person",
        "size": 17.714285714285715,
        "index": 413,
        "x": -788.7242426021298,
        "y": -636.2606637787694,
        "vy": -0.37777225834080214,
        "vx": -0.34973522484873115
      },
      "index": 432
    },
    {
      "source": {
        "id": "Deep Research Bench - Evaluating AI Web Research Agents",
        "title": "Deep Research Bench - Evaluating AI Web Research Agents",
        "tags": [
          "project"
        ],
        "content": "#project \n\nhttps://arxiv.org/abs/2506.06287\nhttps://evals.futuresearch.ai/\n\nBenchmark for evaluating AI agents on complex, multi-step web research tasks that mirror real-world analytical work, using a frozen web environment to ensure consistent and repeatable evaluations. The benchmark tests research capabilities including strategic planning, source credibility assessment, systematic information gathering, and synthesis across contradictory sources through eight different task categories.",
        "size": 21.142857142857142,
        "index": 332,
        "x": -788.3209992852137,
        "y": -565.6379415639828,
        "vy": -0.37590759276131464,
        "vx": -0.2869994643202302
      },
      "target": {
        "id": "Peter Mühlbacher",
        "title": "Peter Mühlbacher",
        "tags": [
          "person"
        ],
        "content": "#person \n\nhttp://peter.muehlbacher.me/",
        "size": 18.285714285714285,
        "index": 238,
        "x": -857.2972832702691,
        "y": -654.0740118249536,
        "vy": -0.3400430834320815,
        "vx": -0.3453910216674885
      },
      "index": 433
    },
    {
      "source": {
        "id": "Deep Research Bench - Evaluating AI Web Research Agents",
        "title": "Deep Research Bench - Evaluating AI Web Research Agents",
        "tags": [
          "project"
        ],
        "content": "#project \n\nhttps://arxiv.org/abs/2506.06287\nhttps://evals.futuresearch.ai/\n\nBenchmark for evaluating AI agents on complex, multi-step web research tasks that mirror real-world analytical work, using a frozen web environment to ensure consistent and repeatable evaluations. The benchmark tests research capabilities including strategic planning, source credibility assessment, systematic information gathering, and synthesis across contradictory sources through eight different task categories.",
        "size": 21.142857142857142,
        "index": 332,
        "x": -788.3209992852137,
        "y": -565.6379415639828,
        "vy": -0.37590759276131464,
        "vx": -0.2869994643202302
      },
      "target": {
        "id": "Lawrence Phillips",
        "title": "Lawrence Phillips",
        "tags": [
          "person"
        ],
        "content": "#person",
        "size": 18.285714285714285,
        "index": 239,
        "x": -786.997452942158,
        "y": -671.6640862022364,
        "vy": -0.3955253133044403,
        "vx": -0.3629004531354729
      },
      "index": 434
    },
    {
      "source": {
        "id": "Deep Research Bench - Evaluating AI Web Research Agents",
        "title": "Deep Research Bench - Evaluating AI Web Research Agents",
        "tags": [
          "project"
        ],
        "content": "#project \n\nhttps://arxiv.org/abs/2506.06287\nhttps://evals.futuresearch.ai/\n\nBenchmark for evaluating AI agents on complex, multi-step web research tasks that mirror real-world analytical work, using a frozen web environment to ensure consistent and repeatable evaluations. The benchmark tests research capabilities including strategic planning, source credibility assessment, systematic information gathering, and synthesis across contradictory sources through eight different task categories.",
        "size": 21.142857142857142,
        "index": 332,
        "x": -788.3209992852137,
        "y": -565.6379415639828,
        "vy": -0.37590759276131464,
        "vx": -0.2869994643202302
      },
      "target": {
        "id": "Dan Schwarz",
        "title": "Dan Schwarz",
        "tags": [
          "person"
        ],
        "content": "#person \n\nSome writing about [Google's internal prediction markets](https://asteriskmag.com/issues/08/the-death-and-life-of-prediction-markets-at-google).",
        "size": 18.285714285714285,
        "index": 404,
        "x": -831.4781389544952,
        "y": -708.941876393585,
        "vy": -0.3657625739173525,
        "vx": -0.3779117138521553
      },
      "index": 435
    },
    {
      "source": {
        "id": "Deep Research Bench - Evaluating AI Web Research Agents",
        "title": "Deep Research Bench - Evaluating AI Web Research Agents",
        "tags": [
          "project"
        ],
        "content": "#project \n\nhttps://arxiv.org/abs/2506.06287\nhttps://evals.futuresearch.ai/\n\nBenchmark for evaluating AI agents on complex, multi-step web research tasks that mirror real-world analytical work, using a frozen web environment to ensure consistent and repeatable evaluations. The benchmark tests research capabilities including strategic planning, source credibility assessment, systematic information gathering, and synthesis across contradictory sources through eight different task categories.",
        "size": 21.142857142857142,
        "index": 332,
        "x": -788.3209992852137,
        "y": -565.6379415639828,
        "vy": -0.37590759276131464,
        "vx": -0.2869994643202302
      },
      "target": {
        "id": "Jack Wildman",
        "title": "Jack Wildman",
        "tags": [
          "person"
        ],
        "content": "#person",
        "size": 17.714285714285715,
        "index": 190,
        "x": -700.5637035103321,
        "y": -656.0808982602216,
        "vy": -0.44997863065831584,
        "vx": -0.36028652459093663
      },
      "index": 436
    },
    {
      "source": {
        "id": "Common Ground",
        "title": "Common Ground",
        "tags": [
          "project"
        ],
        "content": "#project \n\nhttps://www.dembrane.com/en-US/blog/report-openai-october-2023\n\nAI-powered platform for democratic deliberation that organizes people into small groups to discuss and vote on statements, with an AI moderator (powered by GPT-4) synthesizing new statements from their live video conversations with each other. Statements produced by each group are \"cross pollinated\" to each other and ultimately synthesized into final consensus statements.\n\nFunded by [OpenAI Democratic Inputs to AI](https://github.com/openai/democratic-inputs).",
        "size": 22.857142857142858,
        "index": 334,
        "x": 3769.4335996636014,
        "y": 1129.4195938439689,
        "vy": -2.001133633076939,
        "vx": 0.35636612599527046
      },
      "target": {
        "id": "Jorim Theuns",
        "title": "Jorim Theuns",
        "tags": [
          "person"
        ],
        "content": "#person \n\nhttps://jtheuns.com/",
        "size": 17.142857142857142,
        "index": 388,
        "x": 3740.476074878806,
        "y": 1048.2696899969626,
        "vy": -1.9598091972020897,
        "vx": 0.38373243366361753
      },
      "index": 437
    },
    {
      "source": {
        "id": "Common Ground",
        "title": "Common Ground",
        "tags": [
          "project"
        ],
        "content": "#project \n\nhttps://www.dembrane.com/en-US/blog/report-openai-october-2023\n\nAI-powered platform for democratic deliberation that organizes people into small groups to discuss and vote on statements, with an AI moderator (powered by GPT-4) synthesizing new statements from their live video conversations with each other. Statements produced by each group are \"cross pollinated\" to each other and ultimately synthesized into final consensus statements.\n\nFunded by [OpenAI Democratic Inputs to AI](https://github.com/openai/democratic-inputs).",
        "size": 22.857142857142858,
        "index": 334,
        "x": 3769.4335996636014,
        "y": 1129.4195938439689,
        "vy": -2.001133633076939,
        "vx": 0.35636612599527046
      },
      "target": {
        "id": "Evelien Nieuwenburg",
        "title": "Evelien Nieuwenburg",
        "tags": [
          "person"
        ],
        "content": "#person",
        "size": 17.142857142857142,
        "index": 16,
        "x": 3697.499433125433,
        "y": 1087.6426664248283,
        "vy": -1.9305523529355046,
        "vx": 0.4105224081999784
      },
      "index": 438
    },
    {
      "source": {
        "id": "Common Ground",
        "title": "Common Ground",
        "tags": [
          "project"
        ],
        "content": "#project \n\nhttps://www.dembrane.com/en-US/blog/report-openai-october-2023\n\nAI-powered platform for democratic deliberation that organizes people into small groups to discuss and vote on statements, with an AI moderator (powered by GPT-4) synthesizing new statements from their live video conversations with each other. Statements produced by each group are \"cross pollinated\" to each other and ultimately synthesized into final consensus statements.\n\nFunded by [OpenAI Democratic Inputs to AI](https://github.com/openai/democratic-inputs).",
        "size": 22.857142857142858,
        "index": 334,
        "x": 3769.4335996636014,
        "y": 1129.4195938439689,
        "vy": -2.001133633076939,
        "vx": 0.35636612599527046
      },
      "target": {
        "id": "Pepijn Verburg",
        "title": "Pepijn Verburg",
        "tags": [
          "person"
        ],
        "content": "#person",
        "size": 16.571428571428573,
        "index": 88,
        "x": 3750.832351524982,
        "y": 1187.1960449906455,
        "vy": -1.9589516781747354,
        "vx": 0.4700367543361778
      },
      "index": 439
    },
    {
      "source": {
        "id": "Common Ground",
        "title": "Common Ground",
        "tags": [
          "project"
        ],
        "content": "#project \n\nhttps://www.dembrane.com/en-US/blog/report-openai-october-2023\n\nAI-powered platform for democratic deliberation that organizes people into small groups to discuss and vote on statements, with an AI moderator (powered by GPT-4) synthesizing new statements from their live video conversations with each other. Statements produced by each group are \"cross pollinated\" to each other and ultimately synthesized into final consensus statements.\n\nFunded by [OpenAI Democratic Inputs to AI](https://github.com/openai/democratic-inputs).",
        "size": 22.857142857142858,
        "index": 334,
        "x": 3769.4335996636014,
        "y": 1129.4195938439689,
        "vy": -2.001133633076939,
        "vx": 0.35636612599527046
      },
      "target": {
        "id": "Lei Nelissen",
        "title": "Lei Nelissen",
        "tags": [
          "person"
        ],
        "content": "#person",
        "size": 16.571428571428573,
        "index": 1,
        "x": 3692.4366545678186,
        "y": 1142.4553761640127,
        "vy": -1.8894695625681632,
        "vx": 0.45954080612359194
      },
      "index": 440
    },
    {
      "source": {
        "id": "Common Ground",
        "title": "Common Ground",
        "tags": [
          "project"
        ],
        "content": "#project \n\nhttps://www.dembrane.com/en-US/blog/report-openai-october-2023\n\nAI-powered platform for democratic deliberation that organizes people into small groups to discuss and vote on statements, with an AI moderator (powered by GPT-4) synthesizing new statements from their live video conversations with each other. Statements produced by each group are \"cross pollinated\" to each other and ultimately synthesized into final consensus statements.\n\nFunded by [OpenAI Democratic Inputs to AI](https://github.com/openai/democratic-inputs).",
        "size": 22.857142857142858,
        "index": 334,
        "x": 3769.4335996636014,
        "y": 1129.4195938439689,
        "vy": -2.001133633076939,
        "vx": 0.35636612599527046
      },
      "target": {
        "id": "Brett Hennig",
        "title": "Brett Hennig",
        "tags": [
          "person"
        ],
        "content": "#person",
        "size": 16.571428571428573,
        "index": 357,
        "x": 3804.366191394412,
        "y": 1180.9557534355997,
        "vy": -2.080915851140268,
        "vx": 0.48373764616307036
      },
      "index": 441
    },
    {
      "source": {
        "id": "Common Ground",
        "title": "Common Ground",
        "tags": [
          "project"
        ],
        "content": "#project \n\nhttps://www.dembrane.com/en-US/blog/report-openai-october-2023\n\nAI-powered platform for democratic deliberation that organizes people into small groups to discuss and vote on statements, with an AI moderator (powered by GPT-4) synthesizing new statements from their live video conversations with each other. Statements produced by each group are \"cross pollinated\" to each other and ultimately synthesized into final consensus statements.\n\nFunded by [OpenAI Democratic Inputs to AI](https://github.com/openai/democratic-inputs).",
        "size": 22.857142857142858,
        "index": 334,
        "x": 3769.4335996636014,
        "y": 1129.4195938439689,
        "vy": -2.001133633076939,
        "vx": 0.35636612599527046
      },
      "target": {
        "id": "Rich Rippin",
        "title": "Rich Rippin",
        "tags": [
          "person"
        ],
        "content": "#person",
        "size": 16.571428571428573,
        "index": 182,
        "x": 3694.035488581077,
        "y": 1198.3347755218717,
        "vy": -1.8619995214735612,
        "vx": 0.5109609154083653
      },
      "index": 442
    },
    {
      "source": {
        "id": "Common Ground",
        "title": "Common Ground",
        "tags": [
          "project"
        ],
        "content": "#project \n\nhttps://www.dembrane.com/en-US/blog/report-openai-october-2023\n\nAI-powered platform for democratic deliberation that organizes people into small groups to discuss and vote on statements, with an AI moderator (powered by GPT-4) synthesizing new statements from their live video conversations with each other. Statements produced by each group are \"cross pollinated\" to each other and ultimately synthesized into final consensus statements.\n\nFunded by [OpenAI Democratic Inputs to AI](https://github.com/openai/democratic-inputs).",
        "size": 22.857142857142858,
        "index": 334,
        "x": 3769.4335996636014,
        "y": 1129.4195938439689,
        "vy": -2.001133633076939,
        "vx": 0.35636612599527046
      },
      "target": {
        "id": "Ran Haase",
        "title": "Ran Haase",
        "tags": [
          "person"
        ],
        "content": "#person",
        "size": 16.571428571428573,
        "index": 430,
        "x": 3785.769922012224,
        "y": 1234.0769184513977,
        "vy": -2.057489823282561,
        "vx": 0.5519048590181228
      },
      "index": 443
    },
    {
      "source": {
        "id": "Common Ground",
        "title": "Common Ground",
        "tags": [
          "project"
        ],
        "content": "#project \n\nhttps://www.dembrane.com/en-US/blog/report-openai-october-2023\n\nAI-powered platform for democratic deliberation that organizes people into small groups to discuss and vote on statements, with an AI moderator (powered by GPT-4) synthesizing new statements from their live video conversations with each other. Statements produced by each group are \"cross pollinated\" to each other and ultimately synthesized into final consensus statements.\n\nFunded by [OpenAI Democratic Inputs to AI](https://github.com/openai/democratic-inputs).",
        "size": 22.857142857142858,
        "index": 334,
        "x": 3769.4335996636014,
        "y": 1129.4195938439689,
        "vy": -2.001133633076939,
        "vx": 0.35636612599527046
      },
      "target": {
        "id": "Aldo de Moor",
        "title": "Aldo de Moor",
        "tags": [
          "person"
        ],
        "content": "#person",
        "size": 16.571428571428573,
        "index": 197,
        "x": 3730.597492092218,
        "y": 1241.4150514987286,
        "vy": -1.9302460434536304,
        "vx": 0.5851166380772149
      },
      "index": 444
    },
    {
      "source": {
        "id": "Common Ground",
        "title": "Common Ground",
        "tags": [
          "project"
        ],
        "content": "#project \n\nhttps://www.dembrane.com/en-US/blog/report-openai-october-2023\n\nAI-powered platform for democratic deliberation that organizes people into small groups to discuss and vote on statements, with an AI moderator (powered by GPT-4) synthesizing new statements from their live video conversations with each other. Statements produced by each group are \"cross pollinated\" to each other and ultimately synthesized into final consensus statements.\n\nFunded by [OpenAI Democratic Inputs to AI](https://github.com/openai/democratic-inputs).",
        "size": 22.857142857142858,
        "index": 334,
        "x": 3769.4335996636014,
        "y": 1129.4195938439689,
        "vy": -2.001133633076939,
        "vx": 0.35636612599527046
      },
      "target": {
        "id": "CeesJan Mol",
        "title": "CeesJan Mol",
        "tags": [
          "person"
        ],
        "content": "#person",
        "size": 16.571428571428573,
        "index": 325,
        "x": 3847.7837913166422,
        "y": 1109.426556128145,
        "vy": -2.041372250109483,
        "vx": 0.43425165571508595
      },
      "index": 445
    },
    {
      "source": {
        "id": "Common Ground",
        "title": "Common Ground",
        "tags": [
          "project"
        ],
        "content": "#project \n\nhttps://www.dembrane.com/en-US/blog/report-openai-october-2023\n\nAI-powered platform for democratic deliberation that organizes people into small groups to discuss and vote on statements, with an AI moderator (powered by GPT-4) synthesizing new statements from their live video conversations with each other. Statements produced by each group are \"cross pollinated\" to each other and ultimately synthesized into final consensus statements.\n\nFunded by [OpenAI Democratic Inputs to AI](https://github.com/openai/democratic-inputs).",
        "size": 22.857142857142858,
        "index": 334,
        "x": 3769.4335996636014,
        "y": 1129.4195938439689,
        "vy": -2.001133633076939,
        "vx": 0.35636612599527046
      },
      "target": {
        "id": "Naomi Esther",
        "title": "Naomi Esther",
        "tags": [
          "person"
        ],
        "content": "#person",
        "size": 16.571428571428573,
        "index": 380,
        "x": 3858.534690820754,
        "y": 1165.3731749697106,
        "vy": -2.0525162931989174,
        "vx": 0.5044894283414395
      },
      "index": 446
    },
    {
      "source": {
        "id": "Common Ground",
        "title": "Common Ground",
        "tags": [
          "project"
        ],
        "content": "#project \n\nhttps://www.dembrane.com/en-US/blog/report-openai-october-2023\n\nAI-powered platform for democratic deliberation that organizes people into small groups to discuss and vote on statements, with an AI moderator (powered by GPT-4) synthesizing new statements from their live video conversations with each other. Statements produced by each group are \"cross pollinated\" to each other and ultimately synthesized into final consensus statements.\n\nFunded by [OpenAI Democratic Inputs to AI](https://github.com/openai/democratic-inputs).",
        "size": 22.857142857142858,
        "index": 334,
        "x": 3769.4335996636014,
        "y": 1129.4195938439689,
        "vy": -2.001133633076939,
        "vx": 0.35636612599527046
      },
      "target": {
        "id": "Rolf Kleef",
        "title": "Rolf Kleef",
        "tags": [
          "person"
        ],
        "content": "#person",
        "size": 16.571428571428573,
        "index": 245,
        "x": 3843.953645623031,
        "y": 1221.4286225042035,
        "vy": -2.0829605864283147,
        "vx": 0.5168263095023578
      },
      "index": 447
    },
    {
      "source": {
        "id": "Common Ground",
        "title": "Common Ground",
        "tags": [
          "project"
        ],
        "content": "#project \n\nhttps://www.dembrane.com/en-US/blog/report-openai-october-2023\n\nAI-powered platform for democratic deliberation that organizes people into small groups to discuss and vote on statements, with an AI moderator (powered by GPT-4) synthesizing new statements from their live video conversations with each other. Statements produced by each group are \"cross pollinated\" to each other and ultimately synthesized into final consensus statements.\n\nFunded by [OpenAI Democratic Inputs to AI](https://github.com/openai/democratic-inputs).",
        "size": 22.857142857142858,
        "index": 334,
        "x": 3769.4335996636014,
        "y": 1129.4195938439689,
        "vy": -2.001133633076939,
        "vx": 0.35636612599527046
      },
      "target": {
        "id": "Bram Delisse",
        "title": "Bram Delisse",
        "tags": [
          "person"
        ],
        "content": "#person",
        "size": 16.571428571428573,
        "index": 195,
        "x": 3810.4621239709318,
        "y": 1068.047576495609,
        "vy": -1.9700192000287036,
        "vx": 0.376392718988028
      },
      "index": 448
    },
    {
      "source": {
        "id": "Echo",
        "title": "Echo",
        "tags": [
          "project"
        ],
        "content": "#project \n\nhttps://www.dembrane.com/en-US/products/echo\nhttps://github.com/dembrane/echo\n\nECHO is a transcription, analysis, and reporting platform designed for multilingual conversations and stakeholder engagement scenarios. It can capture multiple simultaneous conversations in real-time, transcribe them across multiple languages, and use AI to analyze the content and generate insights from the discussions.",
        "size": 16.571428571428573,
        "index": 335,
        "x": 3612.5665593855706,
        "y": 979.1067686717648,
        "vy": -1.7745497667673695,
        "vx": 0.18551029446696282
      },
      "target": {
        "id": "Dembrane",
        "title": "Dembrane",
        "tags": [
          "organization"
        ],
        "content": "#organization \n\nhttps://www.dembrane.com\n\nFunded (in part) by the European Union via the [AI4Deliberation](https://www.ai4dproject.eu/) project.",
        "size": 17.714285714285715,
        "index": 336,
        "x": 3670.5319699200368,
        "y": 1017.8958794711315,
        "vy": -1.8573349240868908,
        "vx": 0.2592679715006362
      },
      "index": 449
    },
    {
      "source": {
        "id": "Supernotes",
        "title": "Supernotes",
        "tags": [
          "project"
        ],
        "content": "#project \n\nhttps://dl.acm.org/doi/pdf/10.1145/3696410.3714934\n\nAI system that synthesizes multiple community-written fact-checking notes into single, more effective notes that build consensus among diverse users. The system uses an LLM to generate many candidate notes from existing human-written notes, then employs a machine learning model trained on millions of historical ratings to predict which candidates would be most helpful to users with different viewpoints.",
        "size": 18.285714285714285,
        "index": 338,
        "x": 1244.5536392225954,
        "y": 442.2656807259026,
        "vy": -1.3408058114158516,
        "vx": 0.22606212780653964
      },
      "target": {
        "id": "Soham De",
        "title": "Soham De",
        "tags": [
          "person"
        ],
        "content": "#person",
        "size": 16.571428571428573,
        "index": 435,
        "x": 1238.7976786174017,
        "y": 309.8007191828333,
        "vy": -1.3942903559983337,
        "vx": 0.11066162196126357
      },
      "index": 450
    },
    {
      "source": {
        "id": "Supernotes",
        "title": "Supernotes",
        "tags": [
          "project"
        ],
        "content": "#project \n\nhttps://dl.acm.org/doi/pdf/10.1145/3696410.3714934\n\nAI system that synthesizes multiple community-written fact-checking notes into single, more effective notes that build consensus among diverse users. The system uses an LLM to generate many candidate notes from existing human-written notes, then employs a machine learning model trained on millions of historical ratings to predict which candidates would be most helpful to users with different viewpoints.",
        "size": 18.285714285714285,
        "index": 338,
        "x": 1244.5536392225954,
        "y": 442.2656807259026,
        "vy": -1.3408058114158516,
        "vx": 0.22606212780653964
      },
      "target": {
        "id": "Michiel Bakker",
        "title": "Michiel Bakker",
        "tags": [
          "person"
        ],
        "content": "#person \n\nhttps://miba.dev/",
        "size": 21.142857142857142,
        "index": 284,
        "x": 1225.501866244462,
        "y": 730.5079873536822,
        "vy": -1.2714527484014098,
        "vx": 0.27625785273055353
      },
      "index": 451
    },
    {
      "source": {
        "id": "Supernotes",
        "title": "Supernotes",
        "tags": [
          "project"
        ],
        "content": "#project \n\nhttps://dl.acm.org/doi/pdf/10.1145/3696410.3714934\n\nAI system that synthesizes multiple community-written fact-checking notes into single, more effective notes that build consensus among diverse users. The system uses an LLM to generate many candidate notes from existing human-written notes, then employs a machine learning model trained on millions of historical ratings to predict which candidates would be most helpful to users with different viewpoints.",
        "size": 18.285714285714285,
        "index": 338,
        "x": 1244.5536392225954,
        "y": 442.2656807259026,
        "vy": -1.3408058114158516,
        "vx": 0.22606212780653964
      },
      "target": {
        "id": "Jay Baxter",
        "title": "Jay Baxter",
        "tags": [
          "person"
        ],
        "content": "#person \n\nhttps://jaybaxter.net/",
        "size": 17.142857142857142,
        "index": 359,
        "x": 1293.9304234936697,
        "y": 292.7328541518838,
        "vy": -1.4439751239764669,
        "vx": 0.09536195026282479
      },
      "index": 452
    },
    {
      "source": {
        "id": "Supernotes",
        "title": "Supernotes",
        "tags": [
          "project"
        ],
        "content": "#project \n\nhttps://dl.acm.org/doi/pdf/10.1145/3696410.3714934\n\nAI system that synthesizes multiple community-written fact-checking notes into single, more effective notes that build consensus among diverse users. The system uses an LLM to generate many candidate notes from existing human-written notes, then employs a machine learning model trained on millions of historical ratings to predict which candidates would be most helpful to users with different viewpoints.",
        "size": 18.285714285714285,
        "index": 338,
        "x": 1244.5536392225954,
        "y": 442.2656807259026,
        "vy": -1.3408058114158516,
        "vx": 0.22606212780653964
      },
      "target": {
        "id": "Martin Saveski",
        "title": "Martin Saveski",
        "tags": [
          "person"
        ],
        "content": "#person",
        "size": 16.571428571428573,
        "index": 308,
        "x": 1207.126326621334,
        "y": 356.73600567435466,
        "vy": -1.4927494791834877,
        "vx": 0.014308531866461749
      },
      "index": 453
    },
    {
      "source": {
        "id": "Accelerated Preference Elicitation with LLM-Based Proxies",
        "title": "Accelerated Preference Elicitation with LLM-Based Proxies",
        "tags": [
          "project"
        ],
        "content": "#project \n\nhttps://arxiv.org/pdf/2501.14625\n\nAI system that help people communicate their preferences more efficiently in complex auctions by using natural language instead of technical queries. The system uses LLM-powered \"proxies\" that learn what people want through conversational interactions and can infer preferences for items not explicitly discussed.",
        "size": 18.285714285714285,
        "index": 339,
        "x": 1821.5847704294936,
        "y": 212.07982179665802,
        "vy": -1.702243069154774,
        "vx": 0.16090282927444247
      },
      "target": {
        "id": "David Huang",
        "title": "David Huang",
        "tags": [
          "person"
        ],
        "content": "#person \n",
        "size": 16.571428571428573,
        "index": 102,
        "x": 1914.5906312194227,
        "y": 182.3270762034121,
        "vy": -1.8675853972373344,
        "vx": 0.1250632537823081
      },
      "index": 454
    },
    {
      "source": {
        "id": "Accelerated Preference Elicitation with LLM-Based Proxies",
        "title": "Accelerated Preference Elicitation with LLM-Based Proxies",
        "tags": [
          "project"
        ],
        "content": "#project \n\nhttps://arxiv.org/pdf/2501.14625\n\nAI system that help people communicate their preferences more efficiently in complex auctions by using natural language instead of technical queries. The system uses LLM-powered \"proxies\" that learn what people want through conversational interactions and can infer preferences for items not explicitly discussed.",
        "size": 18.285714285714285,
        "index": 339,
        "x": 1821.5847704294936,
        "y": 212.07982179665802,
        "vy": -1.702243069154774,
        "vx": 0.16090282927444247
      },
      "target": {
        "id": "Francisco Marmolejo-Cossío",
        "title": "Francisco Marmolejo-Cossío",
        "tags": [
          "person"
        ],
        "content": "#person \n",
        "size": 16.571428571428573,
        "index": 272,
        "x": 1876.5959770832362,
        "y": 128.6832906753345,
        "vy": -1.8285690562314785,
        "vx": 0.05540064995450731
      },
      "index": 455
    },
    {
      "source": {
        "id": "Accelerated Preference Elicitation with LLM-Based Proxies",
        "title": "Accelerated Preference Elicitation with LLM-Based Proxies",
        "tags": [
          "project"
        ],
        "content": "#project \n\nhttps://arxiv.org/pdf/2501.14625\n\nAI system that help people communicate their preferences more efficiently in complex auctions by using natural language instead of technical queries. The system uses LLM-powered \"proxies\" that learn what people want through conversational interactions and can infer preferences for items not explicitly discussed.",
        "size": 18.285714285714285,
        "index": 339,
        "x": 1821.5847704294936,
        "y": 212.07982179665802,
        "vy": -1.702243069154774,
        "vx": 0.16090282927444247
      },
      "target": {
        "id": "Edwin Lock",
        "title": "Edwin Lock",
        "tags": [
          "person"
        ],
        "content": "#person \n",
        "size": 16.571428571428573,
        "index": 415,
        "x": 1933.7343227307829,
        "y": 127.96908875356733,
        "vy": -1.896385839159013,
        "vx": 0.05459826260134824
      },
      "index": 456
    },
    {
      "source": {
        "id": "Accelerated Preference Elicitation with LLM-Based Proxies",
        "title": "Accelerated Preference Elicitation with LLM-Based Proxies",
        "tags": [
          "project"
        ],
        "content": "#project \n\nhttps://arxiv.org/pdf/2501.14625\n\nAI system that help people communicate their preferences more efficiently in complex auctions by using natural language instead of technical queries. The system uses LLM-powered \"proxies\" that learn what people want through conversational interactions and can infer preferences for items not explicitly discussed.",
        "size": 18.285714285714285,
        "index": 339,
        "x": 1821.5847704294936,
        "y": 212.07982179665802,
        "vy": -1.702243069154774,
        "vx": 0.16090282927444247
      },
      "target": {
        "id": "David Parkes",
        "title": "David Parkes",
        "tags": [
          "person"
        ],
        "content": "#person",
        "size": 18.285714285714285,
        "index": 133,
        "x": 1720.2221261795037,
        "y": 282.75903684678264,
        "vy": -1.5768079791609506,
        "vx": 0.16009266454570117
      },
      "index": 457
    },
    {
      "source": {
        "id": "LLM-Powered Preference Elicitation in Combinatorial Assignment",
        "title": "LLM-Powered Preference Elicitation in Combinatorial Assignment",
        "tags": [
          "project"
        ],
        "content": "#project \n\nhttps://arxiv.org/pdf/2502.10308\n\nAI system that allows people to express complex preferences through natural language rather than answering technical queries in allocation mechanisms. For example, in course registration, students could provide a single free-text description of their preferences (e.g., \"I prefer courses scheduled closely together\" or \"Course A and B complement each other\"), and an LLM proxy would answer comparison queries on their behalf. Experiments testing the system were run using LLM-simulated people.",
        "size": 19.428571428571427,
        "index": 340,
        "x": 1811.4457162469735,
        "y": 87.51711177245846,
        "vy": -1.544661148103091,
        "vx": -0.007825266648511508
      },
      "target": {
        "id": "Ermis Soumalias",
        "title": "Ermis Soumalias",
        "tags": [
          "person"
        ],
        "content": "#person ",
        "size": 16.571428571428573,
        "index": 377,
        "x": 1863.5724705216303,
        "y": 3.587053537558891,
        "vy": -1.7122416124317823,
        "vx": -0.13799260400903532
      },
      "index": 458
    },
    {
      "source": {
        "id": "LLM-Powered Preference Elicitation in Combinatorial Assignment",
        "title": "LLM-Powered Preference Elicitation in Combinatorial Assignment",
        "tags": [
          "project"
        ],
        "content": "#project \n\nhttps://arxiv.org/pdf/2502.10308\n\nAI system that allows people to express complex preferences through natural language rather than answering technical queries in allocation mechanisms. For example, in course registration, students could provide a single free-text description of their preferences (e.g., \"I prefer courses scheduled closely together\" or \"Course A and B complement each other\"), and an LLM proxy would answer comparison queries on their behalf. Experiments testing the system were run using LLM-simulated people.",
        "size": 19.428571428571427,
        "index": 340,
        "x": 1811.4457162469735,
        "y": 87.51711177245846,
        "vy": -1.544661148103091,
        "vx": -0.007825266648511508
      },
      "target": {
        "id": "Yanchen Jiang",
        "title": "Yanchen Jiang",
        "tags": [
          "person"
        ],
        "content": "#person \n",
        "size": 16.571428571428573,
        "index": 187,
        "x": 1810.0639307985211,
        "y": -13.16190561814123,
        "vy": -1.8288132434062896,
        "vx": -0.08678315366296974
      },
      "index": 459
    },
    {
      "source": {
        "id": "LLM-Powered Preference Elicitation in Combinatorial Assignment",
        "title": "LLM-Powered Preference Elicitation in Combinatorial Assignment",
        "tags": [
          "project"
        ],
        "content": "#project \n\nhttps://arxiv.org/pdf/2502.10308\n\nAI system that allows people to express complex preferences through natural language rather than answering technical queries in allocation mechanisms. For example, in course registration, students could provide a single free-text description of their preferences (e.g., \"I prefer courses scheduled closely together\" or \"Course A and B complement each other\"), and an LLM proxy would answer comparison queries on their behalf. Experiments testing the system were run using LLM-simulated people.",
        "size": 19.428571428571427,
        "index": 340,
        "x": 1811.4457162469735,
        "y": 87.51711177245846,
        "vy": -1.544661148103091,
        "vx": -0.007825266648511508
      },
      "target": {
        "id": "Kehang Zhu",
        "title": "Kehang Zhu",
        "tags": [
          "person"
        ],
        "content": "#person \n",
        "size": 16.571428571428573,
        "index": 93,
        "x": 1849.9174012120527,
        "y": -53.883251834481996,
        "vy": -1.7738102197688521,
        "vx": -0.017814827956796852
      },
      "index": 460
    },
    {
      "source": {
        "id": "LLM-Powered Preference Elicitation in Combinatorial Assignment",
        "title": "LLM-Powered Preference Elicitation in Combinatorial Assignment",
        "tags": [
          "project"
        ],
        "content": "#project \n\nhttps://arxiv.org/pdf/2502.10308\n\nAI system that allows people to express complex preferences through natural language rather than answering technical queries in allocation mechanisms. For example, in course registration, students could provide a single free-text description of their preferences (e.g., \"I prefer courses scheduled closely together\" or \"Course A and B complement each other\"), and an LLM proxy would answer comparison queries on their behalf. Experiments testing the system were run using LLM-simulated people.",
        "size": 19.428571428571427,
        "index": 340,
        "x": 1811.4457162469735,
        "y": 87.51711177245846,
        "vy": -1.544661148103091,
        "vx": -0.007825266648511508
      },
      "target": {
        "id": "Michael Curry",
        "title": "Michael Curry",
        "tags": [
          "person"
        ],
        "content": "#person \n",
        "size": 16.571428571428573,
        "index": 373,
        "x": 1919.060025933745,
        "y": 17.24082010191114,
        "vy": -1.5839083316548166,
        "vx": -0.1695144846028996
      },
      "index": 461
    },
    {
      "source": {
        "id": "LLM-Powered Preference Elicitation in Combinatorial Assignment",
        "title": "LLM-Powered Preference Elicitation in Combinatorial Assignment",
        "tags": [
          "project"
        ],
        "content": "#project \n\nhttps://arxiv.org/pdf/2502.10308\n\nAI system that allows people to express complex preferences through natural language rather than answering technical queries in allocation mechanisms. For example, in course registration, students could provide a single free-text description of their preferences (e.g., \"I prefer courses scheduled closely together\" or \"Course A and B complement each other\"), and an LLM proxy would answer comparison queries on their behalf. Experiments testing the system were run using LLM-simulated people.",
        "size": 19.428571428571427,
        "index": 340,
        "x": 1811.4457162469735,
        "y": 87.51711177245846,
        "vy": -1.544661148103091,
        "vx": -0.007825266648511508
      },
      "target": {
        "id": "Sven Seuken",
        "title": "Sven Seuken",
        "tags": [
          "person"
        ],
        "content": "#person \n",
        "size": 16.571428571428573,
        "index": 96,
        "x": 1903.8297731579873,
        "y": -38.7864793208898,
        "vy": -1.6752578198263395,
        "vx": -0.03583841021908137
      },
      "index": 462
    },
    {
      "source": {
        "id": "LLM-Powered Preference Elicitation in Combinatorial Assignment",
        "title": "LLM-Powered Preference Elicitation in Combinatorial Assignment",
        "tags": [
          "project"
        ],
        "content": "#project \n\nhttps://arxiv.org/pdf/2502.10308\n\nAI system that allows people to express complex preferences through natural language rather than answering technical queries in allocation mechanisms. For example, in course registration, students could provide a single free-text description of their preferences (e.g., \"I prefer courses scheduled closely together\" or \"Course A and B complement each other\"), and an LLM proxy would answer comparison queries on their behalf. Experiments testing the system were run using LLM-simulated people.",
        "size": 19.428571428571427,
        "index": 340,
        "x": 1811.4457162469735,
        "y": 87.51711177245846,
        "vy": -1.544661148103091,
        "vx": -0.007825266648511508
      },
      "target": {
        "id": "David Parkes",
        "title": "David Parkes",
        "tags": [
          "person"
        ],
        "content": "#person",
        "size": 18.285714285714285,
        "index": 133,
        "x": 1720.2221261795037,
        "y": 282.75903684678264,
        "vy": -1.5768079791609506,
        "vx": 0.16009266454570117
      },
      "index": 463
    },
    {
      "source": {
        "id": "Jon Evans",
        "title": "Jon Evans",
        "tags": [
          "person"
        ],
        "content": "#person \n\nhttps://rezendi.com",
        "size": 18.285714285714285,
        "index": 341,
        "x": -610.9648619044706,
        "y": -618.6960073290362,
        "vy": -0.476093867230055,
        "vx": -0.32700571013017027
      },
      "target": {
        "id": "Future Search",
        "title": "Future Search",
        "tags": [
          "organization"
        ],
        "content": "#organization \n\nhttps://futuresearch.ai/",
        "size": 22.285714285714285,
        "index": 94,
        "x": -717.8526553292105,
        "y": -719.7276361443852,
        "vy": -0.4404383940953807,
        "vx": -0.3824839533244458
      },
      "index": 464
    },
    {
      "source": {
        "id": "Jon Evans",
        "title": "Jon Evans",
        "tags": [
          "person"
        ],
        "content": "#person \n\nhttps://rezendi.com",
        "size": 18.285714285714285,
        "index": 341,
        "x": -610.9648619044706,
        "y": -618.6960073290362,
        "vy": -0.476093867230055,
        "vx": -0.32700571013017027
      },
      "target": {
        "id": "Metaculus",
        "title": "Metaculus",
        "tags": [
          "organization"
        ],
        "content": "#organization\n\nhttps://www.metaculus.com/",
        "size": 20,
        "index": 126,
        "x": -291.65109336863395,
        "y": -589.3419401920876,
        "vy": -0.7257043832009319,
        "vx": -0.15741051377760798
      },
      "index": 465
    },
    {
      "source": {
        "id": "Prompt Engineering Large Language Models’ Forecasting Capabilities",
        "title": "Prompt Engineering Large Language Models’ Forecasting Capabilities",
        "tags": [
          "project"
        ],
        "content": "#project \n\nhttps://arxiv.org/pdf/2506.01578\n\nPaper presents a systematic study of prompt engineering techniques for improving large language models' forecasting capabilities. The researchers tested 38 different prompts across four major LLMs using 100 forecasting questions, followed by a second study with compound and professionally-designed prompts. Surprisingly, they found that most prompt modifications had negligible or even negative effects on forecasting accuracy, with some techniques like explicit Bayesian reasoning actually making predictions worse.",
        "size": 18.857142857142858,
        "index": 347,
        "x": -349.42584004628867,
        "y": -215.63530292055648,
        "vy": -0.6993735214505079,
        "vx": 0.0487528223764443
      },
      "target": {
        "id": "Philipp Schoenegger",
        "title": "Philipp Schoenegger",
        "tags": [
          "person"
        ],
        "content": "#person \n\nhttps://philipp-schoenegger.weebly.com/",
        "size": 19.428571428571427,
        "index": 120,
        "x": -308.0505670602049,
        "y": -355.92667066361173,
        "vy": -0.7160490223836882,
        "vx": 0.03780161064368572
      },
      "index": 466
    },
    {
      "source": {
        "id": "Prompt Engineering Large Language Models’ Forecasting Capabilities",
        "title": "Prompt Engineering Large Language Models’ Forecasting Capabilities",
        "tags": [
          "project"
        ],
        "content": "#project \n\nhttps://arxiv.org/pdf/2506.01578\n\nPaper presents a systematic study of prompt engineering techniques for improving large language models' forecasting capabilities. The researchers tested 38 different prompts across four major LLMs using 100 forecasting questions, followed by a second study with compound and professionally-designed prompts. Surprisingly, they found that most prompt modifications had negligible or even negative effects on forecasting accuracy, with some techniques like explicit Bayesian reasoning actually making predictions worse.",
        "size": 18.857142857142858,
        "index": 347,
        "x": -349.42584004628867,
        "y": -215.63530292055648,
        "vy": -0.6993735214505079,
        "vx": 0.0487528223764443
      },
      "target": {
        "id": "Cameron Jones",
        "title": "Cameron Jones",
        "tags": [
          "person"
        ],
        "content": "#person",
        "size": 16.571428571428573,
        "index": 115,
        "x": -441.50774791523173,
        "y": -219.45425505903108,
        "vy": -0.673809033264249,
        "vx": 0.05783591825726862
      },
      "index": 467
    },
    {
      "source": {
        "id": "Prompt Engineering Large Language Models’ Forecasting Capabilities",
        "title": "Prompt Engineering Large Language Models’ Forecasting Capabilities",
        "tags": [
          "project"
        ],
        "content": "#project \n\nhttps://arxiv.org/pdf/2506.01578\n\nPaper presents a systematic study of prompt engineering techniques for improving large language models' forecasting capabilities. The researchers tested 38 different prompts across four major LLMs using 100 forecasting questions, followed by a second study with compound and professionally-designed prompts. Surprisingly, they found that most prompt modifications had negligible or even negative effects on forecasting accuracy, with some techniques like explicit Bayesian reasoning actually making predictions worse.",
        "size": 18.857142857142858,
        "index": 347,
        "x": -349.42584004628867,
        "y": -215.63530292055648,
        "vy": -0.6993735214505079,
        "vx": 0.0487528223764443
      },
      "target": {
        "id": "Philip Tetlock",
        "title": "Philip Tetlock",
        "tags": [
          "person"
        ],
        "content": "#person",
        "size": 18.857142857142858,
        "index": 326,
        "x": -269.190694144188,
        "y": -121.36964146524063,
        "vy": -0.698755466059142,
        "vx": 0.0871105431534283
      },
      "index": 468
    },
    {
      "source": {
        "id": "Prompt Engineering Large Language Models’ Forecasting Capabilities",
        "title": "Prompt Engineering Large Language Models’ Forecasting Capabilities",
        "tags": [
          "project"
        ],
        "content": "#project \n\nhttps://arxiv.org/pdf/2506.01578\n\nPaper presents a systematic study of prompt engineering techniques for improving large language models' forecasting capabilities. The researchers tested 38 different prompts across four major LLMs using 100 forecasting questions, followed by a second study with compound and professionally-designed prompts. Surprisingly, they found that most prompt modifications had negligible or even negative effects on forecasting accuracy, with some techniques like explicit Bayesian reasoning actually making predictions worse.",
        "size": 18.857142857142858,
        "index": 347,
        "x": -349.42584004628867,
        "y": -215.63530292055648,
        "vy": -0.6993735214505079,
        "vx": 0.0487528223764443
      },
      "target": {
        "id": "Barbara Mellers",
        "title": "Barbara Mellers",
        "tags": [
          "person"
        ],
        "content": "#person",
        "size": 16.571428571428573,
        "index": 422,
        "x": -419.8277199249365,
        "y": -166.58382412067013,
        "vy": -0.6759889752677909,
        "vx": 0.06315299400717928
      },
      "index": 469
    },
    {
      "source": {
        "id": "Ted Suzman",
        "title": "Ted Suzman",
        "tags": [
          "person"
        ],
        "content": "#person",
        "size": 17.142857142857142,
        "index": 350,
        "x": 2095.8625924779726,
        "y": 794.9387059636165,
        "vy": -1.6339108176425123,
        "vx": 0.3784470371848435
      },
      "target": {
        "id": "The Computational Democracy Project",
        "title": "The Computational Democracy Project",
        "tags": [
          "organization"
        ],
        "content": "#organization \n\nhttps://compdemocracy.org/",
        "size": 19.428571428571427,
        "index": 138,
        "x": 2057.334021465157,
        "y": 849.740764420989,
        "vy": -1.5779545596273115,
        "vx": 0.3854928364433583
      },
      "index": 470
    },
    {
      "source": {
        "id": "Ozzie Gooen",
        "title": "Ozzie Gooen",
        "tags": [
          "person"
        ],
        "content": "#person",
        "size": 18.285714285714285,
        "index": 352,
        "x": -795.6299935195096,
        "y": -1201.5289699537964,
        "vy": -0.36410862655798687,
        "vx": -0.5257118812416671
      },
      "target": {
        "id": "Quantified Uncertainty Research Institute",
        "title": "Quantified Uncertainty Research Institute",
        "tags": [
          "organization"
        ],
        "content": "#organization \n\nhttps://quantifieduncertainty.org/",
        "size": 17.142857142857142,
        "index": 146,
        "x": -798.2335318785231,
        "y": -1310.1155250287434,
        "vy": -0.36104964512126025,
        "vx": -0.5790503716227251
      },
      "index": 471
    },
    {
      "source": {
        "id": "Ryan Lowe",
        "title": "Ryan Lowe",
        "tags": [
          "person"
        ],
        "content": "#person",
        "size": 17.142857142857142,
        "index": 354,
        "x": 1279.688024262066,
        "y": -362.91454807188126,
        "vy": -1.3197815536317754,
        "vx": -0.10514754429952329
      },
      "target": {
        "id": "Meaning Alignment Institute",
        "title": "Meaning Alignment Institute",
        "tags": [
          "organization"
        ],
        "content": "#organization \n\nhttps://www.meaningalignment.org/",
        "size": 17.714285714285715,
        "index": 310,
        "x": 1258.0435242093358,
        "y": -308.1817574200967,
        "vy": -1.311900518523638,
        "vx": -0.08522925964609504
      },
      "index": 472
    },
    {
      "source": {
        "id": "Justin Stimatze",
        "title": "Justin Stimatze",
        "tags": [
          "person"
        ],
        "content": "#person",
        "size": 17.142857142857142,
        "index": 356,
        "x": 662.5538728575052,
        "y": -411.9857351861423,
        "vy": -1.1393261113953697,
        "vx": -0.18924034901002387
      },
      "target": {
        "id": "AI Objectives Institute",
        "title": "AI Objectives Institute",
        "tags": [
          "organization"
        ],
        "content": "#organization \n\nhttps://ai.objectives.institute/",
        "size": 20.57142857142857,
        "index": 304,
        "x": 665.7615191436854,
        "y": -247.07081557119588,
        "vy": -1.1515300453530217,
        "vx": -0.07444975689524554
      },
      "index": 473
    },
    {
      "source": {
        "id": "Nandika Donthi",
        "title": "Nandika Donthi",
        "tags": [
          "person"
        ],
        "content": "#person",
        "size": 17.142857142857142,
        "index": 358,
        "x": 1424.6119078058373,
        "y": 1043.8070404484067,
        "vy": -1.145949676023276,
        "vx": 0.27030601690505907
      },
      "target": {
        "id": "Reddit",
        "title": "Reddit",
        "tags": [
          "organization"
        ],
        "content": "#organization",
        "size": 16.571428571428573,
        "index": 257,
        "x": 1444.619481491618,
        "y": 1191.50396883749,
        "vy": -1.069019289355356,
        "vx": 0.35826758149817045
      },
      "index": 474
    },
    {
      "source": {
        "id": "Jay Baxter",
        "title": "Jay Baxter",
        "tags": [
          "person"
        ],
        "content": "#person \n\nhttps://jaybaxter.net/",
        "size": 17.142857142857142,
        "index": 359,
        "x": 1293.9304234936697,
        "y": 292.7328541518838,
        "vy": -1.4439751239764669,
        "vx": 0.09536195026282479
      },
      "target": {
        "id": "X Community Notes",
        "title": "X Community Notes",
        "tags": [
          "organization"
        ],
        "content": "#organization \n\nhttps://communitynotes.x.com/guide/en/welcome",
        "size": 16.571428571428573,
        "index": 321,
        "x": 1317.2122904651371,
        "y": 183.28666940004246,
        "vy": -1.3987292615268805,
        "vx": 0.02200687733118558
      },
      "index": 475
    },
    {
      "source": {
        "id": "Elicit Tool",
        "title": "Elicit Tool",
        "tags": [
          "project"
        ],
        "content": "#project \n\nhttps://elicit.com/\n\nAI research assistant that helps users conduct literature reviews by searching academic papers, extracting key findings, and synthesizing research evidence to answer specific questions.",
        "size": 17.714285714285715,
        "index": 361,
        "x": -1934.779592795771,
        "y": 498.89241080967696,
        "vy": -0.3068112062450581,
        "vx": 0.05936972970761678
      },
      "target": {
        "id": "Elicit",
        "title": "Elicit",
        "tags": [
          "organization"
        ],
        "content": "#organization \n\nhttps://elicit.com/",
        "size": 20.57142857142857,
        "index": 218,
        "x": -2027.2873240845167,
        "y": 375.96539772345454,
        "vy": -0.29711133317878174,
        "vx": 0.26985144319061544
      },
      "index": 476
    },
    {
      "source": {
        "id": "Elicit Tool",
        "title": "Elicit Tool",
        "tags": [
          "project"
        ],
        "content": "#project \n\nhttps://elicit.com/\n\nAI research assistant that helps users conduct literature reviews by searching academic papers, extracting key findings, and synthesizing research evidence to answer specific questions.",
        "size": 17.714285714285715,
        "index": 361,
        "x": -1934.779592795771,
        "y": 498.89241080967696,
        "vy": -0.3068112062450581,
        "vx": 0.05936972970761678
      },
      "target": {
        "id": "Ought",
        "title": "Ought",
        "tags": [
          "organization"
        ],
        "content": "#organization \n\nhttps://ought.org/ (website is old)\n\nOught 1.0 has effectively become what is now Elicit (the company) continuing development on Elicit (the tool). Ought 2.0 has not yet done any public work.",
        "size": 23.42857142857143,
        "index": 306,
        "x": -1888.3722954977052,
        "y": 453.0511056583274,
        "vy": -0.2547035865836596,
        "vx": 0.0471838457037637
      },
      "index": 477
    },
    {
      "source": {
        "id": "Language models can reduce asymmetry in information markets",
        "title": "Language models can reduce asymmetry in information markets",
        "tags": [
          "project"
        ],
        "content": "#project \n\nhttps://arxiv.org/pdf/2403.14443\n\nPaper presents the \"Information Bazaar,\" a simulated digital marketplace where AI agents powered by LLMs buy and sell information on behalf of principals. The system addresses the buyer's inspection paradox in information markets, where buyers need access to information to assess its value, but sellers need to limit access to prevent theft. The key innovation is that AI agents have dual capabilities: they can evaluate the quality of privileged information and can \"forget\" unpurchased content, allowing temporary inspection without unauthorized retention.",
        "size": 20,
        "index": 363,
        "x": 2431.1917960528526,
        "y": 110.73748944122963,
        "vy": -2.1651621349619505,
        "vx": 0.014246680681215727
      },
      "target": {
        "id": "Martin Weiss",
        "title": "Martin Weiss",
        "tags": [
          "person"
        ],
        "content": "#person",
        "size": 16.571428571428573,
        "index": 41,
        "x": 2554.358120375232,
        "y": 122.40646192028377,
        "vy": -1.9860821404587052,
        "vx": 0.08433476267133538
      },
      "index": 478
    },
    {
      "source": {
        "id": "Language models can reduce asymmetry in information markets",
        "title": "Language models can reduce asymmetry in information markets",
        "tags": [
          "project"
        ],
        "content": "#project \n\nhttps://arxiv.org/pdf/2403.14443\n\nPaper presents the \"Information Bazaar,\" a simulated digital marketplace where AI agents powered by LLMs buy and sell information on behalf of principals. The system addresses the buyer's inspection paradox in information markets, where buyers need access to information to assess its value, but sellers need to limit access to prevent theft. The key innovation is that AI agents have dual capabilities: they can evaluate the quality of privileged information and can \"forget\" unpurchased content, allowing temporary inspection without unauthorized retention.",
        "size": 20,
        "index": 363,
        "x": 2431.1917960528526,
        "y": 110.73748944122963,
        "vy": -2.1651621349619505,
        "vx": 0.014246680681215727
      },
      "target": {
        "id": "Nasim Rahaman",
        "title": "Nasim Rahaman",
        "tags": [
          "person"
        ],
        "content": "#person",
        "size": 16.571428571428573,
        "index": 314,
        "x": 2557.8866861803585,
        "y": 65.65856855303231,
        "vy": -1.9901390027784682,
        "vx": 0.0585493295762896
      },
      "index": 479
    },
    {
      "source": {
        "id": "Language models can reduce asymmetry in information markets",
        "title": "Language models can reduce asymmetry in information markets",
        "tags": [
          "project"
        ],
        "content": "#project \n\nhttps://arxiv.org/pdf/2403.14443\n\nPaper presents the \"Information Bazaar,\" a simulated digital marketplace where AI agents powered by LLMs buy and sell information on behalf of principals. The system addresses the buyer's inspection paradox in information markets, where buyers need access to information to assess its value, but sellers need to limit access to prevent theft. The key innovation is that AI agents have dual capabilities: they can evaluate the quality of privileged information and can \"forget\" unpurchased content, allowing temporary inspection without unauthorized retention.",
        "size": 20,
        "index": 363,
        "x": 2431.1917960528526,
        "y": 110.73748944122963,
        "vy": -2.1651621349619505,
        "vx": 0.014246680681215727
      },
      "target": {
        "id": "Manuel Wüthrich",
        "title": "Manuel Wüthrich",
        "tags": [
          "person"
        ],
        "content": "#person",
        "size": 17.714285714285715,
        "index": 393,
        "x": 2220.4196169294496,
        "y": 182.2761367020348,
        "vy": -1.9336271138999594,
        "vx": -0.03418308573307342
      },
      "index": 480
    },
    {
      "source": {
        "id": "Language models can reduce asymmetry in information markets",
        "title": "Language models can reduce asymmetry in information markets",
        "tags": [
          "project"
        ],
        "content": "#project \n\nhttps://arxiv.org/pdf/2403.14443\n\nPaper presents the \"Information Bazaar,\" a simulated digital marketplace where AI agents powered by LLMs buy and sell information on behalf of principals. The system addresses the buyer's inspection paradox in information markets, where buyers need access to information to assess its value, but sellers need to limit access to prevent theft. The key innovation is that AI agents have dual capabilities: they can evaluate the quality of privileged information and can \"forget\" unpurchased content, allowing temporary inspection without unauthorized retention.",
        "size": 20,
        "index": 363,
        "x": 2431.1917960528526,
        "y": 110.73748944122963,
        "vy": -2.1651621349619505,
        "vx": 0.014246680681215727
      },
      "target": {
        "id": "Yoshua Bengio",
        "title": "Yoshua Bengio",
        "tags": [
          "person"
        ],
        "content": "#person \n\nhttps://yoshuabengio.org",
        "size": 16.571428571428573,
        "index": 21,
        "x": 2505.262233261202,
        "y": 150.56630904175333,
        "vy": -1.9275102136615,
        "vx": 0.09932722699965191
      },
      "index": 481
    },
    {
      "source": {
        "id": "Language models can reduce asymmetry in information markets",
        "title": "Language models can reduce asymmetry in information markets",
        "tags": [
          "project"
        ],
        "content": "#project \n\nhttps://arxiv.org/pdf/2403.14443\n\nPaper presents the \"Information Bazaar,\" a simulated digital marketplace where AI agents powered by LLMs buy and sell information on behalf of principals. The system addresses the buyer's inspection paradox in information markets, where buyers need access to information to assess its value, but sellers need to limit access to prevent theft. The key innovation is that AI agents have dual capabilities: they can evaluate the quality of privileged information and can \"forget\" unpurchased content, allowing temporary inspection without unauthorized retention.",
        "size": 20,
        "index": 363,
        "x": 2431.1917960528526,
        "y": 110.73748944122963,
        "vy": -2.1651621349619505,
        "vx": 0.014246680681215727
      },
      "target": {
        "id": "Li Erran Li",
        "title": "Li Erran Li",
        "tags": [
          "person"
        ],
        "content": "#person",
        "size": 16.571428571428573,
        "index": 42,
        "x": 2455.7047326598263,
        "y": 33.593761978029704,
        "vy": -2.226990276825346,
        "vx": 0.025776861074325107
      },
      "index": 482
    },
    {
      "source": {
        "id": "Language models can reduce asymmetry in information markets",
        "title": "Language models can reduce asymmetry in information markets",
        "tags": [
          "project"
        ],
        "content": "#project \n\nhttps://arxiv.org/pdf/2403.14443\n\nPaper presents the \"Information Bazaar,\" a simulated digital marketplace where AI agents powered by LLMs buy and sell information on behalf of principals. The system addresses the buyer's inspection paradox in information markets, where buyers need access to information to assess its value, but sellers need to limit access to prevent theft. The key innovation is that AI agents have dual capabilities: they can evaluate the quality of privileged information and can \"forget\" unpurchased content, allowing temporary inspection without unauthorized retention.",
        "size": 20,
        "index": 363,
        "x": 2431.1917960528526,
        "y": 110.73748944122963,
        "vy": -2.1651621349619505,
        "vx": 0.014246680681215727
      },
      "target": {
        "id": "Bernhard Schölkopf",
        "title": "Bernhard Schölkopf",
        "tags": [
          "person"
        ],
        "content": "#person",
        "size": 16.571428571428573,
        "index": 194,
        "x": 2511.1998961170652,
        "y": 32.70947325052749,
        "vy": -1.9253565690205687,
        "vx": 0.012763766398038045
      },
      "index": 483
    },
    {
      "source": {
        "id": "Language models can reduce asymmetry in information markets",
        "title": "Language models can reduce asymmetry in information markets",
        "tags": [
          "project"
        ],
        "content": "#project \n\nhttps://arxiv.org/pdf/2403.14443\n\nPaper presents the \"Information Bazaar,\" a simulated digital marketplace where AI agents powered by LLMs buy and sell information on behalf of principals. The system addresses the buyer's inspection paradox in information markets, where buyers need access to information to assess its value, but sellers need to limit access to prevent theft. The key innovation is that AI agents have dual capabilities: they can evaluate the quality of privileged information and can \"forget\" unpurchased content, allowing temporary inspection without unauthorized retention.",
        "size": 20,
        "index": 363,
        "x": 2431.1917960528526,
        "y": 110.73748944122963,
        "vy": -2.1651621349619505,
        "vx": 0.014246680681215727
      },
      "target": {
        "id": "Chris Pal",
        "title": "Chris Pal",
        "tags": [
          "person"
        ],
        "content": "#person",
        "size": 16.571428571428573,
        "index": 141,
        "x": 2505.2364184752696,
        "y": 91.18753166945096,
        "vy": -1.9250048750901696,
        "vx": 0.07293671200988479
      },
      "index": 484
    },
    {
      "source": {
        "id": "AI Tools for Existential Security",
        "title": "AI Tools for Existential Security",
        "tags": [
          "project"
        ],
        "content": "#project \n\nhttps://www.forethought.org/research/ai-tools-for-existential-security\n\nPaper by Forethought that argues that rather than simply trying to slow down AI development, we should strategically accelerate the development of specific AI applications that help humanity navigate existential risks. \n\nThree key categories of beneficial AI tools: \n1. epistemic applications to help us anticipate and plan for emerging challenges \n2. coordination-enabling applications to help diverse groups work together towards shared goals\n3. risk-targeted applications to address specific challenges",
        "size": 17.142857142857142,
        "index": 365,
        "x": -1805.396984903366,
        "y": 381.93763369559025,
        "vy": -0.21441613004750124,
        "vx": 0.18054417313989948
      },
      "target": {
        "id": "Lizka Vaintrob",
        "title": "Lizka Vaintrob",
        "tags": [
          "person"
        ],
        "content": "#person",
        "size": 17.714285714285715,
        "index": 207,
        "x": -1838.159891060826,
        "y": 279.2616617432391,
        "vy": -0.17469176347322712,
        "vx": 0.2327175786848521
      },
      "index": 485
    },
    {
      "source": {
        "id": "AI Tools for Existential Security",
        "title": "AI Tools for Existential Security",
        "tags": [
          "project"
        ],
        "content": "#project \n\nhttps://www.forethought.org/research/ai-tools-for-existential-security\n\nPaper by Forethought that argues that rather than simply trying to slow down AI development, we should strategically accelerate the development of specific AI applications that help humanity navigate existential risks. \n\nThree key categories of beneficial AI tools: \n1. epistemic applications to help us anticipate and plan for emerging challenges \n2. coordination-enabling applications to help diverse groups work together towards shared goals\n3. risk-targeted applications to address specific challenges",
        "size": 17.142857142857142,
        "index": 365,
        "x": -1805.396984903366,
        "y": 381.93763369559025,
        "vy": -0.21441613004750124,
        "vx": 0.18054417313989948
      },
      "target": {
        "id": "Owen Cotton-Barratt",
        "title": "Owen Cotton-Barratt",
        "tags": [
          "person"
        ],
        "content": "#person \n\nhttps://strangecities.substack.com/",
        "size": 17.714285714285715,
        "index": 108,
        "x": -1767.049221344344,
        "y": 489.6781367043218,
        "vy": -0.2228153899267383,
        "vx": 0.1807751081937257
      },
      "index": 486
    },
    {
      "source": {
        "id": "Forecasting Tools",
        "title": "Forecasting Tools",
        "tags": [
          "project"
        ],
        "content": "#project \n\nhttps://github.com/Metaculus/forecasting-tools\n\nPython framework for building AI-powered forecasting systems that help humans predict future events more accurately.",
        "size": 17.142857142857142,
        "index": 367,
        "x": -267.1494960515555,
        "y": -872.7522764512429,
        "vy": -0.7244377138907087,
        "vx": -0.25078557148939884
      },
      "target": {
        "id": "Ben Wilson",
        "title": "Ben Wilson",
        "tags": [
          "person"
        ],
        "content": "#person",
        "size": 18.285714285714285,
        "index": 98,
        "x": -246.84657412896797,
        "y": -802.6439658065143,
        "vy": -0.7451535349150801,
        "vx": -0.25821581807688415
      },
      "index": 487
    },
    {
      "source": {
        "id": "How large language models can reshape collective intelligence",
        "title": "How large language models can reshape collective intelligence",
        "tags": [
          "project"
        ],
        "content": "#project \n\nhttps://www.nature.com/articles/s41562-024-01959-9\n\nPaper examines how LLMs are reshaping collective intelligence (the ability of groups to solve problems better than individuals alone). The authors examine both the opportunities and risks that LLMs present for human collective reasoning and decision-making processes.",
        "size": 32,
        "index": 370,
        "x": 1121.8359764769725,
        "y": 957.7475405469911,
        "vy": -1.1999075847535645,
        "vx": 0.38729640689822026
      },
      "target": {
        "id": "Jason W. Burton",
        "title": "Jason W. Burton",
        "tags": [
          "person"
        ],
        "content": "#person",
        "size": 16.571428571428573,
        "index": 74,
        "x": 1054.4055521605299,
        "y": 1090.8123716623745,
        "vy": -1.4011549463265969,
        "vx": 0.46154691780763196
      },
      "index": 488
    },
    {
      "source": {
        "id": "How large language models can reshape collective intelligence",
        "title": "How large language models can reshape collective intelligence",
        "tags": [
          "project"
        ],
        "content": "#project \n\nhttps://www.nature.com/articles/s41562-024-01959-9\n\nPaper examines how LLMs are reshaping collective intelligence (the ability of groups to solve problems better than individuals alone). The authors examine both the opportunities and risks that LLMs present for human collective reasoning and decision-making processes.",
        "size": 32,
        "index": 370,
        "x": 1121.8359764769725,
        "y": 957.7475405469911,
        "vy": -1.1999075847535645,
        "vx": 0.38729640689822026
      },
      "target": {
        "id": "Ezequiel Lopez-Lopez",
        "title": "Ezequiel Lopez-Lopez",
        "tags": [
          "person"
        ],
        "content": "#person",
        "size": 16.571428571428573,
        "index": 64,
        "x": 1154.027562329465,
        "y": 1134.100690252959,
        "vy": -1.1979702855089043,
        "vx": 0.37869962637954113
      },
      "index": 489
    },
    {
      "source": {
        "id": "How large language models can reshape collective intelligence",
        "title": "How large language models can reshape collective intelligence",
        "tags": [
          "project"
        ],
        "content": "#project \n\nhttps://www.nature.com/articles/s41562-024-01959-9\n\nPaper examines how LLMs are reshaping collective intelligence (the ability of groups to solve problems better than individuals alone). The authors examine both the opportunities and risks that LLMs present for human collective reasoning and decision-making processes.",
        "size": 32,
        "index": 370,
        "x": 1121.8359764769725,
        "y": 957.7475405469911,
        "vy": -1.1999075847535645,
        "vx": 0.38729640689822026
      },
      "target": {
        "id": "Shahar Hechtlinger",
        "title": "Shahar Hechtlinger",
        "tags": [
          "person"
        ],
        "content": "#person",
        "size": 16.571428571428573,
        "index": 11,
        "x": 999.7260100957333,
        "y": 1076.9243071209094,
        "vy": -1.2807176764715804,
        "vx": 0.2678022095086352
      },
      "index": 490
    },
    {
      "source": {
        "id": "How large language models can reshape collective intelligence",
        "title": "How large language models can reshape collective intelligence",
        "tags": [
          "project"
        ],
        "content": "#project \n\nhttps://www.nature.com/articles/s41562-024-01959-9\n\nPaper examines how LLMs are reshaping collective intelligence (the ability of groups to solve problems better than individuals alone). The authors examine both the opportunities and risks that LLMs present for human collective reasoning and decision-making processes.",
        "size": 32,
        "index": 370,
        "x": 1121.8359764769725,
        "y": 957.7475405469911,
        "vy": -1.1999075847535645,
        "vx": 0.38729640689822026
      },
      "target": {
        "id": "Zoe Rahwan",
        "title": "Zoe Rahwan",
        "tags": [
          "person"
        ],
        "content": "#person",
        "size": 16.571428571428573,
        "index": 12,
        "x": 1111.1206944884832,
        "y": 1164.6364880829267,
        "vy": -1.2344401406533358,
        "vx": 0.36604955021744023
      },
      "index": 491
    },
    {
      "source": {
        "id": "How large language models can reshape collective intelligence",
        "title": "How large language models can reshape collective intelligence",
        "tags": [
          "project"
        ],
        "content": "#project \n\nhttps://www.nature.com/articles/s41562-024-01959-9\n\nPaper examines how LLMs are reshaping collective intelligence (the ability of groups to solve problems better than individuals alone). The authors examine both the opportunities and risks that LLMs present for human collective reasoning and decision-making processes.",
        "size": 32,
        "index": 370,
        "x": 1121.8359764769725,
        "y": 957.7475405469911,
        "vy": -1.1999075847535645,
        "vx": 0.38729640689822026
      },
      "target": {
        "id": "Samuel Aeschbach",
        "title": "Samuel Aeschbach",
        "tags": [
          "person"
        ],
        "content": "#person",
        "size": 16.571428571428573,
        "index": 258,
        "x": 1012.454341600771,
        "y": 1122.9833666029804,
        "vy": -1.222950364387343,
        "vx": 0.3954988078404537
      },
      "index": 492
    },
    {
      "source": {
        "id": "How large language models can reshape collective intelligence",
        "title": "How large language models can reshape collective intelligence",
        "tags": [
          "project"
        ],
        "content": "#project \n\nhttps://www.nature.com/articles/s41562-024-01959-9\n\nPaper examines how LLMs are reshaping collective intelligence (the ability of groups to solve problems better than individuals alone). The authors examine both the opportunities and risks that LLMs present for human collective reasoning and decision-making processes.",
        "size": 32,
        "index": 370,
        "x": 1121.8359764769725,
        "y": 957.7475405469911,
        "vy": -1.1999075847535645,
        "vx": 0.38729640689822026
      },
      "target": {
        "id": "Michiel Bakker",
        "title": "Michiel Bakker",
        "tags": [
          "person"
        ],
        "content": "#person \n\nhttps://miba.dev/",
        "size": 21.142857142857142,
        "index": 284,
        "x": 1225.501866244462,
        "y": 730.5079873536822,
        "vy": -1.2714527484014098,
        "vx": 0.27625785273055353
      },
      "index": 493
    },
    {
      "source": {
        "id": "How large language models can reshape collective intelligence",
        "title": "How large language models can reshape collective intelligence",
        "tags": [
          "project"
        ],
        "content": "#project \n\nhttps://www.nature.com/articles/s41562-024-01959-9\n\nPaper examines how LLMs are reshaping collective intelligence (the ability of groups to solve problems better than individuals alone). The authors examine both the opportunities and risks that LLMs present for human collective reasoning and decision-making processes.",
        "size": 32,
        "index": 370,
        "x": 1121.8359764769725,
        "y": 957.7475405469911,
        "vy": -1.1999075847535645,
        "vx": 0.38729640689822026
      },
      "target": {
        "id": "Joshua Becker",
        "title": "Joshua Becker",
        "tags": [
          "person"
        ],
        "content": "#person",
        "size": 16.571428571428573,
        "index": 294,
        "x": 1052.7700255883067,
        "y": 937.2987696504957,
        "vy": -1.2624993291676458,
        "vx": 0.3844416980948613
      },
      "index": 494
    },
    {
      "source": {
        "id": "How large language models can reshape collective intelligence",
        "title": "How large language models can reshape collective intelligence",
        "tags": [
          "project"
        ],
        "content": "#project \n\nhttps://www.nature.com/articles/s41562-024-01959-9\n\nPaper examines how LLMs are reshaping collective intelligence (the ability of groups to solve problems better than individuals alone). The authors examine both the opportunities and risks that LLMs present for human collective reasoning and decision-making processes.",
        "size": 32,
        "index": 370,
        "x": 1121.8359764769725,
        "y": 957.7475405469911,
        "vy": -1.1999075847535645,
        "vx": 0.38729640689822026
      },
      "target": {
        "id": "Aleks Berditchevskaia",
        "title": "Aleks Berditchevskaia",
        "tags": [
          "person"
        ],
        "content": "#person",
        "size": 16.571428571428573,
        "index": 244,
        "x": 1197.361858670253,
        "y": 1041.6197981931798,
        "vy": -1.2657129882205955,
        "vx": 0.4706565632429288
      },
      "index": 495
    },
    {
      "source": {
        "id": "How large language models can reshape collective intelligence",
        "title": "How large language models can reshape collective intelligence",
        "tags": [
          "project"
        ],
        "content": "#project \n\nhttps://www.nature.com/articles/s41562-024-01959-9\n\nPaper examines how LLMs are reshaping collective intelligence (the ability of groups to solve problems better than individuals alone). The authors examine both the opportunities and risks that LLMs present for human collective reasoning and decision-making processes.",
        "size": 32,
        "index": 370,
        "x": 1121.8359764769725,
        "y": 957.7475405469911,
        "vy": -1.1999075847535645,
        "vx": 0.38729640689822026
      },
      "target": {
        "id": "Julian Berger",
        "title": "Julian Berger",
        "tags": [
          "person"
        ],
        "content": "#person",
        "size": 16.571428571428573,
        "index": 122,
        "x": 1067.6834794229983,
        "y": 888.5642714490548,
        "vy": -1.265092664201446,
        "vx": 0.3736220344506425
      },
      "index": 496
    },
    {
      "source": {
        "id": "How large language models can reshape collective intelligence",
        "title": "How large language models can reshape collective intelligence",
        "tags": [
          "project"
        ],
        "content": "#project \n\nhttps://www.nature.com/articles/s41562-024-01959-9\n\nPaper examines how LLMs are reshaping collective intelligence (the ability of groups to solve problems better than individuals alone). The authors examine both the opportunities and risks that LLMs present for human collective reasoning and decision-making processes.",
        "size": 32,
        "index": 370,
        "x": 1121.8359764769725,
        "y": 957.7475405469911,
        "vy": -1.1999075847535645,
        "vx": 0.38729640689822026
      },
      "target": {
        "id": "Levin Brinkmann",
        "title": "Levin Brinkmann",
        "tags": [
          "person"
        ],
        "content": "#person",
        "size": 16.571428571428573,
        "index": 192,
        "x": 1007.1351642394536,
        "y": 967.4523645504034,
        "vy": -1.2694512460910177,
        "vx": 0.2829518401193086
      },
      "index": 497
    },
    {
      "source": {
        "id": "How large language models can reshape collective intelligence",
        "title": "How large language models can reshape collective intelligence",
        "tags": [
          "project"
        ],
        "content": "#project \n\nhttps://www.nature.com/articles/s41562-024-01959-9\n\nPaper examines how LLMs are reshaping collective intelligence (the ability of groups to solve problems better than individuals alone). The authors examine both the opportunities and risks that LLMs present for human collective reasoning and decision-making processes.",
        "size": 32,
        "index": 370,
        "x": 1121.8359764769725,
        "y": 957.7475405469911,
        "vy": -1.1999075847535645,
        "vx": 0.38729640689822026
      },
      "target": {
        "id": "Lucie Flek",
        "title": "Lucie Flek",
        "tags": [
          "person"
        ],
        "content": "#person",
        "size": 16.571428571428573,
        "index": 312,
        "x": 1186.0900331035532,
        "y": 992.1321419547221,
        "vy": -1.2500960310336633,
        "vx": 0.4130680562785176
      },
      "index": 498
    },
    {
      "source": {
        "id": "How large language models can reshape collective intelligence",
        "title": "How large language models can reshape collective intelligence",
        "tags": [
          "project"
        ],
        "content": "#project \n\nhttps://www.nature.com/articles/s41562-024-01959-9\n\nPaper examines how LLMs are reshaping collective intelligence (the ability of groups to solve problems better than individuals alone). The authors examine both the opportunities and risks that LLMs present for human collective reasoning and decision-making processes.",
        "size": 32,
        "index": 370,
        "x": 1121.8359764769725,
        "y": 957.7475405469911,
        "vy": -1.1999075847535645,
        "vx": 0.38729640689822026
      },
      "target": {
        "id": "Stefan M. Herzog",
        "title": "Stefan M. Herzog",
        "tags": [
          "person"
        ],
        "content": "#person",
        "size": 16.571428571428573,
        "index": 135,
        "x": 1012.5821375962063,
        "y": 903.8478373431252,
        "vy": -1.2637119096053626,
        "vx": 0.36399953429702286
      },
      "index": 499
    },
    {
      "source": {
        "id": "How large language models can reshape collective intelligence",
        "title": "How large language models can reshape collective intelligence",
        "tags": [
          "project"
        ],
        "content": "#project \n\nhttps://www.nature.com/articles/s41562-024-01959-9\n\nPaper examines how LLMs are reshaping collective intelligence (the ability of groups to solve problems better than individuals alone). The authors examine both the opportunities and risks that LLMs present for human collective reasoning and decision-making processes.",
        "size": 32,
        "index": 370,
        "x": 1121.8359764769725,
        "y": 957.7475405469911,
        "vy": -1.1999075847535645,
        "vx": 0.38729640689822026
      },
      "target": {
        "id": "Saffron Huang",
        "title": "Saffron Huang",
        "tags": [
          "person"
        ],
        "content": "#person",
        "size": 17.142857142857142,
        "index": 431,
        "x": 1138.196955949223,
        "y": 884.9737635766728,
        "vy": -1.2588665998709374,
        "vx": 0.37285374359340917
      },
      "index": 500
    },
    {
      "source": {
        "id": "How large language models can reshape collective intelligence",
        "title": "How large language models can reshape collective intelligence",
        "tags": [
          "project"
        ],
        "content": "#project \n\nhttps://www.nature.com/articles/s41562-024-01959-9\n\nPaper examines how LLMs are reshaping collective intelligence (the ability of groups to solve problems better than individuals alone). The authors examine both the opportunities and risks that LLMs present for human collective reasoning and decision-making processes.",
        "size": 32,
        "index": 370,
        "x": 1121.8359764769725,
        "y": 957.7475405469911,
        "vy": -1.1999075847535645,
        "vx": 0.38729640689822026
      },
      "target": {
        "id": "Sayash Kapoor",
        "title": "Sayash Kapoor",
        "tags": [
          "person"
        ],
        "content": "#person",
        "size": 16.571428571428573,
        "index": 327,
        "x": 1095.3622312447662,
        "y": 1015.6017130919702,
        "vy": -1.230792857841718,
        "vx": 0.4121828689505459
      },
      "index": 501
    },
    {
      "source": {
        "id": "How large language models can reshape collective intelligence",
        "title": "How large language models can reshape collective intelligence",
        "tags": [
          "project"
        ],
        "content": "#project \n\nhttps://www.nature.com/articles/s41562-024-01959-9\n\nPaper examines how LLMs are reshaping collective intelligence (the ability of groups to solve problems better than individuals alone). The authors examine both the opportunities and risks that LLMs present for human collective reasoning and decision-making processes.",
        "size": 32,
        "index": 370,
        "x": 1121.8359764769725,
        "y": 957.7475405469911,
        "vy": -1.1999075847535645,
        "vx": 0.38729640689822026
      },
      "target": {
        "id": "Arvind Narayanan",
        "title": "Arvind Narayanan",
        "tags": [
          "person"
        ],
        "content": "#person",
        "size": 16.571428571428573,
        "index": 62,
        "x": 1141.6446111346118,
        "y": 1024.7335170157417,
        "vy": -1.374969357327315,
        "vx": -0.09560671319887439
      },
      "index": 502
    },
    {
      "source": {
        "id": "How large language models can reshape collective intelligence",
        "title": "How large language models can reshape collective intelligence",
        "tags": [
          "project"
        ],
        "content": "#project \n\nhttps://www.nature.com/articles/s41562-024-01959-9\n\nPaper examines how LLMs are reshaping collective intelligence (the ability of groups to solve problems better than individuals alone). The authors examine both the opportunities and risks that LLMs present for human collective reasoning and decision-making processes.",
        "size": 32,
        "index": 370,
        "x": 1121.8359764769725,
        "y": 957.7475405469911,
        "vy": -1.1999075847535645,
        "vx": 0.38729640689822026
      },
      "target": {
        "id": "Anne-Marie Nussberger",
        "title": "Anne-Marie Nussberger",
        "tags": [
          "person"
        ],
        "content": "#person",
        "size": 16.571428571428573,
        "index": 286,
        "x": 1049.2268533909287,
        "y": 993.0573248996991,
        "vy": -1.2236323920717103,
        "vx": 0.3958180137119475
      },
      "index": 503
    },
    {
      "source": {
        "id": "How large language models can reshape collective intelligence",
        "title": "How large language models can reshape collective intelligence",
        "tags": [
          "project"
        ],
        "content": "#project \n\nhttps://www.nature.com/articles/s41562-024-01959-9\n\nPaper examines how LLMs are reshaping collective intelligence (the ability of groups to solve problems better than individuals alone). The authors examine both the opportunities and risks that LLMs present for human collective reasoning and decision-making processes.",
        "size": 32,
        "index": 370,
        "x": 1121.8359764769725,
        "y": 957.7475405469911,
        "vy": -1.1999075847535645,
        "vx": 0.38729640689822026
      },
      "target": {
        "id": "Taha Yasseri",
        "title": "Taha Yasseri",
        "tags": [
          "person"
        ],
        "content": "#person",
        "size": 16.571428571428573,
        "index": 434,
        "x": 1151.452256598522,
        "y": 1075.0613355615217,
        "vy": -1.2383048661643294,
        "vx": 0.3616001876855118
      },
      "index": 504
    },
    {
      "source": {
        "id": "How large language models can reshape collective intelligence",
        "title": "How large language models can reshape collective intelligence",
        "tags": [
          "project"
        ],
        "content": "#project \n\nhttps://www.nature.com/articles/s41562-024-01959-9\n\nPaper examines how LLMs are reshaping collective intelligence (the ability of groups to solve problems better than individuals alone). The authors examine both the opportunities and risks that LLMs present for human collective reasoning and decision-making processes.",
        "size": 32,
        "index": 370,
        "x": 1121.8359764769725,
        "y": 957.7475405469911,
        "vy": -1.1999075847535645,
        "vx": 0.38729640689822026
      },
      "target": {
        "id": "Pietro Nickl",
        "title": "Pietro Nickl",
        "tags": [
          "person"
        ],
        "content": "#person",
        "size": 16.571428571428573,
        "index": 125,
        "x": 1005.5126368301044,
        "y": 1026.5466334649998,
        "vy": -1.256251737672293,
        "vx": 0.31542693663521876
      },
      "index": 505
    },
    {
      "source": {
        "id": "How large language models can reshape collective intelligence",
        "title": "How large language models can reshape collective intelligence",
        "tags": [
          "project"
        ],
        "content": "#project \n\nhttps://www.nature.com/articles/s41562-024-01959-9\n\nPaper examines how LLMs are reshaping collective intelligence (the ability of groups to solve problems better than individuals alone). The authors examine both the opportunities and risks that LLMs present for human collective reasoning and decision-making processes.",
        "size": 32,
        "index": 370,
        "x": 1121.8359764769725,
        "y": 957.7475405469911,
        "vy": -1.1999075847535645,
        "vx": 0.38729640689822026
      },
      "target": {
        "id": "Abdullah Almaatouq",
        "title": "Abdullah Almaatouq",
        "tags": [
          "person"
        ],
        "content": "#person",
        "size": 16.571428571428573,
        "index": 391,
        "x": 1095.1486546534297,
        "y": 1065.3037885143005,
        "vy": -1.2481160958233495,
        "vx": 0.3632903462457734
      },
      "index": 506
    },
    {
      "source": {
        "id": "How large language models can reshape collective intelligence",
        "title": "How large language models can reshape collective intelligence",
        "tags": [
          "project"
        ],
        "content": "#project \n\nhttps://www.nature.com/articles/s41562-024-01959-9\n\nPaper examines how LLMs are reshaping collective intelligence (the ability of groups to solve problems better than individuals alone). The authors examine both the opportunities and risks that LLMs present for human collective reasoning and decision-making processes.",
        "size": 32,
        "index": 370,
        "x": 1121.8359764769725,
        "y": 957.7475405469911,
        "vy": -1.1999075847535645,
        "vx": 0.38729640689822026
      },
      "target": {
        "id": "Ulrike Hahn",
        "title": "Ulrike Hahn",
        "tags": [
          "person"
        ],
        "content": "#person",
        "size": 16.571428571428573,
        "index": 110,
        "x": 1046.0842952759554,
        "y": 1051.9234963407023,
        "vy": -1.2945669918711793,
        "vx": 0.42496178476230867
      },
      "index": 507
    },
    {
      "source": {
        "id": "How large language models can reshape collective intelligence",
        "title": "How large language models can reshape collective intelligence",
        "tags": [
          "project"
        ],
        "content": "#project \n\nhttps://www.nature.com/articles/s41562-024-01959-9\n\nPaper examines how LLMs are reshaping collective intelligence (the ability of groups to solve problems better than individuals alone). The authors examine both the opportunities and risks that LLMs present for human collective reasoning and decision-making processes.",
        "size": 32,
        "index": 370,
        "x": 1121.8359764769725,
        "y": 957.7475405469911,
        "vy": -1.1999075847535645,
        "vx": 0.38729640689822026
      },
      "target": {
        "id": "Ralf H. J. M. Kurvers",
        "title": "Ralf H. J. M. Kurvers",
        "tags": [
          "person"
        ],
        "content": "#person",
        "size": 16.571428571428573,
        "index": 214,
        "x": 965.8893510094586,
        "y": 935.0505653114542,
        "vy": -1.2635925574469067,
        "vx": 0.3838876214596957
      },
      "index": 508
    },
    {
      "source": {
        "id": "How large language models can reshape collective intelligence",
        "title": "How large language models can reshape collective intelligence",
        "tags": [
          "project"
        ],
        "content": "#project \n\nhttps://www.nature.com/articles/s41562-024-01959-9\n\nPaper examines how LLMs are reshaping collective intelligence (the ability of groups to solve problems better than individuals alone). The authors examine both the opportunities and risks that LLMs present for human collective reasoning and decision-making processes.",
        "size": 32,
        "index": 370,
        "x": 1121.8359764769725,
        "y": 957.7475405469911,
        "vy": -1.1999075847535645,
        "vx": 0.38729640689822026
      },
      "target": {
        "id": "Susan Leavy",
        "title": "Susan Leavy",
        "tags": [
          "person"
        ],
        "content": "#person",
        "size": 16.571428571428573,
        "index": 91,
        "x": 958.946015279396,
        "y": 1047.1735494609654,
        "vy": -1.249030980846087,
        "vx": 0.36448953676311985
      },
      "index": 509
    },
    {
      "source": {
        "id": "How large language models can reshape collective intelligence",
        "title": "How large language models can reshape collective intelligence",
        "tags": [
          "project"
        ],
        "content": "#project \n\nhttps://www.nature.com/articles/s41562-024-01959-9\n\nPaper examines how LLMs are reshaping collective intelligence (the ability of groups to solve problems better than individuals alone). The authors examine both the opportunities and risks that LLMs present for human collective reasoning and decision-making processes.",
        "size": 32,
        "index": 370,
        "x": 1121.8359764769725,
        "y": 957.7475405469911,
        "vy": -1.1999075847535645,
        "vx": 0.38729640689822026
      },
      "target": {
        "id": "Iyad Rahwan",
        "title": "Iyad Rahwan",
        "tags": [
          "person"
        ],
        "content": "#person",
        "size": 16.571428571428573,
        "index": 343,
        "x": 959.255604596829,
        "y": 991.807045825743,
        "vy": -1.2656969715631823,
        "vx": 0.36573561757665135
      },
      "index": 510
    },
    {
      "source": {
        "id": "How large language models can reshape collective intelligence",
        "title": "How large language models can reshape collective intelligence",
        "tags": [
          "project"
        ],
        "content": "#project \n\nhttps://www.nature.com/articles/s41562-024-01959-9\n\nPaper examines how LLMs are reshaping collective intelligence (the ability of groups to solve problems better than individuals alone). The authors examine both the opportunities and risks that LLMs present for human collective reasoning and decision-making processes.",
        "size": 32,
        "index": 370,
        "x": 1121.8359764769725,
        "y": 957.7475405469911,
        "vy": -1.1999075847535645,
        "vx": 0.38729640689822026
      },
      "target": {
        "id": "Divya Siddarth",
        "title": "Divya Siddarth",
        "tags": [
          "person"
        ],
        "content": "#person \n\nhttps://divyasiddarth.com/",
        "size": 17.714285714285715,
        "index": 414,
        "x": 1251.3647810817142,
        "y": 910.2533943640935,
        "vy": -1.2814979219729066,
        "vx": 0.3150773430583668
      },
      "index": 511
    },
    {
      "source": {
        "id": "How large language models can reshape collective intelligence",
        "title": "How large language models can reshape collective intelligence",
        "tags": [
          "project"
        ],
        "content": "#project \n\nhttps://www.nature.com/articles/s41562-024-01959-9\n\nPaper examines how LLMs are reshaping collective intelligence (the ability of groups to solve problems better than individuals alone). The authors examine both the opportunities and risks that LLMs present for human collective reasoning and decision-making processes.",
        "size": 32,
        "index": 370,
        "x": 1121.8359764769725,
        "y": 957.7475405469911,
        "vy": -1.1999075847535645,
        "vx": 0.38729640689822026
      },
      "target": {
        "id": "Alice Siu",
        "title": "Alice Siu",
        "tags": [
          "person"
        ],
        "content": "#person \n\nhttps://www.alicesiu.com/",
        "size": 17.142857142857142,
        "index": 76,
        "x": 985.0211574111167,
        "y": 1172.7456815191272,
        "vy": -1.229908976771374,
        "vx": 0.3748639981031952
      },
      "index": 512
    },
    {
      "source": {
        "id": "How large language models can reshape collective intelligence",
        "title": "How large language models can reshape collective intelligence",
        "tags": [
          "project"
        ],
        "content": "#project \n\nhttps://www.nature.com/articles/s41562-024-01959-9\n\nPaper examines how LLMs are reshaping collective intelligence (the ability of groups to solve problems better than individuals alone). The authors examine both the opportunities and risks that LLMs present for human collective reasoning and decision-making processes.",
        "size": 32,
        "index": 370,
        "x": 1121.8359764769725,
        "y": 957.7475405469911,
        "vy": -1.1999075847535645,
        "vx": 0.38729640689822026
      },
      "target": {
        "id": "Anita W. Woolley",
        "title": "Anita W. Woolley",
        "tags": [
          "person"
        ],
        "content": "#person",
        "size": 16.571428571428573,
        "index": 330,
        "x": 1199.6621406794438,
        "y": 1097.6159370392663,
        "vy": -1.1959958144619627,
        "vx": 0.34539774436290244
      },
      "index": 513
    },
    {
      "source": {
        "id": "How large language models can reshape collective intelligence",
        "title": "How large language models can reshape collective intelligence",
        "tags": [
          "project"
        ],
        "content": "#project \n\nhttps://www.nature.com/articles/s41562-024-01959-9\n\nPaper examines how LLMs are reshaping collective intelligence (the ability of groups to solve problems better than individuals alone). The authors examine both the opportunities and risks that LLMs present for human collective reasoning and decision-making processes.",
        "size": 32,
        "index": 370,
        "x": 1121.8359764769725,
        "y": 957.7475405469911,
        "vy": -1.1999075847535645,
        "vx": 0.38729640689822026
      },
      "target": {
        "id": "Dirk U. Wulff",
        "title": "Dirk U. Wulff",
        "tags": [
          "person"
        ],
        "content": "#person",
        "size": 16.571428571428573,
        "index": 48,
        "x": 1108.926778892928,
        "y": 1111.2480613974628,
        "vy": -1.2909795014886325,
        "vx": 0.3375928114529963
      },
      "index": 514
    },
    {
      "source": {
        "id": "How large language models can reshape collective intelligence",
        "title": "How large language models can reshape collective intelligence",
        "tags": [
          "project"
        ],
        "content": "#project \n\nhttps://www.nature.com/articles/s41562-024-01959-9\n\nPaper examines how LLMs are reshaping collective intelligence (the ability of groups to solve problems better than individuals alone). The authors examine both the opportunities and risks that LLMs present for human collective reasoning and decision-making processes.",
        "size": 32,
        "index": 370,
        "x": 1121.8359764769725,
        "y": 957.7475405469911,
        "vy": -1.1999075847535645,
        "vx": 0.38729640689822026
      },
      "target": {
        "id": "Ralph Hertwig",
        "title": "Ralph Hertwig",
        "tags": [
          "person"
        ],
        "content": "#person",
        "size": 16.571428571428573,
        "index": 403,
        "x": 1066.70501145734,
        "y": 1140.9324477762468,
        "vy": -1.2216835622235742,
        "vx": 0.39509940586976144
      },
      "index": 515
    },
    {
      "source": {
        "id": "Democratic Fine-Tuning",
        "title": "Democratic Fine-Tuning",
        "tags": [
          "project"
        ],
        "content": "#project \n\nhttps://www.meaningalignment.org/research/openai-dft-the-first-moral-graph\n\nLLM-based system creates \"moral graphs\" by using a two-stage process that aims to uncover shared values underlying political disagreement. A specialized chatbot engages participants in dialogue about contentious scenarios, asking for personal stories and role models to extract the underlying value behind responses rather than collecting ideological commitments like slogans or rules. The system then shows participants stories of people transitioning between different values and asks whether such transitions represent gains in wisdom, creating a graph structure where edges represent consensus about which values are more comprehensive than others\n\nFunded by [OpenAI Democratic Inputs to AI](https://github.com/openai/democratic-inputs).",
        "size": 18.285714285714285,
        "index": 372,
        "x": 1459.291986189716,
        "y": -19.522276055203207,
        "vy": -1.358464310600762,
        "vx": 0.05583502088743932
      },
      "target": {
        "id": "Joe Edelman",
        "title": "Joe Edelman",
        "tags": [
          "person"
        ],
        "content": "#person \n\nhttps://nxhx.org/",
        "size": 17.714285714285715,
        "index": 268,
        "x": 1356.6939030945252,
        "y": -196.06959723472013,
        "vy": -1.3527938545248293,
        "vx": -0.025787874230328266
      },
      "index": 516
    },
    {
      "source": {
        "id": "Democratic Fine-Tuning",
        "title": "Democratic Fine-Tuning",
        "tags": [
          "project"
        ],
        "content": "#project \n\nhttps://www.meaningalignment.org/research/openai-dft-the-first-moral-graph\n\nLLM-based system creates \"moral graphs\" by using a two-stage process that aims to uncover shared values underlying political disagreement. A specialized chatbot engages participants in dialogue about contentious scenarios, asking for personal stories and role models to extract the underlying value behind responses rather than collecting ideological commitments like slogans or rules. The system then shows participants stories of people transitioning between different values and asks whether such transitions represent gains in wisdom, creating a graph structure where edges represent consensus about which values are more comprehensive than others\n\nFunded by [OpenAI Democratic Inputs to AI](https://github.com/openai/democratic-inputs).",
        "size": 18.285714285714285,
        "index": 372,
        "x": 1459.291986189716,
        "y": -19.522276055203207,
        "vy": -1.358464310600762,
        "vx": 0.05583502088743932
      },
      "target": {
        "id": "Oliver Klingefjord",
        "title": "Oliver Klingefjord",
        "tags": [
          "person"
        ],
        "content": "#person \n\nhttps://www.klingefjord.com/",
        "size": 18.285714285714285,
        "index": 44,
        "x": 1178.5910932786057,
        "y": -246.8472025448904,
        "vy": -1.2893218733044343,
        "vx": -0.05174057759054609
      },
      "index": 517
    },
    {
      "source": {
        "id": "Democratic Fine-Tuning",
        "title": "Democratic Fine-Tuning",
        "tags": [
          "project"
        ],
        "content": "#project \n\nhttps://www.meaningalignment.org/research/openai-dft-the-first-moral-graph\n\nLLM-based system creates \"moral graphs\" by using a two-stage process that aims to uncover shared values underlying political disagreement. A specialized chatbot engages participants in dialogue about contentious scenarios, asking for personal stories and role models to extract the underlying value behind responses rather than collecting ideological commitments like slogans or rules. The system then shows participants stories of people transitioning between different values and asks whether such transitions represent gains in wisdom, creating a graph structure where edges represent consensus about which values are more comprehensive than others\n\nFunded by [OpenAI Democratic Inputs to AI](https://github.com/openai/democratic-inputs).",
        "size": 18.285714285714285,
        "index": 372,
        "x": 1459.291986189716,
        "y": -19.522276055203207,
        "vy": -1.358464310600762,
        "vx": 0.05583502088743932
      },
      "target": {
        "id": "Ivan Vendrov",
        "title": "Ivan Vendrov",
        "tags": [
          "person"
        ],
        "content": "#person \n\nhttps://www.vendrov.ai/",
        "size": 17.714285714285715,
        "index": 215,
        "x": 1810.5588734061498,
        "y": 305.9910348849648,
        "vy": -1.443643107030699,
        "vx": 0.22600812026212258
      },
      "index": 518
    },
    {
      "source": {
        "id": "Generative AI Voting - Fair Collective Choice is Resilient to LLM Biases and Inconsistencies",
        "title": "Generative AI Voting - Fair Collective Choice is Resilient to LLM Biases and Inconsistencies",
        "tags": [
          "project"
        ],
        "content": "#project \n\nhttps://arxiv.org/pdf/2406.11871\n\nResearch studies how LLMs can serve as representatives for human voters in democratic processes like participatory budgeting and elections. The research demonstrates that AI systems can effectively represent abstaining voters.",
        "size": 17.714285714285715,
        "index": 378,
        "x": -2881.253012502264,
        "y": 1251.6391269224252,
        "vy": 0.6424803152043262,
        "vx": 0.7439629877093129
      },
      "target": {
        "id": "Srijoni Majumdar",
        "title": "Srijoni Majumdar",
        "tags": [
          "person"
        ],
        "content": "#person",
        "size": 16.571428571428573,
        "index": 174,
        "x": -2928.033314011626,
        "y": 1296.9175044223866,
        "vy": 0.6477834887758266,
        "vx": 0.7379522081712402
      },
      "index": 519
    },
    {
      "source": {
        "id": "Generative AI Voting - Fair Collective Choice is Resilient to LLM Biases and Inconsistencies",
        "title": "Generative AI Voting - Fair Collective Choice is Resilient to LLM Biases and Inconsistencies",
        "tags": [
          "project"
        ],
        "content": "#project \n\nhttps://arxiv.org/pdf/2406.11871\n\nResearch studies how LLMs can serve as representatives for human voters in democratic processes like participatory budgeting and elections. The research demonstrates that AI systems can effectively represent abstaining voters.",
        "size": 17.714285714285715,
        "index": 378,
        "x": -2881.253012502264,
        "y": 1251.6391269224252,
        "vy": 0.6424803152043262,
        "vx": 0.7439629877093129
      },
      "target": {
        "id": "Edith Elkind",
        "title": "Edith Elkind",
        "tags": [
          "person"
        ],
        "content": "#person",
        "size": 16.571428571428573,
        "index": 333,
        "x": -2823.591781504284,
        "y": 1282.3060599742723,
        "vy": 0.8020077973311646,
        "vx": 0.6769490719766871
      },
      "index": 520
    },
    {
      "source": {
        "id": "Generative AI Voting - Fair Collective Choice is Resilient to LLM Biases and Inconsistencies",
        "title": "Generative AI Voting - Fair Collective Choice is Resilient to LLM Biases and Inconsistencies",
        "tags": [
          "project"
        ],
        "content": "#project \n\nhttps://arxiv.org/pdf/2406.11871\n\nResearch studies how LLMs can serve as representatives for human voters in democratic processes like participatory budgeting and elections. The research demonstrates that AI systems can effectively represent abstaining voters.",
        "size": 17.714285714285715,
        "index": 378,
        "x": -2881.253012502264,
        "y": 1251.6391269224252,
        "vy": 0.6424803152043262,
        "vx": 0.7439629877093129
      },
      "target": {
        "id": "Evangelos Pournaras",
        "title": "Evangelos Pournaras",
        "tags": [
          "person"
        ],
        "content": "#person",
        "size": 16.571428571428573,
        "index": 270,
        "x": -2894.3098275292855,
        "y": 1188.511929249774,
        "vy": 0.6558151788477075,
        "vx": 0.5907105419627996
      },
      "index": 521
    },
    {
      "source": {
        "id": "Rai Sur",
        "title": "Rai Sur",
        "tags": [
          "person"
        ],
        "content": "#person \n\nhttps://rai.dev",
        "size": 17.142857142857142,
        "index": 379,
        "x": -1136.882572758444,
        "y": -1562.3371179774033,
        "vy": -0.5651901195125634,
        "vx": -0.6815882967403171
      },
      "target": {
        "id": "Sentinel",
        "title": "Sentinel",
        "tags": [
          "organization"
        ],
        "content": "#organization \n\nhttps://sentinel-team.org",
        "size": 17.142857142857142,
        "index": 69,
        "x": -1070.884753433492,
        "y": -1530.2437019367424,
        "vy": -0.2331068249938424,
        "vx": -0.7395955777031983
      },
      "index": 522
    },
    {
      "source": {
        "id": "Research",
        "title": "Research",
        "tags": [
          "topic"
        ],
        "content": "#topic",
        "size": 17.714285714285715,
        "index": 381,
        "x": -507.4349535393232,
        "y": 795.513991809036,
        "vy": -0.6040391536456098,
        "vx": 0.2947097505475442
      },
      "target": {
        "id": "Elicit Tool",
        "title": "Elicit Tool",
        "tags": [
          "project"
        ],
        "content": "#project \n\nhttps://elicit.com/\n\nAI research assistant that helps users conduct literature reviews by searching academic papers, extracting key findings, and synthesizing research evidence to answer specific questions.",
        "size": 17.714285714285715,
        "index": 361,
        "x": -1934.779592795771,
        "y": 498.89241080967696,
        "vy": -0.3068112062450581,
        "vx": 0.05936972970761678
      },
      "type": "topic_link",
      "index": 523
    },
    {
      "source": {
        "id": "Research",
        "title": "Research",
        "tags": [
          "topic"
        ],
        "content": "#topic",
        "size": 17.714285714285715,
        "index": 381,
        "x": -507.4349535393232,
        "y": 795.513991809036,
        "vy": -0.6040391536456098,
        "vx": 0.2947097505475442
      },
      "target": {
        "id": "LLMs as Research Tools -  A Large Scale Survey of Researchers’ Usage and Perceptions",
        "title": "LLMs as Research Tools -  A Large Scale Survey of Researchers’ Usage and Perceptions",
        "tags": [
          "project"
        ],
        "content": "#project \n\nhttps://arxiv.org/pdf/2411.05025\n\nLarge-scale survey study examining how researchers currently use LLMs in their research workflows and their perceptions of benefits and risks. The study surveyed 816 verified research paper authors across multiple disciplines to understand usage patterns, demographic differences, and attitudes toward LLM tools in academic work.",
        "size": 21.142857142857142,
        "index": 85,
        "x": 1156.1294161915903,
        "y": 1698.8968115333146,
        "vy": -1.293066591526444,
        "vx": 0.7885209896134656
      },
      "type": "topic_link",
      "index": 524
    },
    {
      "source": {
        "id": "Research",
        "title": "Research",
        "tags": [
          "topic"
        ],
        "content": "#topic",
        "size": 17.714285714285715,
        "index": 381,
        "x": -507.4349535393232,
        "y": 795.513991809036,
        "vy": -0.6040391536456098,
        "vx": 0.2947097505475442
      },
      "target": {
        "id": "Deep Research Bench - Evaluating AI Web Research Agents",
        "title": "Deep Research Bench - Evaluating AI Web Research Agents",
        "tags": [
          "project"
        ],
        "content": "#project \n\nhttps://arxiv.org/abs/2506.06287\nhttps://evals.futuresearch.ai/\n\nBenchmark for evaluating AI agents on complex, multi-step web research tasks that mirror real-world analytical work, using a frozen web environment to ensure consistent and repeatable evaluations. The benchmark tests research capabilities including strategic planning, source credibility assessment, systematic information gathering, and synthesis across contradictory sources through eight different task categories.",
        "size": 21.142857142857142,
        "index": 332,
        "x": -788.3209992852137,
        "y": -565.6379415639828,
        "vy": -0.37590759276131464,
        "vx": -0.2869994643202302
      },
      "type": "topic_link",
      "index": 525
    },
    {
      "source": {
        "id": "Bench to the Future - A Pastcasting Benchmark for Forecasting Agents",
        "title": "Bench to the Future - A Pastcasting Benchmark for Forecasting Agents",
        "tags": [
          "project"
        ],
        "content": "#project \n\nhttps://arxiv.org/abs/2506.21558\nhttps://evals.futuresearch.ai/\n\nEstablishes a \"pastcasting\" benchmark that evaluates AI forecasting capabilities by having LLMs predict outcomes of events that have already happened, using carefully curated web snapshots from before those events occurred. The benchmark addresses a key challenge in forecasting research - the long wait times between making predictions and learning outcomes.",
        "size": 21.142857142857142,
        "index": 383,
        "x": -731.0795440958706,
        "y": -611.1986198161612,
        "vy": -0.4275064604701327,
        "vx": -0.3280891561207337
      },
      "target": {
        "id": "Jack Wildman",
        "title": "Jack Wildman",
        "tags": [
          "person"
        ],
        "content": "#person",
        "size": 17.714285714285715,
        "index": 190,
        "x": -700.5637035103321,
        "y": -656.0808982602216,
        "vy": -0.44997863065831584,
        "vx": -0.36028652459093663
      },
      "index": 526
    },
    {
      "source": {
        "id": "Bench to the Future - A Pastcasting Benchmark for Forecasting Agents",
        "title": "Bench to the Future - A Pastcasting Benchmark for Forecasting Agents",
        "tags": [
          "project"
        ],
        "content": "#project \n\nhttps://arxiv.org/abs/2506.21558\nhttps://evals.futuresearch.ai/\n\nEstablishes a \"pastcasting\" benchmark that evaluates AI forecasting capabilities by having LLMs predict outcomes of events that have already happened, using carefully curated web snapshots from before those events occurred. The benchmark addresses a key challenge in forecasting research - the long wait times between making predictions and learning outcomes.",
        "size": 21.142857142857142,
        "index": 383,
        "x": -731.0795440958706,
        "y": -611.1986198161612,
        "vy": -0.4275064604701327,
        "vx": -0.3280891561207337
      },
      "target": {
        "id": "Nikos Bosse",
        "title": "Nikos Bosse",
        "tags": [
          "person"
        ],
        "content": "#person \n\nhttps://followtheargument.org/",
        "size": 18.285714285714285,
        "index": 243,
        "x": -843.047489587698,
        "y": -605.5760820500159,
        "vy": -0.3391263569445478,
        "vx": -0.31376832368339486
      },
      "index": 527
    },
    {
      "source": {
        "id": "Bench to the Future - A Pastcasting Benchmark for Forecasting Agents",
        "title": "Bench to the Future - A Pastcasting Benchmark for Forecasting Agents",
        "tags": [
          "project"
        ],
        "content": "#project \n\nhttps://arxiv.org/abs/2506.21558\nhttps://evals.futuresearch.ai/\n\nEstablishes a \"pastcasting\" benchmark that evaluates AI forecasting capabilities by having LLMs predict outcomes of events that have already happened, using carefully curated web snapshots from before those events occurred. The benchmark addresses a key challenge in forecasting research - the long wait times between making predictions and learning outcomes.",
        "size": 21.142857142857142,
        "index": 383,
        "x": -731.0795440958706,
        "y": -611.1986198161612,
        "vy": -0.4275064604701327,
        "vx": -0.3280891561207337
      },
      "target": {
        "id": "Daniel Hnyk",
        "title": "Daniel Hnyk",
        "tags": [
          "person"
        ],
        "content": "#person",
        "size": 17.714285714285715,
        "index": 413,
        "x": -788.7242426021298,
        "y": -636.2606637787694,
        "vy": -0.37777225834080214,
        "vx": -0.34973522484873115
      },
      "index": 528
    },
    {
      "source": {
        "id": "Bench to the Future - A Pastcasting Benchmark for Forecasting Agents",
        "title": "Bench to the Future - A Pastcasting Benchmark for Forecasting Agents",
        "tags": [
          "project"
        ],
        "content": "#project \n\nhttps://arxiv.org/abs/2506.21558\nhttps://evals.futuresearch.ai/\n\nEstablishes a \"pastcasting\" benchmark that evaluates AI forecasting capabilities by having LLMs predict outcomes of events that have already happened, using carefully curated web snapshots from before those events occurred. The benchmark addresses a key challenge in forecasting research - the long wait times between making predictions and learning outcomes.",
        "size": 21.142857142857142,
        "index": 383,
        "x": -731.0795440958706,
        "y": -611.1986198161612,
        "vy": -0.4275064604701327,
        "vx": -0.3280891561207337
      },
      "target": {
        "id": "Peter Mühlbacher",
        "title": "Peter Mühlbacher",
        "tags": [
          "person"
        ],
        "content": "#person \n\nhttp://peter.muehlbacher.me/",
        "size": 18.285714285714285,
        "index": 238,
        "x": -857.2972832702691,
        "y": -654.0740118249536,
        "vy": -0.3400430834320815,
        "vx": -0.3453910216674885
      },
      "index": 529
    },
    {
      "source": {
        "id": "Bench to the Future - A Pastcasting Benchmark for Forecasting Agents",
        "title": "Bench to the Future - A Pastcasting Benchmark for Forecasting Agents",
        "tags": [
          "project"
        ],
        "content": "#project \n\nhttps://arxiv.org/abs/2506.21558\nhttps://evals.futuresearch.ai/\n\nEstablishes a \"pastcasting\" benchmark that evaluates AI forecasting capabilities by having LLMs predict outcomes of events that have already happened, using carefully curated web snapshots from before those events occurred. The benchmark addresses a key challenge in forecasting research - the long wait times between making predictions and learning outcomes.",
        "size": 21.142857142857142,
        "index": 383,
        "x": -731.0795440958706,
        "y": -611.1986198161612,
        "vy": -0.4275064604701327,
        "vx": -0.3280891561207337
      },
      "target": {
        "id": "Finn Hambly",
        "title": "Finn Hambly",
        "tags": [
          "person"
        ],
        "content": "#person",
        "size": 17.142857142857142,
        "index": 289,
        "x": -777.0782596942344,
        "y": -732.8668921963484,
        "vy": -0.39969189429602353,
        "vx": -0.3928166053995662
      },
      "index": 530
    },
    {
      "source": {
        "id": "Bench to the Future - A Pastcasting Benchmark for Forecasting Agents",
        "title": "Bench to the Future - A Pastcasting Benchmark for Forecasting Agents",
        "tags": [
          "project"
        ],
        "content": "#project \n\nhttps://arxiv.org/abs/2506.21558\nhttps://evals.futuresearch.ai/\n\nEstablishes a \"pastcasting\" benchmark that evaluates AI forecasting capabilities by having LLMs predict outcomes of events that have already happened, using carefully curated web snapshots from before those events occurred. The benchmark addresses a key challenge in forecasting research - the long wait times between making predictions and learning outcomes.",
        "size": 21.142857142857142,
        "index": 383,
        "x": -731.0795440958706,
        "y": -611.1986198161612,
        "vy": -0.4275064604701327,
        "vx": -0.3280891561207337
      },
      "target": {
        "id": "Jon Evans",
        "title": "Jon Evans",
        "tags": [
          "person"
        ],
        "content": "#person \n\nhttps://rezendi.com",
        "size": 18.285714285714285,
        "index": 341,
        "x": -610.9648619044706,
        "y": -618.6960073290362,
        "vy": -0.476093867230055,
        "vx": -0.32700571013017027
      },
      "index": 531
    },
    {
      "source": {
        "id": "Bench to the Future - A Pastcasting Benchmark for Forecasting Agents",
        "title": "Bench to the Future - A Pastcasting Benchmark for Forecasting Agents",
        "tags": [
          "project"
        ],
        "content": "#project \n\nhttps://arxiv.org/abs/2506.21558\nhttps://evals.futuresearch.ai/\n\nEstablishes a \"pastcasting\" benchmark that evaluates AI forecasting capabilities by having LLMs predict outcomes of events that have already happened, using carefully curated web snapshots from before those events occurred. The benchmark addresses a key challenge in forecasting research - the long wait times between making predictions and learning outcomes.",
        "size": 21.142857142857142,
        "index": 383,
        "x": -731.0795440958706,
        "y": -611.1986198161612,
        "vy": -0.4275064604701327,
        "vx": -0.3280891561207337
      },
      "target": {
        "id": "Dan Schwarz",
        "title": "Dan Schwarz",
        "tags": [
          "person"
        ],
        "content": "#person \n\nSome writing about [Google's internal prediction markets](https://asteriskmag.com/issues/08/the-death-and-life-of-prediction-markets-at-google).",
        "size": 18.285714285714285,
        "index": 404,
        "x": -831.4781389544952,
        "y": -708.941876393585,
        "vy": -0.3657625739173525,
        "vx": -0.3779117138521553
      },
      "index": 532
    },
    {
      "source": {
        "id": "Bench to the Future - A Pastcasting Benchmark for Forecasting Agents",
        "title": "Bench to the Future - A Pastcasting Benchmark for Forecasting Agents",
        "tags": [
          "project"
        ],
        "content": "#project \n\nhttps://arxiv.org/abs/2506.21558\nhttps://evals.futuresearch.ai/\n\nEstablishes a \"pastcasting\" benchmark that evaluates AI forecasting capabilities by having LLMs predict outcomes of events that have already happened, using carefully curated web snapshots from before those events occurred. The benchmark addresses a key challenge in forecasting research - the long wait times between making predictions and learning outcomes.",
        "size": 21.142857142857142,
        "index": 383,
        "x": -731.0795440958706,
        "y": -611.1986198161612,
        "vy": -0.4275064604701327,
        "vx": -0.3280891561207337
      },
      "target": {
        "id": "Lawrence Phillips",
        "title": "Lawrence Phillips",
        "tags": [
          "person"
        ],
        "content": "#person",
        "size": 18.285714285714285,
        "index": 239,
        "x": -786.997452942158,
        "y": -671.6640862022364,
        "vy": -0.3955253133044403,
        "vx": -0.3629004531354729
      },
      "index": 533
    },
    {
      "source": {
        "id": "Multi-Agent Consensus Seeking via Large Language Models",
        "title": "Multi-Agent Consensus Seeking via Large Language Models",
        "tags": [
          "project"
        ],
        "content": "#project \n\nhttps://arxiv.org/abs/2310.20151\n\nPaper studies how multiple AI agents powered by large language models can reach consensus through negotiation, where each agent starts with a numerical value and must collectively agree on a final shared value. The research reveals that LLM agents naturally tend to use averaging strategies for consensus-seeking and that factors like agent personality (stubborn vs. suggestible), network topology, and group size significantly influence both the speed and outcome of negotiations.",
        "size": 18.285714285714285,
        "index": 384,
        "x": -1717.4694519754769,
        "y": 2825.164101417893,
        "vy": -0.05908329557207245,
        "vx": 1.252093697801418
      },
      "target": {
        "id": "Huaben Chen",
        "title": "Huaben Chen",
        "tags": [
          "person"
        ],
        "content": "#person",
        "size": 16.571428571428573,
        "index": 132,
        "x": -1772.3711753708837,
        "y": 2785.444406256389,
        "vy": -0.5681609577201365,
        "vx": 1.4998186849327853
      },
      "index": 534
    },
    {
      "source": {
        "id": "Multi-Agent Consensus Seeking via Large Language Models",
        "title": "Multi-Agent Consensus Seeking via Large Language Models",
        "tags": [
          "project"
        ],
        "content": "#project \n\nhttps://arxiv.org/abs/2310.20151\n\nPaper studies how multiple AI agents powered by large language models can reach consensus through negotiation, where each agent starts with a numerical value and must collectively agree on a final shared value. The research reveals that LLM agents naturally tend to use averaging strategies for consensus-seeking and that factors like agent personality (stubborn vs. suggestible), network topology, and group size significantly influence both the speed and outcome of negotiations.",
        "size": 18.285714285714285,
        "index": 384,
        "x": -1717.4694519754769,
        "y": 2825.164101417893,
        "vy": -0.05908329557207245,
        "vx": 1.252093697801418
      },
      "target": {
        "id": "Wenkang Ji",
        "title": "Wenkang Ji",
        "tags": [
          "person"
        ],
        "content": "#person",
        "size": 16.571428571428573,
        "index": 90,
        "x": -1680.132786153885,
        "y": 2769.8636134537037,
        "vy": 0.12011380033144903,
        "vx": 1.1858783948353995
      },
      "index": 535
    },
    {
      "source": {
        "id": "Multi-Agent Consensus Seeking via Large Language Models",
        "title": "Multi-Agent Consensus Seeking via Large Language Models",
        "tags": [
          "project"
        ],
        "content": "#project \n\nhttps://arxiv.org/abs/2310.20151\n\nPaper studies how multiple AI agents powered by large language models can reach consensus through negotiation, where each agent starts with a numerical value and must collectively agree on a final shared value. The research reveals that LLM agents naturally tend to use averaging strategies for consensus-seeking and that factors like agent personality (stubborn vs. suggestible), network topology, and group size significantly influence both the speed and outcome of negotiations.",
        "size": 18.285714285714285,
        "index": 384,
        "x": -1717.4694519754769,
        "y": 2825.164101417893,
        "vy": -0.05908329557207245,
        "vx": 1.252093697801418
      },
      "target": {
        "id": "Lufeng Xu",
        "title": "Lufeng Xu",
        "tags": [
          "person"
        ],
        "content": "#person",
        "size": 16.571428571428573,
        "index": 410,
        "x": -1757.090913567145,
        "y": 2879.0580800670987,
        "vy": -0.08958956156833082,
        "vx": 1.383548970612921
      },
      "index": 536
    },
    {
      "source": {
        "id": "Multi-Agent Consensus Seeking via Large Language Models",
        "title": "Multi-Agent Consensus Seeking via Large Language Models",
        "tags": [
          "project"
        ],
        "content": "#project \n\nhttps://arxiv.org/abs/2310.20151\n\nPaper studies how multiple AI agents powered by large language models can reach consensus through negotiation, where each agent starts with a numerical value and must collectively agree on a final shared value. The research reveals that LLM agents naturally tend to use averaging strategies for consensus-seeking and that factors like agent personality (stubborn vs. suggestible), network topology, and group size significantly influence both the speed and outcome of negotiations.",
        "size": 18.285714285714285,
        "index": 384,
        "x": -1717.4694519754769,
        "y": 2825.164101417893,
        "vy": -0.05908329557207245,
        "vx": 1.252093697801418
      },
      "target": {
        "id": "Shiyu Zhao",
        "title": "Shiyu Zhao",
        "tags": [
          "person"
        ],
        "content": "#person",
        "size": 16.571428571428573,
        "index": 212,
        "x": -1661.961705989539,
        "y": 2862.074841626482,
        "vy": -0.4825403287650243,
        "vx": 1.7624908606330056
      },
      "index": 537
    },
    {
      "source": {
        "id": "Online Deliberation Platform",
        "title": "Online Deliberation Platform",
        "tags": [
          "project"
        ],
        "content": "#project\n\nhttps://stanforddeliberate.org/\n\nStanford Deliberate is an AI-assisted online video platform designed to facilitate structured small group discussions using deliberative polling methodology, with an automated moderator that manages speaking queues, timed agendas, and ensures equitable participation among participants. The platform has been used for large-scale national deliberative polling events across multiple countries and languages, allowing unlimited participants to deliberate simultaneously in small groups on policy issues.",
        "size": 16.571428571428573,
        "index": 385,
        "x": 779.8822167499907,
        "y": 1495.1953011555815,
        "vy": -1.2280962678241771,
        "vx": 0.497580297681657
      },
      "target": {
        "id": "Deliberative Democracy Lab",
        "title": "Deliberative Democracy Lab",
        "tags": [
          "organization"
        ],
        "content": "#organization \n\nhttps://deliberation.stanford.edu/",
        "size": 17.142857142857142,
        "index": 408,
        "x": 847.3404089335676,
        "y": 1390.6348583925135,
        "vy": -1.2492320205522307,
        "vx": 0.46651654763471045
      },
      "index": 538
    },
    {
      "source": {
        "id": "How AI Agents Will Improve the Consultation Process",
        "title": "How AI Agents Will Improve the Consultation Process",
        "tags": [
          "project"
        ],
        "content": "#project \n\nhttps://ai.objectives.institute/blog/how-ai-agents-will-improve-consultation-process\n\nBlog post describes how AI agents can transform consultation processes by making them more interactive and engaging than traditional surveys. The AI Objectives Institute proposes using different types of AI agents: \n1. \"domain experts\" to help participants understand complex questions and organize their thoughts, \n2. \"professional interviewers\" to help refine and improve responses, and \n3. \"cross-pollination\" agents to help participants exchange ideas and find consensus by synthesizing diverse viewpoints.",
        "size": 16.571428571428573,
        "index": 386,
        "x": 655.732779664985,
        "y": -186.75619409967763,
        "vy": -1.1307629805623067,
        "vx": 0.049491168764767376
      },
      "target": {
        "id": "Bruno Marnette",
        "title": "Bruno Marnette",
        "tags": [
          "person"
        ],
        "content": "#person \n\nhttps://brunomarnette.substack.com/",
        "size": 17.714285714285715,
        "index": 129,
        "x": 731.6434970566758,
        "y": -167.79134490423854,
        "vy": -1.1638735314984259,
        "vx": -0.004925001736821977
      },
      "index": 539
    },
    {
      "source": {
        "id": "Toby Shevlane",
        "title": "Toby Shevlane",
        "tags": [
          "person"
        ],
        "content": "#person",
        "size": 16.571428571428573,
        "index": 387,
        "x": -484.5355005677755,
        "y": -986.3923507962332,
        "vy": -0.3173739960044455,
        "vx": -0.5674595202647124
      },
      "target": {
        "id": "Mantic",
        "title": "Mantic",
        "tags": [
          "organization"
        ],
        "content": "#organization \n\nhttps://mntc.ai/",
        "size": 17.714285714285715,
        "index": 275,
        "x": -472.4300783671211,
        "y": -887.3407076068335,
        "vy": -0.49223288520611863,
        "vx": -0.32327819541462854
      },
      "index": 540
    },
    {
      "source": {
        "id": "Jorim Theuns",
        "title": "Jorim Theuns",
        "tags": [
          "person"
        ],
        "content": "#person \n\nhttps://jtheuns.com/",
        "size": 17.142857142857142,
        "index": 388,
        "x": 3740.476074878806,
        "y": 1048.2696899969626,
        "vy": -1.9598091972020897,
        "vx": 0.38373243366361753
      },
      "target": {
        "id": "Dembrane",
        "title": "Dembrane",
        "tags": [
          "organization"
        ],
        "content": "#organization \n\nhttps://www.dembrane.com\n\nFunded (in part) by the European Union via the [AI4Deliberation](https://www.ai4dproject.eu/) project.",
        "size": 17.714285714285715,
        "index": 336,
        "x": 3670.5319699200368,
        "y": 1017.8958794711315,
        "vy": -1.8573349240868908,
        "vx": 0.2592679715006362
      },
      "index": 541
    },
    {
      "source": {
        "id": "Jungwon Byun",
        "title": "Jungwon Byun",
        "tags": [
          "person"
        ],
        "content": "#person",
        "size": 17.714285714285715,
        "index": 392,
        "x": -1943.1560321890236,
        "y": 443.664957671305,
        "vy": -0.16095307487519664,
        "vx": 0.253426240202621
      },
      "target": {
        "id": "Elicit",
        "title": "Elicit",
        "tags": [
          "organization"
        ],
        "content": "#organization \n\nhttps://elicit.com/",
        "size": 20.57142857142857,
        "index": 218,
        "x": -2027.2873240845167,
        "y": 375.96539772345454,
        "vy": -0.29711133317878174,
        "vx": 0.26985144319061544
      },
      "index": 542
    },
    {
      "source": {
        "id": "Jungwon Byun",
        "title": "Jungwon Byun",
        "tags": [
          "person"
        ],
        "content": "#person",
        "size": 17.714285714285715,
        "index": 392,
        "x": -1943.1560321890236,
        "y": 443.664957671305,
        "vy": -0.16095307487519664,
        "vx": 0.253426240202621
      },
      "target": {
        "id": "Ought",
        "title": "Ought",
        "tags": [
          "organization"
        ],
        "content": "#organization \n\nhttps://ought.org/ (website is old)\n\nOught 1.0 has effectively become what is now Elicit (the company) continuing development on Elicit (the tool). Ought 2.0 has not yet done any public work.",
        "size": 23.42857142857143,
        "index": 306,
        "x": -1888.3722954977052,
        "y": 453.0511056583274,
        "vy": -0.2547035865836596,
        "vx": 0.0471838457037637
      },
      "index": 543
    },
    {
      "source": {
        "id": "Manuel Wüthrich",
        "title": "Manuel Wüthrich",
        "tags": [
          "person"
        ],
        "content": "#person",
        "size": 17.714285714285715,
        "index": 393,
        "x": 2220.4196169294496,
        "y": 182.2761367020348,
        "vy": -1.9336271138999594,
        "vx": -0.03418308573307342
      },
      "target": {
        "id": "Midjourney",
        "title": "Midjourney",
        "tags": [
          "organization"
        ],
        "content": "#organization \n\nhttps://www.midjourney.com\n\nThe Collective Intelligence team at Midjourney (currently stealth).",
        "size": 17.142857142857142,
        "index": 348,
        "x": 2062.822718578165,
        "y": 201.81415254647143,
        "vy": -1.9018493554178864,
        "vx": 0.1061578931981707
      },
      "index": 544
    },
    {
      "source": {
        "id": "Pattie Maes",
        "title": "Pattie Maes",
        "tags": [
          "person"
        ],
        "content": "#person",
        "size": 17.714285714285715,
        "index": 396,
        "x": 974.5602589949608,
        "y": -2755.20656340418,
        "vy": -1.1595146505930813,
        "vx": -0.4926021629325841
      },
      "target": {
        "id": "MIT Media Lab",
        "title": "MIT Media Lab",
        "tags": [
          "organization"
        ],
        "content": "#organization \n\nhttps://www.media.mit.edu/",
        "size": 17.714285714285715,
        "index": 269,
        "x": 1006.2843605632936,
        "y": -2684.9292905918865,
        "vy": -1.0825039215382666,
        "vx": -0.515555946443613
      },
      "index": 545
    },
    {
      "source": {
        "id": "Lucid Lens",
        "title": "Lucid Lens",
        "tags": [
          "project"
        ],
        "content": "#project \n\nhttps://ai.objectives.institute/blog/straightlines-surfaces-the-content-beneath-the-headline\nhttps://github.com/AIObjectives/lucidlens\n\nChrome extension that automatically rewrites sensational or misleading news headlines to more accurately reflect the actual content of articles, using AI to read and summarize the full text before generating clearer headlines.",
        "size": 17.714285714285715,
        "index": 400,
        "x": 689.1935988815694,
        "y": -503.81105078624074,
        "vy": -1.1660902259956802,
        "vx": -0.24641032725372922
      },
      "target": {
        "id": "Alek Chakroff",
        "title": "Alek Chakroff",
        "tags": [
          "person"
        ],
        "content": "#person",
        "size": 16.571428571428573,
        "index": 29,
        "x": 703.5530019082382,
        "y": -617.439855261721,
        "vy": -1.1663790104745284,
        "vx": -0.2765048066656935
      },
      "index": 546
    },
    {
      "source": {
        "id": "Lucid Lens",
        "title": "Lucid Lens",
        "tags": [
          "project"
        ],
        "content": "#project \n\nhttps://ai.objectives.institute/blog/straightlines-surfaces-the-content-beneath-the-headline\nhttps://github.com/AIObjectives/lucidlens\n\nChrome extension that automatically rewrites sensational or misleading news headlines to more accurately reflect the actual content of articles, using AI to read and summarize the full text before generating clearer headlines.",
        "size": 17.714285714285715,
        "index": 400,
        "x": 689.1935988815694,
        "y": -503.81105078624074,
        "vy": -1.1660902259956802,
        "vx": -0.24641032725372922
      },
      "target": {
        "id": "Justin Stimatze",
        "title": "Justin Stimatze",
        "tags": [
          "person"
        ],
        "content": "#person",
        "size": 17.142857142857142,
        "index": 356,
        "x": 662.5538728575052,
        "y": -411.9857351861423,
        "vy": -1.1393261113953697,
        "vx": -0.18924034901002387
      },
      "index": 547
    },
    {
      "source": {
        "id": "Lucid Lens",
        "title": "Lucid Lens",
        "tags": [
          "project"
        ],
        "content": "#project \n\nhttps://ai.objectives.institute/blog/straightlines-surfaces-the-content-beneath-the-headline\nhttps://github.com/AIObjectives/lucidlens\n\nChrome extension that automatically rewrites sensational or misleading news headlines to more accurately reflect the actual content of articles, using AI to read and summarize the full text before generating clearer headlines.",
        "size": 17.714285714285715,
        "index": 400,
        "x": 689.1935988815694,
        "y": -503.81105078624074,
        "vy": -1.1660902259956802,
        "vx": -0.24641032725372922
      },
      "target": {
        "id": "Natasha Jensen",
        "title": "Natasha Jensen",
        "tags": [
          "person"
        ],
        "content": "#person",
        "size": 17.142857142857142,
        "index": 418,
        "x": 692.9581003769802,
        "y": -362.25848907181665,
        "vy": -1.1841351233367319,
        "vx": -0.11582662964557375
      },
      "index": 548
    },
    {
      "source": {
        "id": "Ian Baker",
        "title": "Ian Baker",
        "tags": [
          "person"
        ],
        "content": "#person",
        "size": 17.142857142857142,
        "index": 401,
        "x": 1649.2806837888666,
        "y": 1143.6348851705998,
        "vy": -1.6785613446472192,
        "vx": 0.5623285705514641
      },
      "target": {
        "id": "Plurality Institute",
        "title": "Plurality Institute",
        "tags": [
          "organization"
        ],
        "content": "#organization",
        "size": 20,
        "index": 364,
        "x": 1548.928220590021,
        "y": 1070.4694466931655,
        "vy": -1.5138983339043548,
        "vx": 0.41914174319685277
      },
      "index": 549
    },
    {
      "source": {
        "id": "Dan Schwarz",
        "title": "Dan Schwarz",
        "tags": [
          "person"
        ],
        "content": "#person \n\nSome writing about [Google's internal prediction markets](https://asteriskmag.com/issues/08/the-death-and-life-of-prediction-markets-at-google).",
        "size": 18.285714285714285,
        "index": 404,
        "x": -831.4781389544952,
        "y": -708.941876393585,
        "vy": -0.3657625739173525,
        "vx": -0.3779117138521553
      },
      "target": {
        "id": "Future Search",
        "title": "Future Search",
        "tags": [
          "organization"
        ],
        "content": "#organization \n\nhttps://futuresearch.ai/",
        "size": 22.285714285714285,
        "index": 94,
        "x": -717.8526553292105,
        "y": -719.7276361443852,
        "vy": -0.4404383940953807,
        "vx": -0.3824839533244458
      },
      "index": 550
    },
    {
      "source": {
        "id": "Ben Turtel",
        "title": "Ben Turtel",
        "tags": [
          "person"
        ],
        "content": "#person",
        "size": 17.714285714285715,
        "index": 405,
        "x": -483.86316913010216,
        "y": -469.8714481131538,
        "vy": -0.5954542916718669,
        "vx": 0.08327716391928561
      },
      "target": {
        "id": "Lightning Rod Labs",
        "title": "Lightning Rod Labs",
        "tags": [
          "organization"
        ],
        "content": "#organization \n\nhttps://www.lightningrod.ai/",
        "size": 18.285714285714285,
        "index": 99,
        "x": -485.4756382927939,
        "y": -568.3602203235023,
        "vy": -0.5387280910122677,
        "vx": -0.015714766705498383
      },
      "index": 551
    },
    {
      "source": {
        "id": "Houtan Bastani",
        "title": "Houtan Bastani",
        "tags": [
          "person"
        ],
        "content": "#person",
        "size": 17.142857142857142,
        "index": 411,
        "x": -167.68177451662453,
        "y": 10.18398720815008,
        "vy": -0.7666032941425005,
        "vx": 0.2770692053409178
      },
      "target": {
        "id": "Forecasting Research Institute",
        "title": "Forecasting Research Institute",
        "tags": [
          "organization"
        ],
        "content": "#organization \n\nhttps://forecastingresearch.org/",
        "size": 18.857142857142858,
        "index": 317,
        "x": -153.6122917815696,
        "y": -87.61093230240742,
        "vy": -0.8005381090437106,
        "vx": 0.1603930148987801
      },
      "index": 552
    },
    {
      "source": {
        "id": "Charlie George",
        "title": "Charlie George",
        "tags": [
          "person"
        ],
        "content": "#person",
        "size": 17.714285714285715,
        "index": 412,
        "x": -1942.725667713989,
        "y": 384.2379487239744,
        "vy": -0.16115006137629795,
        "vx": 0.22652937902231374
      },
      "target": {
        "id": "Elicit",
        "title": "Elicit",
        "tags": [
          "organization"
        ],
        "content": "#organization \n\nhttps://elicit.com/",
        "size": 20.57142857142857,
        "index": 218,
        "x": -2027.2873240845167,
        "y": 375.96539772345454,
        "vy": -0.29711133317878174,
        "vx": 0.26985144319061544
      },
      "index": 553
    },
    {
      "source": {
        "id": "Charlie George",
        "title": "Charlie George",
        "tags": [
          "person"
        ],
        "content": "#person",
        "size": 17.714285714285715,
        "index": 412,
        "x": -1942.725667713989,
        "y": 384.2379487239744,
        "vy": -0.16115006137629795,
        "vx": 0.22652937902231374
      },
      "target": {
        "id": "Ought",
        "title": "Ought",
        "tags": [
          "organization"
        ],
        "content": "#organization \n\nhttps://ought.org/ (website is old)\n\nOught 1.0 has effectively become what is now Elicit (the company) continuing development on Elicit (the tool). Ought 2.0 has not yet done any public work.",
        "size": 23.42857142857143,
        "index": 306,
        "x": -1888.3722954977052,
        "y": 453.0511056583274,
        "vy": -0.2547035865836596,
        "vx": 0.0471838457037637
      },
      "index": 554
    },
    {
      "source": {
        "id": "Daniel Hnyk",
        "title": "Daniel Hnyk",
        "tags": [
          "person"
        ],
        "content": "#person",
        "size": 17.714285714285715,
        "index": 413,
        "x": -788.7242426021298,
        "y": -636.2606637787694,
        "vy": -0.37777225834080214,
        "vx": -0.34973522484873115
      },
      "target": {
        "id": "Future Search",
        "title": "Future Search",
        "tags": [
          "organization"
        ],
        "content": "#organization \n\nhttps://futuresearch.ai/",
        "size": 22.285714285714285,
        "index": 94,
        "x": -717.8526553292105,
        "y": -719.7276361443852,
        "vy": -0.4404383940953807,
        "vx": -0.3824839533244458
      },
      "index": 555
    },
    {
      "source": {
        "id": "Divya Siddarth",
        "title": "Divya Siddarth",
        "tags": [
          "person"
        ],
        "content": "#person \n\nhttps://divyasiddarth.com/",
        "size": 17.714285714285715,
        "index": 414,
        "x": 1251.3647810817142,
        "y": 910.2533943640935,
        "vy": -1.2814979219729066,
        "vx": 0.3150773430583668
      },
      "target": {
        "id": "Collective Intelligence Project",
        "title": "Collective Intelligence Project",
        "tags": [
          "organization"
        ],
        "content": "#organization \n\nhttps://www.cip.org/",
        "size": 18.285714285714285,
        "index": 71,
        "x": 1196.948459287815,
        "y": 832.5435137077104,
        "vy": -1.3117327264452452,
        "vx": 0.30198095758611615
      },
      "index": 556
    },
    {
      "source": {
        "id": "Natasha Jensen",
        "title": "Natasha Jensen",
        "tags": [
          "person"
        ],
        "content": "#person",
        "size": 17.142857142857142,
        "index": 418,
        "x": 692.9581003769802,
        "y": -362.25848907181665,
        "vy": -1.1841351233367319,
        "vx": -0.11582662964557375
      },
      "target": {
        "id": "AI Objectives Institute",
        "title": "AI Objectives Institute",
        "tags": [
          "organization"
        ],
        "content": "#organization \n\nhttps://ai.objectives.institute/",
        "size": 20.57142857142857,
        "index": 304,
        "x": 665.7615191436854,
        "y": -247.07081557119588,
        "vy": -1.1515300453530217,
        "vx": -0.07444975689524554
      },
      "index": 557
    },
    {
      "source": {
        "id": "Don't Just Tell Me, Ask Me",
        "title": "Don't Just Tell Me, Ask Me",
        "tags": [
          "project"
        ],
        "content": "#project \n\nhttps://dl.acm.org/doi/pdf/10.1145/3544548.3580672\n\nResearch paper presents \"AI-framed Questioning\" - a method where AI systems ask users strategic questions instead of directly telling them answers, designed to improve human critical thinking and logical reasoning. The study found that when people were asked questions like \"If (premise), does it follow that (conclusion)?\" they performed significantly better at identifying logical fallacies in socially divisive statements compared to both receiving direct AI explanations or no assistance at all.",
        "size": 18.285714285714285,
        "index": 419,
        "x": 1065.3486294410825,
        "y": -2781.468046533273,
        "vy": -1.0894069288343675,
        "vx": -0.5144498983365884
      },
      "target": {
        "id": "Valdemar Danry",
        "title": "Valdemar Danry",
        "tags": [
          "person"
        ],
        "content": "#person\n\nhttps://valdemardanry.com/",
        "size": 18.857142857142858,
        "index": 172,
        "x": 1102.4705595757653,
        "y": -2731.308366831243,
        "vy": -1.122899896351339,
        "vx": -0.5144777169524257
      },
      "index": 558
    },
    {
      "source": {
        "id": "Don't Just Tell Me, Ask Me",
        "title": "Don't Just Tell Me, Ask Me",
        "tags": [
          "project"
        ],
        "content": "#project \n\nhttps://dl.acm.org/doi/pdf/10.1145/3544548.3580672\n\nResearch paper presents \"AI-framed Questioning\" - a method where AI systems ask users strategic questions instead of directly telling them answers, designed to improve human critical thinking and logical reasoning. The study found that when people were asked questions like \"If (premise), does it follow that (conclusion)?\" they performed significantly better at identifying logical fallacies in socially divisive statements compared to both receiving direct AI explanations or no assistance at all.",
        "size": 18.285714285714285,
        "index": 419,
        "x": 1065.3486294410825,
        "y": -2781.468046533273,
        "vy": -1.0894069288343675,
        "vx": -0.5144498983365884
      },
      "target": {
        "id": "Pat Pataranutaporn",
        "title": "Pat Pataranutaporn",
        "tags": [
          "person"
        ],
        "content": "#person \n\nhttps://patpat.world/",
        "size": 18.285714285714285,
        "index": 298,
        "x": 1066.2378443172674,
        "y": -2682.567814798428,
        "vy": -1.1214877655381226,
        "vx": -0.5139886416200151
      },
      "index": 559
    },
    {
      "source": {
        "id": "Don't Just Tell Me, Ask Me",
        "title": "Don't Just Tell Me, Ask Me",
        "tags": [
          "project"
        ],
        "content": "#project \n\nhttps://dl.acm.org/doi/pdf/10.1145/3544548.3580672\n\nResearch paper presents \"AI-framed Questioning\" - a method where AI systems ask users strategic questions instead of directly telling them answers, designed to improve human critical thinking and logical reasoning. The study found that when people were asked questions like \"If (premise), does it follow that (conclusion)?\" they performed significantly better at identifying logical fallacies in socially divisive statements compared to both receiving direct AI explanations or no assistance at all.",
        "size": 18.285714285714285,
        "index": 419,
        "x": 1065.3486294410825,
        "y": -2781.468046533273,
        "vy": -1.0894069288343675,
        "vx": -0.5144498983365884
      },
      "target": {
        "id": "Yaoli Mao",
        "title": "Yaoli Mao",
        "tags": [
          "person"
        ],
        "content": "#person",
        "size": 17.142857142857142,
        "index": 424,
        "x": 1013.6770029340609,
        "y": -2810.870854276536,
        "vy": -1.1259491261914438,
        "vx": -0.4540425695118114
      },
      "index": 560
    },
    {
      "source": {
        "id": "Don't Just Tell Me, Ask Me",
        "title": "Don't Just Tell Me, Ask Me",
        "tags": [
          "project"
        ],
        "content": "#project \n\nhttps://dl.acm.org/doi/pdf/10.1145/3544548.3580672\n\nResearch paper presents \"AI-framed Questioning\" - a method where AI systems ask users strategic questions instead of directly telling them answers, designed to improve human critical thinking and logical reasoning. The study found that when people were asked questions like \"If (premise), does it follow that (conclusion)?\" they performed significantly better at identifying logical fallacies in socially divisive statements compared to both receiving direct AI explanations or no assistance at all.",
        "size": 18.285714285714285,
        "index": 419,
        "x": 1065.3486294410825,
        "y": -2781.468046533273,
        "vy": -1.0894069288343675,
        "vx": -0.5144498983365884
      },
      "target": {
        "id": "Pattie Maes",
        "title": "Pattie Maes",
        "tags": [
          "person"
        ],
        "content": "#person",
        "size": 17.714285714285715,
        "index": 396,
        "x": 974.5602589949608,
        "y": -2755.20656340418,
        "vy": -1.1595146505930813,
        "vx": -0.4926021629325841
      },
      "index": 561
    },
    {
      "source": {
        "id": "Rob Gordon",
        "title": "Rob Gordon",
        "tags": [
          "person"
        ],
        "content": "#person",
        "size": 16.571428571428573,
        "index": 427,
        "x": -580.8594683434645,
        "y": -938.8235871374885,
        "vy": -0.37232802923992997,
        "vx": -0.26711286726318384
      },
      "target": {
        "id": "Goodheart Labs",
        "title": "Goodheart Labs",
        "tags": [
          "organization"
        ],
        "content": "#organization \n\nhttps://goodheartlabs.com/",
        "size": 17.714285714285715,
        "index": 229,
        "x": -536.1165178685933,
        "y": -862.8993316329166,
        "vy": -0.48917270690236947,
        "vx": -0.3239942741752614
      },
      "index": 562
    },
    {
      "source": {
        "id": "Ben Day",
        "title": "Ben Day",
        "tags": [
          "person"
        ],
        "content": "#person",
        "size": 16.571428571428573,
        "index": 428,
        "x": -427.6532491184252,
        "y": -980.951001789479,
        "vy": -0.6322899034057724,
        "vx": -0.536506530077704
      },
      "target": {
        "id": "Mantic",
        "title": "Mantic",
        "tags": [
          "organization"
        ],
        "content": "#organization \n\nhttps://mntc.ai/",
        "size": 17.714285714285715,
        "index": 275,
        "x": -472.4300783671211,
        "y": -887.3407076068335,
        "vy": -0.49223288520611863,
        "vx": -0.32327819541462854
      },
      "index": 563
    },
    {
      "source": {
        "id": "Pantheon",
        "title": "Pantheon",
        "tags": [
          "project"
        ],
        "content": "#project \n\nhttps://pantheon.chat/\n\nExperimental AI tool that reverses the typical human-AI interaction pattern by having LLM-powered \"daemons\" proactively respond to and comment on a user's stream of consciousness writing, rather than waiting for human prompts. The system allows users to write their thoughts in a diary-like format while customizable AI advisors offer questions, insights, and challenges designed to stimulate deeper thinking and explore new directions.",
        "size": 17.142857142857142,
        "index": 429,
        "x": 3613.1899275864844,
        "y": -1005.5420987577774,
        "vy": -0.977246001490673,
        "vx": 0.15880672004855584
      },
      "target": {
        "id": "Sofi Vanhanen",
        "title": "Sofi Vanhanen",
        "tags": [
          "person"
        ],
        "content": "#person \n\nhttps://sofiavanhanen.fi/",
        "size": 18.285714285714285,
        "index": 177,
        "x": 3629.495175318134,
        "y": -1087.890476428783,
        "vy": -0.9980357709893994,
        "vx": 0.19329731806245978
      },
      "index": 564
    },
    {
      "source": {
        "id": "Pantheon",
        "title": "Pantheon",
        "tags": [
          "project"
        ],
        "content": "#project \n\nhttps://pantheon.chat/\n\nExperimental AI tool that reverses the typical human-AI interaction pattern by having LLM-powered \"daemons\" proactively respond to and comment on a user's stream of consciousness writing, rather than waiting for human prompts. The system allows users to write their thoughts in a diary-like format while customizable AI advisors offer questions, insights, and challenges designed to stimulate deeper thinking and explore new directions.",
        "size": 17.142857142857142,
        "index": 429,
        "x": 3613.1899275864844,
        "y": -1005.5420987577774,
        "vy": -0.977246001490673,
        "vx": 0.15880672004855584
      },
      "target": {
        "id": "Niki Dupuis",
        "title": "Niki Dupuis",
        "tags": [
          "person"
        ],
        "content": "#person",
        "size": 17.714285714285715,
        "index": 65,
        "x": 3688.7093262635094,
        "y": -985.8406135311669,
        "vy": -0.9897340634814135,
        "vx": 0.17742209799442776
      },
      "index": 565
    },
    {
      "source": {
        "id": "Saffron Huang",
        "title": "Saffron Huang",
        "tags": [
          "person"
        ],
        "content": "#person",
        "size": 17.142857142857142,
        "index": 431,
        "x": 1138.196955949223,
        "y": 884.9737635766728,
        "vy": -1.2588665998709374,
        "vx": 0.37285374359340917
      },
      "target": {
        "id": "Collective Intelligence Project",
        "title": "Collective Intelligence Project",
        "tags": [
          "organization"
        ],
        "content": "#organization \n\nhttps://www.cip.org/",
        "size": 18.285714285714285,
        "index": 71,
        "x": 1196.948459287815,
        "y": 832.5435137077104,
        "vy": -1.3117327264452452,
        "vx": 0.30198095758611615
      },
      "index": 566
    },
    {
      "source": {
        "id": "Beth Goldberg",
        "title": "Beth Goldberg",
        "tags": [
          "person"
        ],
        "content": "#person",
        "size": 17.142857142857142,
        "index": 436,
        "x": 1614.3717603909563,
        "y": 961.5912998628878,
        "vy": -1.4139817481595707,
        "vx": 0.2944467547795422
      },
      "target": {
        "id": "Google Jigsaw",
        "title": "Google Jigsaw",
        "tags": [
          "organization"
        ],
        "content": "#organization \n\nhttps://jigsaw.google.com/",
        "size": 19.428571428571427,
        "index": 390,
        "x": 1784.38820840888,
        "y": 1035.7563657120636,
        "vy": -1.5493284452037834,
        "vx": 0.4016872013074959
      },
      "index": 567
    },
    {
      "source": {
        "id": "Peter Darche",
        "title": "Peter Darche",
        "tags": [
          "person"
        ],
        "content": "#person",
        "size": 17.142857142857142,
        "index": 437,
        "x": 1703.5768193864037,
        "y": 1164.827931289991,
        "vy": -1.6237876670598477,
        "vx": 0.5410514915146413
      },
      "target": {
        "id": "Plurality Institute",
        "title": "Plurality Institute",
        "tags": [
          "organization"
        ],
        "content": "#organization",
        "size": 20,
        "index": 364,
        "x": 1548.928220590021,
        "y": 1070.4694466931655,
        "vy": -1.5138983339043548,
        "vx": 0.41914174319685277
      },
      "index": 568
    }
  ]
}