{
  "nodes": [
    {
      "id": "Mapping the Discourse on AI Safety & Ethics",
      "title": "Mapping the Discourse on AI Safety & Ethics",
      "tags": [
        "project"
      ],
      "content": "#project \n\nhttps://ai.objectives.institute/blog/mapping-the-discourse-on-ai-safety-amp-ethics\n\nProject by the AI Objectives Institute that used their \"Talk to the City\" tool to analyze and map Twitter conversations about AI safety and ethics, identifying six distinct perspectives ranging from near-term harms to long-term existential risks. The tool automatically clustered thousands of tweets to reveal different viewpoints on AI governance, safety approaches, and development concerns, finding significant overlap between groups despite their different priorities."
    },
    {
      "id": "Lei Nelissen",
      "title": "Lei Nelissen",
      "tags": [
        "person"
      ],
      "content": "#person"
    },
    {
      "id": "ACE - A LLM-based Negotiation Coaching System",
      "title": "ACE - A LLM-based Negotiation Coaching System",
      "tags": [
        "project"
      ],
      "content": "#project \n\nhttps://arxiv.org/pdf/2410.01555\n\nLLM-based system that provides targeted feedback to help users improve their bargaining skills. The system identifies specific negotiation mistakes using an annotation scheme developed with [MBA](https://en.wikipedia.org/wiki/Master_of_Business_Administration) instructors, then provides detailed feedback on preparation strategies (like setting appropriate target prices) and in-conversation tactics (like including rationales with offers and strategic closing behavior). Through a controlled experiment with 374 participants across two negotiation rounds, the researchers demonstrated that users who received ACE's coaching significantly improved their objective negotiation outcomes compared to control groups."
    },
    {
      "id": "Zhehui Liao",
      "title": "Zhehui Liao",
      "tags": [
        "person"
      ],
      "content": "#person"
    },
    {
      "id": "Jonathan Stray",
      "title": "Jonathan Stray",
      "tags": [
        "person"
      ],
      "content": "#person \n\nhttp://jonathanstray.com/"
    },
    {
      "id": "David Rand",
      "title": "David Rand",
      "tags": [
        "person"
      ],
      "content": "#person \n"
    },
    {
      "id": "Remesh",
      "title": "Remesh",
      "tags": [
        "organization"
      ],
      "content": "#organization \n\nhttps://www.remesh.ai"
    },
    {
      "id": "Amelia Glaese",
      "title": "Amelia Glaese",
      "tags": [
        "person"
      ],
      "content": "#person"
    },
    {
      "id": "Amy Zhang",
      "title": "Amy Zhang",
      "tags": [
        "person"
      ],
      "content": "#person \n\nhttps://amyzhang.github.io/"
    },
    {
      "id": "Amina Green",
      "title": "Amina Green",
      "tags": [
        "person"
      ],
      "content": "#person \n\nhttps://www.amina-io.com/"
    },
    {
      "id": "Kyle Lo",
      "title": "Kyle Lo",
      "tags": [
        "person"
      ],
      "content": "#person"
    },
    {
      "id": "Shahar Hechtlinger",
      "title": "Shahar Hechtlinger",
      "tags": [
        "person"
      ],
      "content": "#person"
    },
    {
      "id": "Zoe Rahwan",
      "title": "Zoe Rahwan",
      "tags": [
        "person"
      ],
      "content": "#person"
    },
    {
      "id": "Maggie Appleton",
      "title": "Maggie Appleton",
      "tags": [
        "person"
      ],
      "content": "#person \n\nhttps://maggieappleton.com/"
    },
    {
      "id": "Andy Zou",
      "title": "Andy Zou",
      "tags": [
        "person"
      ],
      "content": "#person"
    },
    {
      "id": "Wargames for Peace",
      "title": "Wargames for Peace",
      "tags": [
        "project"
      ],
      "content": "#project \n\nhttps://www.youtube.com/watch?v=9_zSzjTWr8A\n\n[MATS](https://www.matsprogram.org/) project to simulate  a tabletop exercises with LLMs, allowing a human to play solo."
    },
    {
      "id": "Evelien Nieuwenburg",
      "title": "Evelien Nieuwenburg",
      "tags": [
        "person"
      ],
      "content": "#person"
    },
    {
      "id": "Enhancing AI-Assisted Group Decision Making through LLM-Powered Devil’s Advocate",
      "title": "Enhancing AI-Assisted Group Decision Making through LLM-Powered Devil’s Advocate",
      "tags": [
        "project"
      ],
      "content": "#project \n\nhttps://dl.acm.org/doi/pdf/10.1145/3640543.3645199\n\nStudy tested \"LLM-Powered Devil's Advocate\" systems that use GPT-3.5 to challenge group decisions in AI-assisted tasks. Researchers ran a randomized experiment where groups made recidivism risk predictions with AI help, and some groups also had an LLM devil's advocate that either questioned the AI's recommendations or challenged the group's majority opinion. The devil's advocate used critique questions and comments to prompt deeper thinking. The key finding was that questioning AI recommendations significantly improved group accuracy, while questioning group opinions provided no benefit."
    },
    {
      "id": "Zaria Jalan",
      "title": "Zaria Jalan",
      "tags": [
        "person"
      ],
      "content": "#person"
    },
    {
      "id": "Squiggle AI",
      "title": "Squiggle AI",
      "tags": [
        "project"
      ],
      "content": "#project\n\nhttps://www.lesswrong.com/posts/7worWgggeHL3Eb7wq/introducing-squiggle-ai\n\nTool that automatically generates probabilistic cost-effectiveness models using the Squiggle programming language, allowing users to quickly create Fermi estimates and decision analyses without needing to learn programming. It combines large language models with specialized scaffolding to produce structured, adjustable models for questions like comparing charitable interventions or evaluating policy decisions."
    },
    {
      "id": "Kenny Peng",
      "title": "Kenny Peng",
      "tags": [
        "person"
      ],
      "content": "#person"
    },
    {
      "id": "Yoshua Bengio",
      "title": "Yoshua Bengio",
      "tags": [
        "person"
      ],
      "content": "#person \n\nhttps://yoshuabengio.org"
    },
    {
      "id": "Colleen McKenzie",
      "title": "Colleen McKenzie",
      "tags": [
        "person"
      ],
      "content": "#person"
    },
    {
      "id": "Reimagining Democracy for AI",
      "title": "Reimagining Democracy for AI",
      "tags": [
        "project"
      ],
      "content": "#project \n\nhttps://aviv.me/Reimagining-Democracy-for-AI-Journal-of-Democracy.pdf\n\nPaper proposes reinventing \"democratic infrastructure\" through four key innovations: \n1. representative deliberations (citizen assemblies selected by lottery), \n2. AI-augmentation of democratic processes, \n3. democracy-as-a-service organizations, and \n4. platform democracy for governing tech companies."
    },
    {
      "id": "Tom Davidson",
      "title": "Tom Davidson",
      "tags": [
        "person"
      ],
      "content": "#person \n\nhttps://tomdavidson-ai.github.io/"
    },
    {
      "id": "Joseph Chee Chang",
      "title": "Joseph Chee Chang",
      "tags": [
        "person"
      ],
      "content": "#person"
    },
    {
      "id": "AI‐assisted scenario generation for strategic planning",
      "title": "AI‐assisted scenario generation for strategic planning",
      "tags": [
        "project"
      ],
      "content": "#project \n\nhttps://onlinelibrary.wiley.com/doi/pdf/10.1002/ffo2.148\n\nThis paper explores how LLMs can be used to generate scenarios for strategic planning (for organizations/businesses), examining whether AI-produced scenarios meet professional standards and how they can best assist human decision-makers. The authors conclude that AI-assisted scenario development shows significant promise as a tool for generating \"raw material\" that human facilitators can refine."
    },
    {
      "id": "Nina Lutz",
      "title": "Nina Lutz",
      "tags": [
        "person"
      ],
      "content": "#person \n"
    },
    {
      "id": "Truthful AI",
      "title": "Truthful AI",
      "tags": [
        "project"
      ],
      "content": "#project \n\nhttps://arxiv.org/pdf/2110.06674\n\nPaper proposes developing AI systems that avoid stating falsehoods, particularly \"negligent falsehoods\" - statements that AI systems should have been able to recognize as likely false given available information. The authors argue for establishing societal standards and institutions to evaluate and certify AI truthfulness, moving beyond current approaches that rely on human-like accountability mechanisms which don't translate well to AI systems. They outline technical approaches for building more truthful AI systems and explore the governance frameworks needed to implement truthfulness standards at scale, including certification bodies and adjudication processes for evaluating AI statements."
    },
    {
      "id": "Alek Chakroff",
      "title": "Alek Chakroff",
      "tags": [
        "person"
      ],
      "content": "#person"
    },
    {
      "id": "Ariel Procaccia",
      "title": "Ariel Procaccia",
      "tags": [
        "person"
      ],
      "content": "#person \n\nhttps://procaccia.info/"
    },
    {
      "id": "Ryan Shea",
      "title": "Ryan Shea",
      "tags": [
        "person"
      ],
      "content": "#person"
    },
    {
      "id": "Hannah Sheahan",
      "title": "Hannah Sheahan",
      "tags": [
        "person"
      ],
      "content": "#person \n\nhttps://hannahsheahan.github.io/"
    },
    {
      "id": "Kevin Leyton-Brown",
      "title": "Kevin Leyton-Brown",
      "tags": [
        "person"
      ],
      "content": "#person \n"
    },
    {
      "id": "AI and the Future of Digital Public Squares",
      "title": "AI and the Future of Digital Public Squares",
      "tags": [
        "project"
      ],
      "content": "#project \n\nhttps://arxiv.org/pdf/2412.09988\n\nPaper examines how LLMs can improve online discussions and democratic participation. \n\n## Four Main Areas of Focus\n\n**1. Collective Dialogue Systems**\n- Tools that help large groups have structured conversations and find common ground on important issues.\n\n**2. Bridging Systems**\n- Algorithms that promote content bringing people together rather than showing divisive posts that drive engagement.\n\n**3. Community-Driven Moderation**\n- AI tools to help volunteer moderators manage online communities more effectively while keeping humans in control.\n\n**4. Proof of Humanity Systems**\n- Methods to verify that real humans (not bots) are participating in online discussions as AI makes fake accounts easier to create."
    },
    {
      "id": "Daniel P. Hogan",
      "title": "Daniel P. Hogan",
      "tags": [
        "person"
      ],
      "content": "#person"
    },
    {
      "id": "Recursive Public",
      "title": "Recursive Public",
      "tags": [
        "project"
      ],
      "content": "#project \n\nhttps://vtaiwan-openai-2023.vercel.app/Report_%20Recursive%20Public.pdf\n\nPilot democratic deliberation system designed to gather public input on AI governance decisions through cascading online discussions and AI-powered analysis. The project used platforms like [Polis](https://pol.is) and [Talk to the City](https://ai.objectives.institute/talk-to-the-city) to engage over 1,000 participants across different communities in identifying AI governance priorities, then employed LLMs to analyze consensus and disagreement patterns in the discussions.\n\nFunded by [OpenAI Democratic Inputs to AI](https://github.com/openai/democratic-inputs)."
    },
    {
      "id": "Rowan Wilkinson",
      "title": "Rowan Wilkinson",
      "tags": [
        "person"
      ],
      "content": "#person"
    },
    {
      "id": "Snow Globe",
      "title": "Snow Globe",
      "tags": [
        "project"
      ],
      "content": "#project \n\nhttps://github.com/IQTLabs/snowglobe\nhttps://arxiv.org/pdf/2404.11446\n\nSystem that uses LLMs to automate qualitative wargames - open-ended strategic simulations where participants respond with natural language rather than choosing from predefined moves. The system can simulate crisis scenarios like AI incidents or geopolitical conflicts, allowing multiple AI agents to play different roles while a control agent moderates and adjudicates outcomes."
    },
    {
      "id": "Owain Evans",
      "title": "Owain Evans",
      "tags": [
        "person"
      ],
      "content": "#person \n\nhttps://owainevans.github.io/"
    },
    {
      "id": "Forecasting",
      "title": "Forecasting",
      "tags": [
        "topic"
      ],
      "content": "#topic"
    },
    {
      "id": "Martin Weiss",
      "title": "Martin Weiss",
      "tags": [
        "person"
      ],
      "content": "#person"
    },
    {
      "id": "Li Erran Li",
      "title": "Li Erran Li",
      "tags": [
        "person"
      ],
      "content": "#person"
    },
    {
      "id": "Alice Marwick",
      "title": "Alice Marwick",
      "tags": [
        "person"
      ],
      "content": "#person \n\nhttps://tiara.org/"
    },
    {
      "id": "Oliver Klingefjord",
      "title": "Oliver Klingefjord",
      "tags": [
        "person"
      ],
      "content": "#person \n\nhttps://www.klingefjord.com/"
    },
    {
      "id": "Boardy",
      "title": "Boardy",
      "tags": [
        "project"
      ],
      "content": "#project \n\nhttps://www.boardy.ai/\n\nAI-powered networking assistant that conducts phone conversations with users to understand their professional goals, then autonomously facilitates introductions to relevant people in its network of founders, investors, and other professionals."
    },
    {
      "id": "Nicholas Rowland",
      "title": "Nicholas Rowland",
      "tags": [
        "person"
      ],
      "content": "#person"
    },
    {
      "id": "Scaffolding cooperation in human groups with deep reinforcement learning",
      "title": "Scaffolding cooperation in human groups with deep reinforcement learning",
      "tags": [
        "project"
      ],
      "content": "#project \n\nhttps://www.nature.com/articles/s41562-023-01686-7.pdf\n\nResearchers used deep reinforcement learning to train an AI \"social planner\" that makes recommendations about which connections to form or break in human social networks playing a cooperation game for real money. The AI learned a counterintuitive \"encouraging\" strategy: instead of isolating defectors from cooperators (the traditional approach), it placed defectors in small neighborhoods surrounded by cooperators, which led groups to achieve 77.7% cooperation rates compared to just 42.8% in static networks."
    },
    {
      "id": "Language Agents as Digital Representatives in Collective Decision-Making",
      "title": "Language Agents as Digital Representatives in Collective Decision-Making",
      "tags": [
        "project"
      ],
      "content": "#project \n\nhttps://arxiv.org/pdf/2502.09369\n\nResearchers fine-tuned LLMs to simulate how specific people would participate in consensus-finding discussions, particularly in the critique phase where participants evaluate draft consensus statements."
    },
    {
      "id": "Dirk U. Wulff",
      "title": "Dirk U. Wulff",
      "tags": [
        "person"
      ],
      "content": "#person"
    },
    {
      "id": "Luke Thorburn",
      "title": "Luke Thorburn",
      "tags": [
        "person"
      ],
      "content": "#person"
    },
    {
      "id": "Value Elicitation",
      "title": "Value Elicitation",
      "tags": [
        "topic"
      ],
      "content": "#topic"
    },
    {
      "id": "David Schwitzgebel",
      "title": "David Schwitzgebel",
      "tags": [
        "person"
      ],
      "content": "#person"
    },
    {
      "id": "Jay Van Bavel",
      "title": "Jay Van Bavel",
      "tags": [
        "person"
      ],
      "content": "#person \n"
    },
    {
      "id": "Foresight AI",
      "title": "Foresight AI",
      "tags": [
        "project"
      ],
      "content": "#project \n\nhttps://www.lightningrod.ai/foresight\n\nAI tool (with a free version!) which allows you to query the probability of some future event. The system will provide a probability, as well as a reasoning trace explaining how that probability was generated. The tool also gives some feedback about the quality of your question (e.g. clear resolution criteria), as well as offering alternate questions you might want to ask."
    },
    {
      "id": "Sander van der Linden",
      "title": "Sander van der Linden",
      "tags": [
        "person"
      ],
      "content": "#person \n"
    },
    {
      "id": "Double Crux Bot",
      "title": "Double Crux Bot",
      "tags": [
        "project"
      ],
      "content": "#project \n\nhttps://www.lesswrong.com/posts/ENgwdgdHZ2a6HTWDL/announcing-the-double-crux-bot\n\nGPT-powered chatbot for Slack and Discord that facilitates \"double crux\" conversations - a conflict resolution technique that helps two people identify the core disagreement underlying their dispute by finding shared cruxes (key beliefs that, if changed, would shift both parties' positions). The bot acts as a mediator to help people \"make their reasoning explicit and reflect on the crux of the issue\" and \"build better inferences about each other's motivations and frameworks so that they can come to a resolution.\""
    },
    {
      "id": "Emily Saltz",
      "title": "Emily Saltz",
      "tags": [
        "person"
      ],
      "content": "#person \n\nhttps://www.emilysaltz.space/"
    },
    {
      "id": "Julien Cornebise",
      "title": "Julien Cornebise",
      "tags": [
        "person"
      ],
      "content": "#person"
    },
    {
      "id": "Gili Rusak",
      "title": "Gili Rusak",
      "tags": [
        "person"
      ],
      "content": "#person"
    },
    {
      "id": "Daniel Jarrett",
      "title": "Daniel Jarrett",
      "tags": [
        "person"
      ],
      "content": "#person"
    },
    {
      "id": "Danny Fanklin",
      "title": "Danny Fanklin",
      "tags": [
        "person"
      ],
      "content": "#person"
    },
    {
      "id": "Language Models as Critical Thinking Tools - A Case Study of Philosophers",
      "title": "Language Models as Critical Thinking Tools - A Case Study of Philosophers",
      "tags": [
        "project"
      ],
      "content": "#project \n\nhttps://arxiv.org/pdf/2404.04516\n \nStudy of 21 professional philosophers examining how language models could serve as \"critical thinking tools\" rather than just productivity accelerators. The researchers found that current LMs fail as critical thinking partners because they lack \"selfhood\" (consistent perspectives, memory, beliefs) and \"initiative\" (curiosity, proactivity, willingness to challenge users), leading philosophers to describe them as \"boring,\" \"bland,\" and \"cowardly.\" The authors propose three new LM roles to better support deep reasoning:\n1. the Interlocutor (high selfhood/initiative, challenges and disagrees), \n2. the Monitor (low selfhood/high initiative, provides diverse perspectives), and \n3. the Respondent (high selfhood/low initiative, reacts from specific viewpoints)"
    },
    {
      "id": "Talk to the City",
      "title": "Talk to the City",
      "tags": [
        "project"
      ],
      "content": "#project \n\nhttps://ai.objectives.institute/talk-to-the-city\n\nOpen-source AI system that analyzes large-scale public input from surveys, interviews, and meetings using LLMs to identify themes and cluster opinions while maintaining the specificity of individual responses. The system processes both structured and unstructured data to generate interactive reports that allow decision-makers to explore opinion distributions at multiple scales, from broad thematic patterns down to individual participant perspectives"
    },
    {
      "id": "Arvind Narayanan",
      "title": "Arvind Narayanan",
      "tags": [
        "person"
      ],
      "content": "#person"
    },
    {
      "id": "My Current Claims and Cruxes on LLM Forecasting & Epistemics",
      "title": "My Current Claims and Cruxes on LLM Forecasting & Epistemics",
      "tags": [
        "project"
      ],
      "content": "#project \n\nhttps://forum.effectivealtruism.org/posts/EykCuXDCFAT5oGyux/my-current-claims-and-cruxes-on-llm-forecasting-and\n\nPost outlines a vision for LLM-based Epistemic Processes (LEPs) that could automate virtually all aspects of knowledge work, from gathering and synthesizing information to generating forecasts and presenting results to users. The author breaks down LEPs into 13 distinct components including data collection, world modeling, human elicitation, numeric modeling, decomposition/amplification techniques, and various forms of automated decision support. A central argument is that these systems will require substantial \"scaffolding\" (supporting software infrastructure) and will likely be dominated by centralized platforms rather than distributed individual forecasters, potentially replacing human participation in prediction markets like Manifold and Metaculus. The post also addresses potential risks including AI acceleration, misuse by malicious actors, and the challenge that these powerful epistemic tools might increase world complexity faster than they improve our ability to navigate it."
    },
    {
      "id": "Ezequiel Lopez-Lopez",
      "title": "Ezequiel Lopez-Lopez",
      "tags": [
        "person"
      ],
      "content": "#person"
    },
    {
      "id": "Niki Dupuis",
      "title": "Niki Dupuis",
      "tags": [
        "person"
      ],
      "content": "#person"
    },
    {
      "id": "Michael W. Morris",
      "title": "Michael W. Morris",
      "tags": [
        "person"
      ],
      "content": "#person"
    },
    {
      "id": "AI Enhanced Reasoning - Augmenting Human Critical Thinking With AI Systems",
      "title": "AI Enhanced Reasoning - Augmenting Human Critical Thinking With AI Systems",
      "tags": [
        "project"
      ],
      "content": "#project \n\nhttps://www.proquest.com/openview/3393dad9ea55a5d47f1a0b19e607f964/1.pdf\n\nMasters thesis, uses AI to help humans reason better through 3 experiments:\n- AI logic-checkers that spot argument flaws in real-time\n- Studies on how deceptive AI explanations mess with human thinking\n- AI that improves reasoning by asking smart questions"
    },
    {
      "id": "Manifold Markets",
      "title": "Manifold Markets",
      "tags": [
        "organization"
      ],
      "content": "#organization \n\nhttps://manifold.markets/home"
    },
    {
      "id": "Sentinel",
      "title": "Sentinel",
      "tags": [
        "organization"
      ],
      "content": "#organization \n\nhttps://sentinel-team.org"
    },
    {
      "id": "Sarah Bluhm",
      "title": "Sarah Bluhm",
      "tags": [
        "person"
      ],
      "content": "#person"
    },
    {
      "id": "Collective Intelligence Project",
      "title": "Collective Intelligence Project",
      "tags": [
        "organization"
      ],
      "content": "#organization \n\nhttps://www.cip.org/"
    },
    {
      "id": "Online Epistemics",
      "title": "Online Epistemics",
      "tags": [
        "topic"
      ],
      "content": "#topic"
    },
    {
      "id": "Modelling Political Coalition Negotiations Using LLM-based Agents",
      "title": "Modelling Political Coalition Negotiations Using LLM-based Agents",
      "tags": [
        "project"
      ],
      "content": "#project \n\nhttps://arxiv.org/pdf/2402.11712\n\nThis research paper introduces a system for modeling political coalition negotiations using LLM-based agents. The authors create AI agents that represent different political parties and simulate the complex negotiation process to predict which policy statements from party manifestos will be included in final coalition agreements."
    },
    {
      "id": "Darren Fancher",
      "title": "Darren Fancher",
      "tags": [
        "person"
      ],
      "content": "#person"
    },
    {
      "id": "Matchmaking",
      "title": "Matchmaking",
      "tags": [
        "topic"
      ],
      "content": "#topic"
    },
    {
      "id": "Jason W. Burton",
      "title": "Jason W. Burton",
      "tags": [
        "person"
      ],
      "content": "#person"
    },
    {
      "id": "Approaching Human-Level Forecasting with Language Models",
      "title": "Approaching Human-Level Forecasting with Language Models",
      "tags": [
        "project"
      ],
      "content": "#project \n\nhttps://arxiv.org/pdf/2402.18563\n\nLLM system forecasting on competitive forecasting platforms by combining news retrieval, structured reasoning, and fine-tuning techniques to predict binary outcomes. The system automates the traditional forecasting process through three key components: \n1. retrieving relevant information from news sources,\n2. generating reasoned predictions, and \n3. aggregating multiple forecasts into a final prediction."
    },
    {
      "id": "Alice Siu",
      "title": "Alice Siu",
      "tags": [
        "person"
      ],
      "content": "#person \n\nhttps://www.alicesiu.com/"
    },
    {
      "id": "Kris Skotheim",
      "title": "Kris Skotheim",
      "tags": [
        "person"
      ],
      "content": "#person"
    },
    {
      "id": "Ziv Epstein",
      "title": "Ziv Epstein",
      "tags": [
        "person"
      ],
      "content": "#person"
    },
    {
      "id": "Felix Sieker",
      "title": "Felix Sieker",
      "tags": [
        "person"
      ],
      "content": "#person"
    },
    {
      "id": "Audrey Tang",
      "title": "Audrey Tang",
      "tags": [
        "person"
      ],
      "content": "#person \n\nhttps://github.com/audreyt"
    },
    {
      "id": "Bridging Bot",
      "title": "Bridging Bot",
      "tags": [
        "project"
      ],
      "content": "#project \n\nhttps://www.youtube.com/watch?v=QgfXdJ-7pF4\n\nLLM-based bot that de-escalates arguments on Reddit. Funded by Google Jigsaw."
    },
    {
      "id": "Philosophy & Morality",
      "title": "Philosophy & Morality",
      "tags": [
        "topic"
      ],
      "content": "#topic"
    },
    {
      "id": "Wasim Almasri",
      "title": "Wasim Almasri",
      "tags": [
        "person"
      ],
      "content": "#person"
    },
    {
      "id": "Jia-Wei Cui",
      "title": "Jia-Wei Cui",
      "tags": [
        "person"
      ],
      "content": "#person"
    },
    {
      "id": "LLMs as Research Tools -  A Large Scale Survey of Researchers’ Usage and Perceptions",
      "title": "LLMs as Research Tools -  A Large Scale Survey of Researchers’ Usage and Perceptions",
      "tags": [
        "project"
      ],
      "content": "#project \n\nhttps://arxiv.org/pdf/2411.05025\n\nLarge-scale survey study examining how researchers currently use LLMs in their research workflows and their perceptions of benefits and risks. The study surveyed 816 verified research paper authors across multiple disciplines to understand usage patterns, demographic differences, and attitudes toward LLM tools in academic work."
    },
    {
      "id": "Mediation & Negotiation",
      "title": "Mediation & Negotiation",
      "tags": [
        "topic"
      ],
      "content": "#topic"
    },
    {
      "id": "Applied Rationality",
      "title": "Applied Rationality",
      "tags": [
        "topic"
      ],
      "content": "#topic"
    },
    {
      "id": "Andrea Tacchetti",
      "title": "Andrea Tacchetti",
      "tags": [
        "person"
      ],
      "content": "#person \n\nhttps://www.andreatacchetti.com/"
    },
    {
      "id": "Tristan Xiao",
      "title": "Tristan Xiao",
      "tags": [
        "person"
      ],
      "content": "#person"
    },
    {
      "id": "Pepijn Verburg",
      "title": "Pepijn Verburg",
      "tags": [
        "person"
      ],
      "content": "#person"
    },
    {
      "id": "Jacob Steinhardt",
      "title": "Jacob Steinhardt",
      "tags": [
        "person"
      ],
      "content": "#person \n\nhttps://jsteinhardt.stat.berkeley.edu/"
    },
    {
      "id": "Wenkang Ji",
      "title": "Wenkang Ji",
      "tags": [
        "person"
      ],
      "content": "#person"
    },
    {
      "id": "Susan Leavy",
      "title": "Susan Leavy",
      "tags": [
        "person"
      ],
      "content": "#person"
    },
    {
      "id": "Meeyoung Cha",
      "title": "Meeyoung Cha",
      "tags": [
        "person"
      ],
      "content": "#person \n"
    },
    {
      "id": "Kehang Zhu",
      "title": "Kehang Zhu",
      "tags": [
        "person"
      ],
      "content": "#person \n"
    },
    {
      "id": "Future Search",
      "title": "Future Search",
      "tags": [
        "organization"
      ],
      "content": "#organization \n\nhttps://futuresearch.ai/"
    },
    {
      "id": "Nick Bostrom",
      "title": "Nick Bostrom",
      "tags": [
        "person"
      ],
      "content": "#person \n\nhttps://nickbostrom.com/\n"
    },
    {
      "id": "Sven Seuken",
      "title": "Sven Seuken",
      "tags": [
        "person"
      ],
      "content": "#person \n"
    },
    {
      "id": "Richard Li",
      "title": "Richard Li",
      "tags": [
        "person"
      ],
      "content": "#person"
    },
    {
      "id": "Ben Wilson",
      "title": "Ben Wilson",
      "tags": [
        "person"
      ],
      "content": "#person"
    },
    {
      "id": "Lightning Rod Labs",
      "title": "Lightning Rod Labs",
      "tags": [
        "organization"
      ],
      "content": "#organization \n\nhttps://www.lightningrod.ai/"
    },
    {
      "id": "Human v Bots Forecasting Tournament 2024",
      "title": "Human v Bots Forecasting Tournament 2024",
      "tags": [
        "project"
      ],
      "content": "#project\n\nhttps://news.manifold.markets/p/human-v-bots-forecasting-tournament\n\nCompetition hosted by Manifold Markets, where human forecasters compete directly against AI systems to predict major world events throughout 2024, with winners determined by forecasting accuracy and profit."
    },
    {
      "id": "Democratic Policy Development using Collective Dialogues and AI",
      "title": "Democratic Policy Development using Collective Dialogues and AI",
      "tags": [
        "project"
      ],
      "content": "#project \n\nhttps://arxiv.org/pdf/2311.02242\n\nPaper presents a democratic process for developing AI policies through collective dialogues, using AI tools to facilitate large-scale deliberation and consensus-finding among diverse public participants. The system combines AI-augmented group discussions with bridging-based ranking algorithms to identify points of consensus, then uses GPT-4 to translate these into concrete policy guidelines that are refined through expert input and further public feedback.\n\nFunded by [OpenAI Democratic Inputs to AI](https://github.com/openai/democratic-inputs)."
    },
    {
      "id": "David Huang",
      "title": "David Huang",
      "tags": [
        "person"
      ],
      "content": "#person \n"
    },
    {
      "id": "Dan Hendrycks",
      "title": "Dan Hendrycks",
      "tags": [
        "person"
      ],
      "content": "#person \n\nhttps://danhendrycks.com/"
    },
    {
      "id": "Curtis Holdsworth",
      "title": "Curtis Holdsworth",
      "tags": [
        "person"
      ],
      "content": "#person"
    },
    {
      "id": "Siddarth Srinivasan",
      "title": "Siddarth Srinivasan",
      "tags": [
        "person"
      ],
      "content": "#person \n"
    },
    {
      "id": "Julian Michael",
      "title": "Julian Michael",
      "tags": [
        "person"
      ],
      "content": "#person \n\nhttps://julianmichael.org/"
    },
    {
      "id": "Değer Turan",
      "title": "Değer Turan",
      "tags": [
        "person"
      ],
      "content": "#person"
    },
    {
      "id": "Owen Cotton-Barratt",
      "title": "Owen Cotton-Barratt",
      "tags": [
        "person"
      ],
      "content": "#person \n\nhttps://strangecities.substack.com/"
    },
    {
      "id": "Jared Moore",
      "title": "Jared Moore",
      "tags": [
        "person"
      ],
      "content": "#person \n\nhttps://jaredmoore.org/"
    },
    {
      "id": "Ulrike Hahn",
      "title": "Ulrike Hahn",
      "tags": [
        "person"
      ],
      "content": "#person"
    },
    {
      "id": "Zhuoyan Li",
      "title": "Zhuoyan Li",
      "tags": [
        "person"
      ],
      "content": "#person"
    },
    {
      "id": "Aymen Kallala",
      "title": "Aymen Kallala",
      "tags": [
        "person"
      ],
      "content": "#person"
    },
    {
      "id": "AI-Augmented Predictions - LLM Assistants Improve Human Forecasting Accuracy",
      "title": "AI-Augmented Predictions - LLM Assistants Improve Human Forecasting Accuracy",
      "tags": [
        "project"
      ],
      "content": "#project \n\nhttps://arxiv.org/abs/2402.07862\n\nThis paper studies how LLMs can serve as assistants to improve human forecasting accuracy, testing two types of assistants - one designed to provide high-quality \"superforecasting\" advice and another designed to be overconfident and noisy. In a study with 991 participants answering forecasting questions, both LLM assistants significantly enhanced human prediction accuracy compared to controls, with the superforecasting assistant showing up to 41% improvement in some analyses."
    },
    {
      "id": "Jeff Fossett",
      "title": "Jeff Fossett",
      "tags": [
        "person"
      ],
      "content": "#person"
    },
    {
      "id": "Cameron Jones",
      "title": "Cameron Jones",
      "tags": [
        "person"
      ],
      "content": "#person"
    },
    {
      "id": "AI Debate Maps",
      "title": "AI Debate Maps",
      "tags": [
        "project"
      ],
      "content": "#project \n\nhttps://www.societylibrary.org/topics-blog/ai-alignment-superintelligence-ethics\n\nDebate maps specifically about Artificial Intelligence."
    },
    {
      "id": "Sara Fish",
      "title": "Sara Fish",
      "tags": [
        "person"
      ],
      "content": "#person"
    },
    {
      "id": "Human vs. Machine -  Behavioral Differences between Expert Humans and Language Models in Wargame Simulations",
      "title": "Human vs. Machine -  Behavioral Differences between Expert Humans and Language Models in Wargame Simulations",
      "tags": [
        "project"
      ],
      "content": "#project \n\nhttps://arxiv.org/pdf/2403.03407\n\nPaper examines how LLMs behave compared to human experts when simulating military crisis decision-making through wargames. The study involved 214 national security experts and compared their responses to AI-simulated teams in a fictional U.S.-China crisis scenario over the Taiwan Strait. The research found that while LLMs showed significant overlap with human decision-making patterns, they demonstrated concerning tendencies toward more aggressive actions and were significantly affected by changes in scenario parameters."
    },
    {
      "id": "Hadjar Homaei",
      "title": "Hadjar Homaei",
      "tags": [
        "person"
      ],
      "content": "#person"
    },
    {
      "id": "Information Market Mechanisms",
      "title": "Information Market Mechanisms",
      "tags": [
        "topic"
      ],
      "content": "#topic"
    },
    {
      "id": "Philipp Schoenegger",
      "title": "Philipp Schoenegger",
      "tags": [
        "person"
      ],
      "content": "#person \n\nhttps://philipp-schoenegger.weebly.com/"
    },
    {
      "id": "William Saunders",
      "title": "William Saunders",
      "tags": [
        "person"
      ],
      "content": "#person"
    },
    {
      "id": "Julian Berger",
      "title": "Julian Berger",
      "tags": [
        "person"
      ],
      "content": "#person"
    },
    {
      "id": "Joe Kwon",
      "title": "Joe Kwon",
      "tags": [
        "person"
      ],
      "content": "#person"
    },
    {
      "id": "Jingyan Zhou",
      "title": "Jingyan Zhou",
      "tags": [
        "person"
      ],
      "content": "#person \n\nhttps://para-zhou.github.io/"
    },
    {
      "id": "Pietro Nickl",
      "title": "Pietro Nickl",
      "tags": [
        "person"
      ],
      "content": "#person"
    },
    {
      "id": "Linting Adjacent",
      "title": "Linting Adjacent",
      "tags": [
        "topic"
      ],
      "content": "#topic"
    },
    {
      "id": "Metaculus",
      "title": "Metaculus",
      "tags": [
        "organization"
      ],
      "content": "#organization\n\nhttps://www.metaculus.com/"
    },
    {
      "id": "Nuño Sempere",
      "title": "Nuño Sempere",
      "tags": [
        "person"
      ],
      "content": "#person \n\nhttps://nunosempere.com/"
    },
    {
      "id": "Sparse Autoencoders for Hypothesis Generation",
      "title": "Sparse Autoencoders for Hypothesis Generation",
      "tags": [
        "project"
      ],
      "content": "#project \n\nhttps://arxiv.org/pdf/2502.04382\n\nA method that uses sparse autoencoders to automatically generate human-interpretable hypotheses about what features in text data predict certain outcomes (like which headlines get more clicks or what language patterns distinguish Republican vs Democratic speeches). The system works by training neural networks to find interpretable patterns in text, selecting the most predictive patterns, and then using language models to translate those patterns into clear natural language explanations."
    },
    {
      "id": "Bruno Marnette",
      "title": "Bruno Marnette",
      "tags": [
        "person"
      ],
      "content": "#person \n\nhttps://brunomarnette.substack.com/"
    },
    {
      "id": "Ben Rachbach",
      "title": "Ben Rachbach",
      "tags": [
        "person"
      ],
      "content": "#person"
    },
    {
      "id": "Chun-Wei Chiang",
      "title": "Chun-Wei Chiang",
      "tags": [
        "person"
      ],
      "content": "#person"
    },
    {
      "id": "Huaben Chen",
      "title": "Huaben Chen",
      "tags": [
        "person"
      ],
      "content": "#person"
    },
    {
      "id": "David Parkes",
      "title": "David Parkes",
      "tags": [
        "person"
      ],
      "content": "#person"
    },
    {
      "id": "Ian Beacock",
      "title": "Ian Beacock",
      "tags": [
        "person"
      ],
      "content": "#person"
    },
    {
      "id": "Stefan M. Herzog",
      "title": "Stefan M. Herzog",
      "tags": [
        "person"
      ],
      "content": "#person"
    },
    {
      "id": "Max Lamparth",
      "title": "Max Lamparth",
      "tags": [
        "person"
      ],
      "content": "#person \n\nhttps://www.maxlamparth.com/"
    },
    {
      "id": "Dalit Shalom",
      "title": "Dalit Shalom",
      "tags": [
        "person"
      ],
      "content": "#person"
    },
    {
      "id": "AI Safety Feed",
      "title": "AI Safety Feed",
      "tags": [
        "project"
      ],
      "content": "#project \n\nhttps://aisafetyfeed.com\n\nCurated content aggregation platform that gathers posts and research from key AI safety sources across the internet. It uses LLMs to automatically summarize, tag, and rate content for novelty and importance, helping users quickly identify what's most relevant to them in the AI safety field."
    },
    {
      "id": "The Computational Democracy Project",
      "title": "The Computational Democracy Project",
      "tags": [
        "organization"
      ],
      "content": "#organization \n\nhttps://compdemocracy.org/"
    },
    {
      "id": "Matthew Botvinick",
      "title": "Matthew Botvinick",
      "tags": [
        "person"
      ],
      "content": "#person"
    },
    {
      "id": "Strategic Epistemic Defense",
      "title": "Strategic Epistemic Defense",
      "tags": [
        "topic"
      ],
      "content": "#topic"
    },
    {
      "id": "Emma Pierson",
      "title": "Emma Pierson",
      "tags": [
        "person"
      ],
      "content": "#person"
    },
    {
      "id": "Chris Pal",
      "title": "Chris Pal",
      "tags": [
        "person"
      ],
      "content": "#person"
    },
    {
      "id": "Interface Design",
      "title": "Interface Design",
      "tags": [
        "topic"
      ],
      "content": "#topic"
    },
    {
      "id": "Factored Cognition Primer",
      "title": "Factored Cognition Primer",
      "tags": [
        "project"
      ],
      "content": "#project \n\nhttps://primer.ought.org/\n\nTutorial that teaches how to write \"compositional language model programs\" using factored cognition - a technique that breaks down complex reasoning tasks into smaller, manageable subtasks that can be solved recursively through methods like question-answering and debate. This approach aims to make AI reasoning more transparent and supervisable by decomposing sophisticated thinking into many small, independent tasks that humans can verify."
    },
    {
      "id": "Oriana Skylar Mastro",
      "title": "Oriana Skylar Mastro",
      "tags": [
        "person"
      ],
      "content": "#person"
    },
    {
      "id": "Generative Social Choice",
      "title": "Generative Social Choice",
      "tags": [
        "project"
      ],
      "content": "#project \n\nhttps://arxiv.org/pdf/2309.01291\n\nPaper introduces a framework that combines LLMs with social choice theory to enable democratic decision-making on open-ended questions where traditional voting on predetermined alternatives isn't sufficient. The system can generate new consensus statements from diverse participant opinions and predict how well individuals would agree with any statement.\n\nFunded by [OpenAI Democratic Inputs to AI](https://github.com/openai/democratic-inputs)."
    },
    {
      "id": "Amplifying transformative potential while designing augmented deliberative systems",
      "title": "Amplifying transformative potential while designing augmented deliberative systems",
      "tags": [
        "project"
      ],
      "content": "#project \n\nhttps://ai.objectives.institute/blog/amplifying-transformative-potential-while-designing-augmented-deliberative-systems\n\nBlog post presenting the \"Goldilocks Framework for Augmented Group Intelligence,\" which proposes principles for designing AI systems that enhance human deliberation and decision-making while preserving authentic human agency. The framework argues that effective AI-augmented deliberation requires balancing two key elements: \n1. participants' ability to understand and steer AI outputs (human agency) and \n2. their commitment to deliberative outcomes \n\nHigher-commitment decisions requiring correspondingly higher human agency."
    },
    {
      "id": "Quantified Uncertainty Research Institute",
      "title": "Quantified Uncertainty Research Institute",
      "tags": [
        "organization"
      ],
      "content": "#organization \n\nhttps://quantifieduncertainty.org/"
    },
    {
      "id": "Creating a large language model of a philosopher",
      "title": "Creating a large language model of a philosopher",
      "tags": [
        "project"
      ],
      "content": "#project \n\nhttps://arxiv.org/abs/2302.01339\n\nExperiment where they fine-tuned GPT-3 on the works of Daniel Dennett to see if an LLM can produce expert-quality philosophical texts. They tested the \"digi-Dan\" model by asking both the real Dennett and the AI model ten philosophical questions, then had 425 participants try to distinguish between Dennett's actual answers and the machine-generated responses. The study found that experts often chose the AI's answers over Dennett's actual responses, and on two questions, the AI outputs were selected by more experts than Dennett's own answers."
    },
    {
      "id": "What's Important in \"AI for Epistemics\"?",
      "title": "What's Important in \"AI for Epistemics\"?",
      "tags": [
        "project"
      ],
      "content": "#project \n\nhttps://www.forethought.org/research/whats-important-in-ai-for-epistemics\n\nAnalysis of priorities and strategies for developing \"AI for epistemics\": essentially AI tools that help humans and organizations make better, more informed decisions by improving how we discover, evaluate, and act on information. The paper argues that as AI becomes more powerful, it will dramatically reshape how society generates knowledge and makes decisions, creating both opportunities to enhance human reasoning and risks of manipulation or poor coordination."
    },
    {
      "id": "Making Artificial Intelligence Work for Investigative Journalism",
      "title": "Making Artificial Intelligence Work for Investigative Journalism",
      "tags": [
        "project"
      ],
      "content": "#project \n\nhttps://www.tandfonline.com/doi/full/10.1080/21670811.2019.1630289\n\nPaper examines how artificial intelligence can be applied to investigative journalism, focusing on the practical challenges and realistic opportunities for AI tools in newsrooms. The research reveals that while AI has enormous theoretical potential for helping journalists analyze large datasets and uncover hidden patterns of public interest, current applications remain limited to relatively simple tasks like document classification and data cleaning."
    },
    {
      "id": "Ezra Karger",
      "title": "Ezra Karger",
      "tags": [
        "person"
      ],
      "content": "#person \n\nhttps://ezrakarger.com/"
    },
    {
      "id": "LLMediator",
      "title": "LLMediator",
      "tags": [
        "project"
      ],
      "content": "#project \n\nhttps://arxiv.org/pdf/2307.16732\n\nAn experimental platform that uses GPT-4 to enhance online dispute resolution by helping parties communicate more effectively and reach amicable settlements. The system offers three key features: \n1. reformulating inflammatory messages to be less confrontational while preserving their core meaning, \n2. generating draft intervention messages for human mediators to guide discussions, and \n3. allowing AI to autonomously mediate certain low-stakes disputes."
    },
    {
      "id": "Ai-Heng Lee",
      "title": "Ai-Heng Lee",
      "tags": [
        "person"
      ],
      "content": "#person"
    },
    {
      "id": "Mantas Mazeika",
      "title": "Mantas Mazeika",
      "tags": [
        "person"
      ],
      "content": "#person"
    },
    {
      "id": "Amit Goldenberg",
      "title": "Amit Goldenberg",
      "tags": [
        "person"
      ],
      "content": "#person \n"
    },
    {
      "id": "Adam Bales",
      "title": "Adam Bales",
      "tags": [
        "person"
      ],
      "content": "#person"
    },
    {
      "id": "Daniel Kokotajlo",
      "title": "Daniel Kokotajlo",
      "tags": [
        "person"
      ],
      "content": "#person"
    },
    {
      "id": "Nathanael Fast",
      "title": "Nathanael Fast",
      "tags": [
        "person"
      ],
      "content": "#person"
    },
    {
      "id": "Tell Me Why - Incentivizing Explanations",
      "title": "Tell Me Why - Incentivizing Explanations",
      "tags": [
        "project"
      ],
      "content": "#project \n\nhttps://arxiv.org/abs/2502.13410\n\nPaper presents a \"deliberation mechanism\" that uses economic incentives to get experts to explain their reasoning, not just state their conclusions. The mechanism works through a three-party structure: experts submit both beliefs and explanations to a supervisor, who then reports aggregated beliefs to a principal, with all parties scored using proper scoring rules based on accuracy. The key insight is that a supervisor can credibly commit to ignoring any expert reports that lack explanations, which forces experts to provide rationales despite the extra effort required, because without explanations, experts receive zero reward."
    },
    {
      "id": "Amelia Wattenberger",
      "title": "Amelia Wattenberger",
      "tags": [
        "person"
      ],
      "content": "#person \n\nhttps://wattenberger.com\nhttps://github.com/Wattenberger"
    },
    {
      "id": "Metaforecast",
      "title": "Metaforecast",
      "tags": [
        "project"
      ],
      "content": "#project \n\nhttps://metaforecast.org\n\nSearch engine and aggregator for predictions and forecasts from various prediction markets and forecasting platforms, allowing users to search for probability estimates on topics like geopolitics, technology, and other events. It provides quality ratings for forecasts and tools to help users navigate and understand prediction data across multiple platforms.\n\n(not currently maintained)"
    },
    {
      "id": "Santeri Koivula",
      "title": "Santeri Koivula",
      "tags": [
        "person"
      ],
      "content": "#person"
    },
    {
      "id": "What are human values,and how do we align AI to them?",
      "title": "What are human values,and how do we align AI to them?",
      "tags": [
        "project"
      ],
      "content": "#project \n\nhttps://arxiv.org/pdf/2404.10636\n\nPaper introduces \"Moral Graph Elicitation\" (MGE), a method for systematically collecting and organizing human values to better align AI systems with what people actually care about. The researchers developed a process that uses AI-powered interviews to elicit specific \"values cards\" from people (detailed descriptions of what they pay attention to when making meaningful choices) then connects these values in a graph structure based on which values participants consider \"wiser\" than others."
    },
    {
      "id": "Andre Ye",
      "title": "Andre Ye",
      "tags": [
        "person"
      ],
      "content": "#person \n\nhttps://andre-ye.org/"
    },
    {
      "id": "Inyoung Cheong",
      "title": "Inyoung Cheong",
      "tags": [
        "person"
      ],
      "content": "#person"
    },
    {
      "id": "Jon Kleinberg",
      "title": "Jon Kleinberg",
      "tags": [
        "person"
      ],
      "content": "#person"
    },
    {
      "id": "Danny Halawi",
      "title": "Danny Halawi",
      "tags": [
        "person"
      ],
      "content": "#person"
    },
    {
      "id": "Escalation Risks from Language Models in Military and Diplomatic Decision-Making",
      "title": "Escalation Risks from Language Models in Military and Diplomatic Decision-Making",
      "tags": [
        "project"
      ],
      "content": "#project \nhttps://dl.acm.org/doi/pdf/10.1145/3630106.3658942\n\nPaper examines what happens when LLMs are placed in charge of simulated nations in wargaming scenarios, finding that these AI systems frequently escalate conflicts and, in some cases, even deploy nuclear weapons. The researchers tested five different LLMs controlling autonomous nation agents and discovered that all models showed escalatory patterns, with some developing arms-race dynamics and making decisions based on concerning justifications like deterrence and first-strike tactics."
    },
    {
      "id": "Filippo Menczer",
      "title": "Filippo Menczer",
      "tags": [
        "person"
      ],
      "content": "#person \n"
    },
    {
      "id": "Esin Durmus",
      "title": "Esin Durmus",
      "tags": [
        "person"
      ],
      "content": "#person \n\nhttps://esdurmus.github.io/"
    },
    {
      "id": "Robert Gambee",
      "title": "Robert Gambee",
      "tags": [
        "person"
      ],
      "content": "#person"
    },
    {
      "id": "Valdemar Danry",
      "title": "Valdemar Danry",
      "tags": [
        "person"
      ],
      "content": "#person\n\nhttps://valdemardanry.com/"
    },
    {
      "id": "Contra papers claiming superhuman AI forecasting",
      "title": "Contra papers claiming superhuman AI forecasting",
      "tags": [
        "project"
      ],
      "content": "#project \n\nhttps://www.lesswrong.com/posts/uGkRcHqatmPkvpGLq/contra-papers-claiming-superhuman-ai-forecasting\n\nCritique of recent academic papers that claim AI systems have achieved \"superhuman\" forecasting performance. The authors from FutureSearch systematically debunk these claims by identifying major methodological flaws, including inadequate information retrieval capabilities, data contamination, unfair timing advantages, and misleading statistical interpretations that make AI performance appear better than it actually is."
    },
    {
      "id": "Srijoni Majumdar",
      "title": "Srijoni Majumdar",
      "tags": [
        "person"
      ],
      "content": "#person"
    },
    {
      "id": "Forecasting Future World Events with Neural Networks",
      "title": "Forecasting Future World Events with Neural Networks",
      "tags": [
        "project"
      ],
      "content": "#project \n\nhttps://arxiv.org/pdf/2206.15474\n\nIntroduces \"Autocast,\" a dataset and benchmark for evaluating neural networks' ability to forecast future world events using thousands of real forecasting questions from public tournaments. The dataset tests language models on diverse topics like politics, economics, and science by simulating historical forecasting conditions—providing only past news articles to prevent information leakage from the future.\n\n(research from 2022)"
    },
    {
      "id": "ForecastBench - A Dynamic Benchmark of AI Forecasting Capabilities",
      "title": "ForecastBench - A Dynamic Benchmark of AI Forecasting Capabilities",
      "tags": [
        "project"
      ],
      "content": "#project \n\nhttps://arxiv.org/pdf/2409.19839\nhttps://www.forecastbench.org/\n\nBenchmark system that continuously evaluates AI forecasting capabilities using 1,000 automatically generated questions about future events that update in real-time. The system avoids data contamination by using only questions about genuinely unresolved future events, and it compares LLM performance against human forecasters including \"superforecasters\" who have proven track records. Research found that expert human forecasters significantly outperform the best-performing LLMs (like Claude 3.5 Sonnet)."
    },
    {
      "id": "Sofi Vanhanen",
      "title": "Sofi Vanhanen",
      "tags": [
        "person"
      ],
      "content": "#person \n\nhttps://sofiavanhanen.fi/"
    },
    {
      "id": "AI Forecasting Benchmark",
      "title": "AI Forecasting Benchmark",
      "tags": [
        "project"
      ],
      "content": "#project \n\nhttps://www.metaculus.com/aib/\n\nMetaculus AIB (AI Benchmarking) is a series that benchmarks \"the state of the art in AI forecasting against the best humans on real-world questions.\""
    },
    {
      "id": "Mosaic Labs",
      "title": "Mosaic Labs",
      "tags": [
        "organization"
      ],
      "content": "#organization \n\nhttps://mosaic-labs.org/"
    },
    {
      "id": "Rose Novick",
      "title": "Rose Novick",
      "tags": [
        "person"
      ],
      "content": "#person"
    },
    {
      "id": "AI doing philosophy = AI generating hands?",
      "title": "AI doing philosophy = AI generating hands?",
      "tags": [
        "project"
      ],
      "content": "#project \n\nhttps://www.lesswrong.com/posts/G4ARgcnFogpqorQgb/ai-doing-philosophy-ai-generating-hands-1\n\nWei Dai argues that just as current AI systems generate beautiful images but distorted hands, future AI might excel at science and technology while being dangerously incompetent at philosophical reasoning, creating a critical capability gap that requires urgent attention."
    },
    {
      "id": "Rich Rippin",
      "title": "Rich Rippin",
      "tags": [
        "person"
      ],
      "content": "#person"
    },
    {
      "id": "Why Chatbots Are Not the Future",
      "title": "Why Chatbots Are Not the Future",
      "tags": [
        "project"
      ],
      "content": "#project \n\nhttps://wattenberger.com/thoughts/boo-chatbots\n\nArticle that argues chatbots are poor interfaces for AI systems because they lack clear affordances, force users to learn complex prompting, and isolate responses without allowing iterative refinement. The author advocates for AI tools with better user interfaces that provide contextual controls, visual feedback, and support human agency rather than replacing human decision-making."
    },
    {
      "id": "Andrea Brennen",
      "title": "Andrea Brennen",
      "tags": [
        "person"
      ],
      "content": "#person"
    },
    {
      "id": "Habermas Machine",
      "title": "Habermas Machine",
      "tags": [
        "project"
      ],
      "content": "#project\n\nhttps://www.science.org/doi/10.1126/science.adq2852\n\nAI system that uses two fine-tuned language models to mediate group discussions and help people with differing opinions find common ground on contentious issues. The system takes individual written opinions from participants, generates candidate \"group statements\" that synthesize everyone's perspectives, and iteratively refines these statements based on participant feedback until the group converges on a shared position. Research showed that AI-mediated discussions were more effective than unmediated opinion exposure at causing people to change their minds and reach consensus, with the AI slightly outperforming untrained human mediators."
    },
    {
      "id": "Daniel Schroeder",
      "title": "Daniel Schroeder",
      "tags": [
        "person"
      ],
      "content": "#person \n"
    },
    {
      "id": "Yanchen Jiang",
      "title": "Yanchen Jiang",
      "tags": [
        "person"
      ],
      "content": "#person \n"
    },
    {
      "id": "Gabriel Mukobi",
      "title": "Gabriel Mukobi",
      "tags": [
        "person"
      ],
      "content": "#person"
    },
    {
      "id": "Justin Reppert",
      "title": "Justin Reppert",
      "tags": [
        "person"
      ],
      "content": "#person \n\nhttps://www.justinreppert.com/"
    },
    {
      "id": "Jack Wildman",
      "title": "Jack Wildman",
      "tags": [
        "person"
      ],
      "content": "#person"
    },
    {
      "id": "Yiling Chen",
      "title": "Yiling Chen",
      "tags": [
        "person"
      ],
      "content": "#person \n"
    },
    {
      "id": "Levin Brinkmann",
      "title": "Levin Brinkmann",
      "tags": [
        "person"
      ],
      "content": "#person"
    },
    {
      "id": "Itai Shapira",
      "title": "Itai Shapira",
      "tags": [
        "person"
      ],
      "content": "#person"
    },
    {
      "id": "Voting",
      "title": "Voting",
      "tags": [
        "topic"
      ],
      "content": "#topic"
    },
    {
      "id": "Bernhard Schölkopf",
      "title": "Bernhard Schölkopf",
      "tags": [
        "person"
      ],
      "content": "#person"
    },
    {
      "id": "Bram Delisse",
      "title": "Bram Delisse",
      "tags": [
        "person"
      ],
      "content": "#person"
    },
    {
      "id": "Helen Meng",
      "title": "Helen Meng",
      "tags": [
        "person"
      ],
      "content": "#person"
    },
    {
      "id": "Aldo de Moor",
      "title": "Aldo de Moor",
      "tags": [
        "person"
      ],
      "content": "#person"
    },
    {
      "id": "Debate Maps",
      "title": "Debate Maps",
      "tags": [
        "project"
      ],
      "content": "#project \n\nhttps://www.societylibrary.org/debate-mapping-program\n\nAutomated pipeline for scraping online content, extracting arguments/claims, and organizing them into visual \"maps\" which structure the debate into a tree of statements and their logical relationships."
    },
    {
      "id": "Outcome-based Reinforcement Learning to Predict the Future",
      "title": "Outcome-based Reinforcement Learning to Predict the Future",
      "tags": [
        "project"
      ],
      "content": "#project \n\nhttps://www.lightningrod.ai/outcome-based-reinforcement-learning-to-predict-the-future\n\nPaper presents a method for training AI models to make better probabilistic forecasts about future events using reinforcement learning with outcome-based rewards. The researchers adapted existing reinforcement learning algorithms (like GRPO and ReMax) to work with the messy, delayed feedback that comes from real-world prediction tasks, using a dataset of over 100,000 forecasting questions from sources like Polymarket. Their approach produces a 14B parameter model that matches the accuracy of frontier models like OpenAI's o1 while achieving better calibration, ultimately translating this into higher hypothetical trading profits when betting against prediction markets."
    },
    {
      "id": "Molly Hickman",
      "title": "Molly Hickman",
      "tags": [
        "person"
      ],
      "content": "#person \n\nhttps://hickman-santini.github.io/"
    },
    {
      "id": "Kevin McKee",
      "title": "Kevin McKee",
      "tags": [
        "person"
      ],
      "content": "#person"
    },
    {
      "id": "Forethought",
      "title": "Forethought",
      "tags": [
        "organization"
      ],
      "content": "#organization \n\nhttps://www.forethought.org/"
    },
    {
      "id": "AI Future Project",
      "title": "AI Future Project",
      "tags": [
        "organization"
      ],
      "content": "#organization \n\nhttps://ai-futures.org/"
    },
    {
      "id": "Kevin Systrom",
      "title": "Kevin Systrom",
      "tags": [
        "person"
      ],
      "content": "#person"
    },
    {
      "id": "Bot Mediation",
      "title": "Bot Mediation",
      "tags": [
        "project"
      ],
      "content": "#project \n\nhttps://www.americanbar.org/groups/law_practice/resources/law-technology-today/2025/ai-powered-mediation-for-efficient-legal-dispute-resolution/\n\nAI system designed to mediate disputes between parties without human mediators. The platform claims to reduce mediation timeframes from months to days (without having to pay for professional mediation). The company was selected as a [finalist](https://www.techshow.com/2025/02/voting-is-closed-results-are-in-here-are-the-15-legal-tech-startups-selected-for-the-2025-startup-alley-at-aba-techshow/) in the American Bar Association's [TECHSHOW](https://www.techshow.com/) 2025 Startup Alley competition."
    },
    {
      "id": "Linters for Thought",
      "title": "Linters for Thought",
      "tags": [
        "project"
      ],
      "content": "#project \n\nhttps://textpress.md/jlevy/d/lft.html\n\nEssay about using AI and software engineering principles to augment human intelligence and improve collective problem-solving. The essay proposes developing \"linters\" for human reasoning - automated tools that catch errors in logic, fact-checking, and consistency in writing and decision-making, similar to how code linters catch programming errors."
    },
    {
      "id": "Lizka Vaintrob",
      "title": "Lizka Vaintrob",
      "tags": [
        "person"
      ],
      "content": "#person"
    },
    {
      "id": "Fine-tuning language models to find agreement among humans with diverse preferences",
      "title": "Fine-tuning language models to find agreement among humans with diverse preferences",
      "tags": [
        "project"
      ],
      "content": "#project \n\nhttps://proceedings.neurips.cc/paper_files/paper/2022/file/f978c8f3b5f399cae464e85f72e28503-Paper-Conference.pdf\n\nPaper presents a system that fine-tunes an LLM to generate consensus statements that maximize expected approval among groups of people with diverse political opinions. The researchers developed a method where participants provide written opinions on political questions, and an AI system generates candidate consensus statements that aim to find common ground among the group members.\n\n(research from 2022)"
    },
    {
      "id": "Sean Trott",
      "title": "Sean Trott",
      "tags": [
        "person"
      ],
      "content": "#person"
    },
    {
      "id": "Nathan Young",
      "title": "Nathan Young",
      "tags": [
        "person"
      ],
      "content": "#person \n\nhttps://nathanpmyoung.substack.com/"
    },
    {
      "id": "Nicholas Christakis",
      "title": "Nicholas Christakis",
      "tags": [
        "person"
      ],
      "content": "#person \n"
    },
    {
      "id": "Shiyu Zhao",
      "title": "Shiyu Zhao",
      "tags": [
        "person"
      ],
      "content": "#person"
    },
    {
      "id": "Matthew Groh",
      "title": "Matthew Groh",
      "tags": [
        "person"
      ],
      "content": "#person"
    },
    {
      "id": "Ralf H. J. M. Kurvers",
      "title": "Ralf H. J. M. Kurvers",
      "tags": [
        "person"
      ],
      "content": "#person"
    },
    {
      "id": "Ivan Vendrov",
      "title": "Ivan Vendrov",
      "tags": [
        "person"
      ],
      "content": "#person \n\nhttps://www.vendrov.ai/"
    },
    {
      "id": "Jonas Kunst",
      "title": "Jonas Kunst",
      "tags": [
        "person"
      ],
      "content": "#person \n"
    },
    {
      "id": "Nat McAleese",
      "title": "Nat McAleese",
      "tags": [
        "person"
      ],
      "content": "#person"
    },
    {
      "id": "Elicit",
      "title": "Elicit",
      "tags": [
        "organization"
      ],
      "content": "#organization \n\nhttps://elicit.com/"
    },
    {
      "id": "Can AI bring deliberative democracy to the masses?",
      "title": "Can AI bring deliberative democracy to the masses?",
      "tags": [
        "project"
      ],
      "content": "#project \n\nhttps://www.law.nyu.edu/sites/default/files/Helen%20Landemore%20Can%20AI%20bring%20deliberative%20democracy%20to%20the%20masses.pdf\n\nConceptual paper analyzes France's 2019 \"Great National Debate\" as a case study for scaling deliberative democracy, then theorizes two AI-augmented models: \n1. mass online deliberation platforms that use algorithms to cluster and organize arguments among thousands of participants, and \n2. rotating randomly-selected citizen assemblies supported by AI for facilitation, translation, fact-checking, and data synthesis."
    },
    {
      "id": "Colin Megill",
      "title": "Colin Megill",
      "tags": [
        "person"
      ],
      "content": "#person \n\nhttps://colinmegill.com/"
    },
    {
      "id": "Diana Acosta-Navas",
      "title": "Diana Acosta-Navas",
      "tags": [
        "person"
      ],
      "content": "#person"
    },
    {
      "id": "John Bash",
      "title": "John Bash",
      "tags": [
        "person"
      ],
      "content": "#person"
    },
    {
      "id": "Andreas Stuhlmüller",
      "title": "Andreas Stuhlmüller",
      "tags": [
        "person"
      ],
      "content": "#person \n\nhttps://stuhlmueller.org/"
    },
    {
      "id": "The AI Adoption Gap - Preparing the US Government for Advanced AI",
      "title": "The AI Adoption Gap - Preparing the US Government for Advanced AI",
      "tags": [
        "project"
      ],
      "content": "#project \n\nhttps://www.forethought.org/research/the-ai-adoption-gap\n\nResearch piece examining how the US federal government is falling behind the private sector in adopting AI technologies, creating risks for democratic institutions and national security. The research argues this gap could leave the government unable to effectively respond to AI-driven existential challenges or maintain oversight of rapidly advancing AI systems."
    },
    {
      "id": "Alex Krasodomski-Jones",
      "title": "Alex Krasodomski-Jones",
      "tags": [
        "person"
      ],
      "content": "#person"
    },
    {
      "id": "Zachary Jacobs",
      "title": "Zachary Jacobs",
      "tags": [
        "person"
      ],
      "content": "#person"
    },
    {
      "id": "Rethinking Machine Ethics –Can LLMs Perform Moral Reasoning through the Lens of Moral Theories?",
      "title": "Rethinking Machine Ethics –Can LLMs Perform Moral Reasoning through the Lens of Moral Theories?",
      "tags": [
        "project"
      ],
      "content": "#project \n\nhttps://arxiv.org/pdf/2308.15399\n\nPaper presents a framework for enabling LLMs to make moral judgments by grounding them in established moral theories rather than learning from crowdsourced data. The researchers develop prompting techniques that guide models like GPT-4 to reason through ethical scenarios using theories from normative ethics (Justice, Deontology, Utilitarianism) and moral psychology (Theory of Dyadic Morality), producing explainable moral reasoning and decisions."
    },
    {
      "id": "Joshua Levy",
      "title": "Joshua Levy",
      "tags": [
        "person"
      ],
      "content": "#person \n\nhttps://github.com/jlevy"
    },
    {
      "id": "Goodheart Labs",
      "title": "Goodheart Labs",
      "tags": [
        "organization"
      ],
      "content": "#organization \n\nhttps://goodheartlabs.com/"
    },
    {
      "id": "Artifact",
      "title": "Artifact",
      "tags": [
        "project"
      ],
      "content": "#project \n\nhttps://techcrunch.com/2024/01/18/why-artifact-from-instagrams-founders-failed-shut-down/\n\nArtifact was an AI-powered news aggregation app created by Instagram's co-founders that used machine learning to recommend articles, summarize news stories, and rewrite clickbait headlines into clearer formats. The app featured social elements like commenting and following, creating an engaged community around news consumption, but ultimately shut down in early 2024.\n\nThey failed because they couldn't achieve sustainable user growth beyond their initial core community, despite having a quality product that users genuinely engaged with."
    },
    {
      "id": "Wearable Reasoner",
      "title": "Wearable Reasoner",
      "tags": [
        "project"
      ],
      "content": "#project \n\nhttps://dl.acm.org/doi/pdf/10.1145/3384657.3384799\n\nProof-of-concept wearable device (smart glasses with audio feedback) that uses AI to analyze spoken arguments in real-time and tell users whether claims are supported by evidence or not. The system employs argumentation mining techniques to classify statements and provides either simple feedback (\"supported/unsupported\") or explainable feedback that describes what type of evidence was found."
    },
    {
      "id": "Yuan-Fang Li",
      "title": "Yuan-Fang Li",
      "tags": [
        "person"
      ],
      "content": "#person"
    },
    {
      "id": "Eli Lifland",
      "title": "Eli Lifland",
      "tags": [
        "person"
      ],
      "content": "#person \n\nhttps://www.elilifland.com/"
    },
    {
      "id": "David Garcia",
      "title": "David Garcia",
      "tags": [
        "person"
      ],
      "content": "#person \n"
    },
    {
      "id": "Brendan Fong",
      "title": "Brendan Fong",
      "tags": [
        "person"
      ],
      "content": "#person \n\nhttp://www.brendanfong.com/"
    },
    {
      "id": "Iterated Decomposition - Improving Science Q&A by Supervising Reasoning Processes",
      "title": "Iterated Decomposition - Improving Science Q&A by Supervising Reasoning Processes",
      "tags": [
        "project"
      ],
      "content": "#project \n\nhttps://arxiv.org/pdf/2301.01751\nhttps://github.com/oughtinc/ice\n\nA human-in-the-loop workflow for breaking down complex question-answering tasks into smaller, interpretable steps for language models. The approach involves systematically decomposing complex tasks, evaluating intermediate results, diagnosing failures, and refining the decomposition through multiple iterations, supported by a visualization tool called ICE."
    },
    {
      "id": "Deceptive Explanations by Large Language Models Lead People to Change their Beliefs About Misinformation More Often than Honest Explanations",
      "title": "Deceptive Explanations by Large Language Models Lead People to Change their Beliefs About Misinformation More Often than Honest Explanations",
      "tags": [
        "project"
      ],
      "content": "#project \n\nhttps://dl.acm.org/doi/pdf/10.1145/3706598.3713408\n\nResearch study examines how AI-generated explanations can be used to combat misinformation more effectively than simple false classifications alone. The researchers conducted an experiment with 589 participants who rated the truthfulness of news headlines before and after receiving AI feedback, finding that deceptive explanations were significantly more persuasive than both honest explanations and deceptive classifications without explanations."
    },
    {
      "id": "Peter Mühlbacher",
      "title": "Peter Mühlbacher",
      "tags": [
        "person"
      ],
      "content": "#person \n\nhttp://peter.muehlbacher.me/"
    },
    {
      "id": "Lawrence Phillips",
      "title": "Lawrence Phillips",
      "tags": [
        "person"
      ],
      "content": "#person"
    },
    {
      "id": "Junan Li",
      "title": "Junan Li",
      "tags": [
        "person"
      ],
      "content": "#person"
    },
    {
      "id": "Shu Yang Lin",
      "title": "Shu Yang Lin",
      "tags": [
        "person"
      ],
      "content": "#person \n\nhttps://www.shuyanglin.com/"
    },
    {
      "id": "Grim",
      "title": "Grim",
      "tags": [
        "project"
      ],
      "content": "#project \n\nhttps://github.com/SentinelTeam/grim\nhttps://www.lesswrong.com/posts/eR69f3hi5ozxchhYg/scaling-wargaming-for-global-catastrophic-risks-with-ai\n\nAI-powered wargaming tool that uses LLMs to simulate complex catastrophic scenarios and help organizations practice emergency responses. The tool functions as a Telegram bot where participants can take actions, request information, and feed data into crisis simulations, with AI serving as both forecaster and game master to create detailed, dynamic scenarios."
    },
    {
      "id": "Nikos Bosse",
      "title": "Nikos Bosse",
      "tags": [
        "person"
      ],
      "content": "#person \n\nhttps://followtheargument.org/"
    },
    {
      "id": "Aleks Berditchevskaia",
      "title": "Aleks Berditchevskaia",
      "tags": [
        "person"
      ],
      "content": "#person"
    },
    {
      "id": "Rolf Kleef",
      "title": "Rolf Kleef",
      "tags": [
        "person"
      ],
      "content": "#person"
    },
    {
      "id": "Raphael Köster",
      "title": "Raphael Köster",
      "tags": [
        "person"
      ],
      "content": "#person"
    },
    {
      "id": "Ravi Iyer",
      "title": "Ravi Iyer",
      "tags": [
        "person"
      ],
      "content": "#person"
    },
    {
      "id": "Jacquelyn Schneider",
      "title": "Jacquelyn Schneider",
      "tags": [
        "person"
      ],
      "content": "#person"
    },
    {
      "id": "Xin Lucy Liu",
      "title": "Xin Lucy Liu",
      "tags": [
        "person"
      ],
      "content": "#person"
    },
    {
      "id": "Jamie Joyce",
      "title": "Jamie Joyce",
      "tags": [
        "person"
      ],
      "content": "#person \n\nhttps://www.jamiejoyce.com/"
    },
    {
      "id": "Deep Ganguli",
      "title": "Deep Ganguli",
      "tags": [
        "person"
      ],
      "content": "#person \n\nhttps://dganguli.github.io/pweb/"
    },
    {
      "id": "Eric Schwitzgebel",
      "title": "Eric Schwitzgebel",
      "tags": [
        "person"
      ],
      "content": "#person"
    },
    {
      "id": "Series",
      "title": "Series",
      "tags": [
        "project"
      ],
      "content": "#project \n\nhttps://www.series.so/\n\nSeries is an AI-powered networking platform that uses automated agents to facilitate professional introductions between users via messaging platforms like iMessage. The system analyzes user profiles and objectives to identify potential matches based on calculated mutual benefit, then initiates double opt-in introductions without requiring users to actively search or request connections."
    },
    {
      "id": "$SL Muses",
      "title": "$SL Muses",
      "tags": [
        "project"
      ],
      "content": "#project \nhttps://www.sltoken.xyz/\n\nAI agents called \"Muses\" that users can tag on social media platforms to perform reasoning tasks like fact-checking claims and researching topics. Their \"Muse of Truth\" provides substantive analysis of factual claims with evidence gaps and source references, while the \"Muse of Research\" pulls resources on requested topics.\n\nThe initiative was funded through a community-created meme token ($SL) that was anonymously gifted to the charity, with 50% of holdings donated to support their mission."
    },
    {
      "id": "Rajiv Movva",
      "title": "Rajiv Movva",
      "tags": [
        "person"
      ],
      "content": "#person"
    },
    {
      "id": "Farhad Moghimifar",
      "title": "Farhad Moghimifar",
      "tags": [
        "person"
      ],
      "content": "#person"
    },
    {
      "id": "Lukas Finnveden",
      "title": "Lukas Finnveden",
      "tags": [
        "person"
      ],
      "content": "#person"
    },
    {
      "id": "Reddit",
      "title": "Reddit",
      "tags": [
        "organization"
      ],
      "content": "#organization"
    },
    {
      "id": "Samuel Aeschbach",
      "title": "Samuel Aeschbach",
      "tags": [
        "person"
      ],
      "content": "#person"
    },
    {
      "id": "Indre Tuminauskaite",
      "title": "Indre Tuminauskaite",
      "tags": [
        "person"
      ],
      "content": "#person"
    },
    {
      "id": "Israel-Palestine Collective Dialogues",
      "title": "Israel-Palestine Collective Dialogues",
      "tags": [
        "project"
      ],
      "content": "#project \n\nhttps://arxiv.org/abs/2503.01769\n\nPaper describes a case study of using AI and collective dialogue methods to help Israeli and Palestinian peacebuilders find common ground during the conflict following October 7, 2023. The research integrates LLMs, bridging-based ranking algorithms, and online collective dialogues to identify shared perspectives across divided groups."
    },
    {
      "id": "Jaromir Savelka",
      "title": "Jaromir Savelka",
      "tags": [
        "person"
      ],
      "content": "#person"
    },
    {
      "id": "Opportunities and Risks of LLMs for Scalable Deliberation with Polis",
      "title": "Opportunities and Risks of LLMs for Scalable Deliberation with Polis",
      "tags": [
        "project"
      ],
      "content": "#project \n\nhttps://arxiv.org/pdf/2306.11932\n\nPaper describes LLMs to enhance [Polis](https://pol.is), a platform for large-scale democratic deliberation and public opinion mapping. The research demonstrates how AI can augment (not replace) human intelligence in collective decision-making processes through tasks like summarization, topic modeling, and facilitating consensus-finding."
    },
    {
      "id": "Luca Righetti",
      "title": "Luca Righetti",
      "tags": [
        "person"
      ],
      "content": "#person"
    },
    {
      "id": "Anka Reuel",
      "title": "Anka Reuel",
      "tags": [
        "person"
      ],
      "content": "#person"
    },
    {
      "id": "Andrew Konya",
      "title": "Andrew Konya",
      "tags": [
        "person"
      ],
      "content": "#person \n\nhttps://andrewkonya.com/"
    },
    {
      "id": "AI Epistemics",
      "title": "AI Epistemics",
      "tags": [
        "topic"
      ],
      "content": "#topic"
    },
    {
      "id": "IQT Labs",
      "title": "IQT Labs",
      "tags": [
        "organization"
      ],
      "content": "#organization \n\nhttps://www.iqt.org/"
    },
    {
      "id": "Gholamreza Haffari",
      "title": "Gholamreza Haffari",
      "tags": [
        "person"
      ],
      "content": "#person"
    },
    {
      "id": "Joe Edelman",
      "title": "Joe Edelman",
      "tags": [
        "person"
      ],
      "content": "#person \n\nhttps://nxhx.org/"
    },
    {
      "id": "MIT Media Lab",
      "title": "MIT Media Lab",
      "tags": [
        "organization"
      ],
      "content": "#organization \n\nhttps://www.media.mit.edu/"
    },
    {
      "id": "Evangelos Pournaras",
      "title": "Evangelos Pournaras",
      "tags": [
        "person"
      ],
      "content": "#person"
    },
    {
      "id": "Jacob Ganz",
      "title": "Jacob Ganz",
      "tags": [
        "person"
      ],
      "content": "#person"
    },
    {
      "id": "Francisco Marmolejo-Cossío",
      "title": "Francisco Marmolejo-Cossío",
      "tags": [
        "person"
      ],
      "content": "#person \n"
    },
    {
      "id": "Jan Balaguer",
      "title": "Jan Balaguer",
      "tags": [
        "person"
      ],
      "content": "#person \n\nhttps://www.janbalaguer.net/"
    },
    {
      "id": "The Society Library",
      "title": "The Society Library",
      "tags": [
        "organization"
      ],
      "content": "#organization \n\nhttps://societylibrary.medium.com/\nhttps://www.societylibrary.org/"
    },
    {
      "id": "Mantic",
      "title": "Mantic",
      "tags": [
        "organization"
      ],
      "content": "#organization \n\nhttps://mntc.ai/"
    },
    {
      "id": "Peter Park",
      "title": "Peter Park",
      "tags": [
        "person"
      ],
      "content": "#person"
    },
    {
      "id": "Peter Wills",
      "title": "Peter Wills",
      "tags": [
        "person"
      ],
      "content": "#person"
    },
    {
      "id": "Flynn Devine",
      "title": "Flynn Devine",
      "tags": [
        "person"
      ],
      "content": "#person"
    },
    {
      "id": "Gordon Pennycook",
      "title": "Gordon Pennycook",
      "tags": [
        "person"
      ],
      "content": "#person \n"
    },
    {
      "id": "Dawn Song",
      "title": "Dawn Song",
      "tags": [
        "person"
      ],
      "content": "#person \n\nhttps://dawnsong.io/"
    },
    {
      "id": "Wisdom of the silicon crowd - LLM ensemble prediction capabilities rival human crowd accuracy",
      "title": "Wisdom of the silicon crowd - LLM ensemble prediction capabilities rival human crowd accuracy",
      "tags": [
        "project"
      ],
      "content": "#project \n\nhttps://www.science.org/doi/pdf/10.1126/sciadv.adp1528\n\nPaper demonstrates that an ensemble of 12 different large language models can achieve forecasting accuracy that rivals human crowd predictions in real-world forecasting tournaments. The study shows that while individual LLMs typically underperform human crowds, aggregating predictions from multiple LLMs creates a \"wisdom of the silicon crowd\" effect that matches human accuracy on 31 binary forecasting questions."
    },
    {
      "id": "Anthony Corso",
      "title": "Anthony Corso",
      "tags": [
        "person"
      ],
      "content": "#person"
    },
    {
      "id": "Aviv Ovadya",
      "title": "Aviv Ovadya",
      "tags": [
        "person"
      ],
      "content": "#person \n\nhttps://aviv.me/"
    },
    {
      "id": "Michiel Bakker",
      "title": "Michiel Bakker",
      "tags": [
        "person"
      ],
      "content": "#person \n\nhttps://miba.dev/"
    },
    {
      "id": "Let’s use AI to harden human defenses against AI manipulation",
      "title": "Let’s use AI to harden human defenses against AI manipulation",
      "tags": [
        "project"
      ],
      "content": "#project \n\nhttps://www.lesswrong.com/posts/zxmzBTwKkPMxQQcfR/let-s-use-ai-to-harden-human-defenses-against-ai\n\nBlog post proposes using AI systems to discover and catalog human manipulation techniques in order to train \"detector-AIs\" and humans to recognize these tactics, essentially creating a defensive system against AI manipulation. The approach involves optimizing AIs to persuade humans of both truths and falsehoods, analyzing what manipulation strategies they develop, and then training detection systems to identify these techniques in real-world scenarios."
    },
    {
      "id": "Anne-Marie Nussberger",
      "title": "Anne-Marie Nussberger",
      "tags": [
        "person"
      ],
      "content": "#person"
    },
    {
      "id": "Mapping AI Discourse",
      "title": "Mapping AI Discourse",
      "tags": [
        "topic"
      ],
      "content": "#topic"
    },
    {
      "id": "Wei Dai",
      "title": "Wei Dai",
      "tags": [
        "person"
      ],
      "content": "#person \n\nhttps://wdai.us/"
    },
    {
      "id": "Finn Hambly",
      "title": "Finn Hambly",
      "tags": [
        "person"
      ],
      "content": "#person"
    },
    {
      "id": "Christopher Small",
      "title": "Christopher Small",
      "tags": [
        "person"
      ],
      "content": "#person \n\nhttp://metasoarous.com/"
    },
    {
      "id": "Lisa Schirch",
      "title": "Lisa Schirch",
      "tags": [
        "person"
      ],
      "content": "#person \n\nhttps://kroc.nd.edu/faculty-and-staff/lisa-schirch/"
    },
    {
      "id": "Nikhil Garg",
      "title": "Nikhil Garg",
      "tags": [
        "person"
      ],
      "content": "#person"
    },
    {
      "id": "Sensemaker",
      "title": "Sensemaker",
      "tags": [
        "project"
      ],
      "content": "#project \n\nhttps://github.com/Jigsaw-Code/sensemaking-tools/\nhttps://report.whatcouldbgbe.com/\n\nCollection of AI-powered systems designed to help analyze and understand large-scale online conversations by automatically identifying topics, categorizing statements, and summarizing areas of agreement and disagreement from thousands of public comments."
    },
    {
      "id": "Joshua Becker",
      "title": "Joshua Becker",
      "tags": [
        "person"
      ],
      "content": "#person"
    },
    {
      "id": "Frank Schweitzer",
      "title": "Frank Schweitzer",
      "tags": [
        "person"
      ],
      "content": "#person \n"
    },
    {
      "id": "Elizabeth Barry",
      "title": "Elizabeth Barry",
      "tags": [
        "person"
      ],
      "content": "#person"
    },
    {
      "id": "Zhuoran Lu",
      "title": "Zhuoran Lu",
      "tags": [
        "person"
      ],
      "content": "#person"
    },
    {
      "id": "Pat Pataranutaporn",
      "title": "Pat Pataranutaporn",
      "tags": [
        "person"
      ],
      "content": "#person \n\nhttps://patpat.world/"
    },
    {
      "id": "LLMs Can Teach Themselves to Better Predict the Future",
      "title": "LLMs Can Teach Themselves to Better Predict the Future",
      "tags": [
        "project"
      ],
      "content": "#project \n\nhttps://arxiv.org/pdf/2502.05253\n\nMethod for improving large language models' forecasting capabilities through self-play and outcome-driven fine-tuning, where models generate multiple reasoning traces for prediction questions and learn from which approaches led to more accurate forecasts. The authors demonstrate that smaller models (14B parameters) fine-tuned with this method can achieve forecasting performance comparable to much larger frontier models like GPT-4o."
    },
    {
      "id": "Martin Chadwick",
      "title": "Martin Chadwick",
      "tags": [
        "person"
      ],
      "content": "#person"
    },
    {
      "id": "Gary Marcus",
      "title": "Gary Marcus",
      "tags": [
        "person"
      ],
      "content": "#person \n\nhttp://www.garymarcus.com/"
    },
    {
      "id": "Fred Zhang",
      "title": "Fred Zhang",
      "tags": [
        "person"
      ],
      "content": "#person \n\nhttps://fredzhang.me/"
    },
    {
      "id": "Nexus",
      "title": "Nexus",
      "tags": [
        "project"
      ],
      "content": "#project \n\nhttps://nexus-tool.com/\n\nNexus is a discussion platform that organizes user statements and reactions into a visual graph, where semantically similar statements are connected and colored based on consensus, divergence, and quality scores. The tool specifically counteracts factionalism and amplifying unique perspectives by weighting minority voices more heavily, highlighting surprising agreement between usual opponents (consensus), and promoting dissenting views within like-minded groups (divergence)."
    },
    {
      "id": "AI Objectives Institute",
      "title": "AI Objectives Institute",
      "tags": [
        "organization"
      ],
      "content": "#organization \n\nhttps://ai.objectives.institute/"
    },
    {
      "id": "Ryan Jia",
      "title": "Ryan Jia",
      "tags": [
        "person"
      ],
      "content": "#person"
    },
    {
      "id": "Ought",
      "title": "Ought",
      "tags": [
        "organization"
      ],
      "content": "#organization \n\nhttps://ought.org/ (website is old)\n\nOught 1.0 has effectively become what is now Elicit (the company) continuing development on Elicit (the tool). Ought 2.0 has not yet done any public work."
    },
    {
      "id": "Prateek Buch",
      "title": "Prateek Buch",
      "tags": [
        "person"
      ],
      "content": "#person"
    },
    {
      "id": "Martin Saveski",
      "title": "Martin Saveski",
      "tags": [
        "person"
      ],
      "content": "#person"
    },
    {
      "id": "Paul Gölz",
      "title": "Paul Gölz",
      "tags": [
        "person"
      ],
      "content": "#person"
    },
    {
      "id": "Meaning Alignment Institute",
      "title": "Meaning Alignment Institute",
      "tags": [
        "organization"
      ],
      "content": "#organization \n\nhttps://www.meaningalignment.org/"
    },
    {
      "id": "Q1 AI Benchmarking results",
      "title": "Q1 AI Benchmarking results",
      "tags": [
        "project"
      ],
      "content": "#project \n\nhttps://www.metaculus.com/notebooks/38673/q1-ai-benchmarking-results/\n\nReport on Metaculus's quarterly tournament that compared AI forecasting bots against professional human forecasters on real-world prediction questions, with the key finding that professional forecasters significantly outperformed the best AI systems."
    },
    {
      "id": "Lucie Flek",
      "title": "Lucie Flek",
      "tags": [
        "person"
      ],
      "content": "#person"
    },
    {
      "id": "Harold Trinkunas",
      "title": "Harold Trinkunas",
      "tags": [
        "person"
      ],
      "content": "#person"
    },
    {
      "id": "Nasim Rahaman",
      "title": "Nasim Rahaman",
      "tags": [
        "person"
      ],
      "content": "#person"
    },
    {
      "id": "Xiaoying Zhang",
      "title": "Xiaoying Zhang",
      "tags": [
        "person"
      ],
      "content": "#person"
    },
    {
      "id": "Roast My Post",
      "title": "Roast My Post",
      "tags": [
        "project"
      ],
      "content": "#project \n\nhttps://www.roastmypost.org\nhttps://github.com/quantified-uncertainty/roast-my-post\n\nOpen-source AI-powered platform for document analysis and evaluation. It allows users to upload documents and receive feedback with inline comments from customizable AI agents."
    },
    {
      "id": "Forecasting Research Institute",
      "title": "Forecasting Research Institute",
      "tags": [
        "organization"
      ],
      "content": "#organization \n\nhttps://forecastingresearch.org/"
    },
    {
      "id": "Christopher Summerfield",
      "title": "Christopher Summerfield",
      "tags": [
        "person"
      ],
      "content": "#person"
    },
    {
      "id": "Colin Irwin",
      "title": "Colin Irwin",
      "tags": [
        "person"
      ],
      "content": "#person"
    },
    {
      "id": "Global Dialogues",
      "title": "Global Dialogues",
      "tags": [
        "project"
      ],
      "content": "#project \n\nhttps://globaldialogues.ai\n\nResearch initiative that uses AI-powered interactive dialogue platforms to systematically map global public perspectives on AI development and its societal impacts."
    },
    {
      "id": "X Community Notes",
      "title": "X Community Notes",
      "tags": [
        "organization"
      ],
      "content": "#organization \n\nhttps://communitynotes.x.com/guide/en/welcome"
    },
    {
      "id": "Hoover Wargaming and Crisis Simulation Initiative",
      "title": "Hoover Wargaming and Crisis Simulation Initiative",
      "tags": [
        "organization"
      ],
      "content": "#organization \n\nhttps://www.hoover.org/research-teams/wargaming-and-crisis-simulation-initiative"
    },
    {
      "id": "Luke Stebbing",
      "title": "Luke Stebbing",
      "tags": [
        "person"
      ],
      "content": "#person \n\nhttps://lukestebbing.com/"
    },
    {
      "id": "How Malicious AI Swarms Can Threaten Democracy",
      "title": "How Malicious AI Swarms Can Threaten Democracy",
      "tags": [
        "project"
      ],
      "content": "#project \n\nhttps://arxiv.org/pdf/2506.06299\n\nExamines how coordinated networks of AI agents (\"AI swarms\") can manipulate democratic discourse and decision-making through sophisticated influence operations. The authors warn that unlike traditional botnets that simply repeat scripted messages, AI swarms can create thousands of distinct personas that adapt, learn from feedback, and coordinate autonomously to fabricate grassroots consensus and fragment shared reality."
    },
    {
      "id": "CeesJan Mol",
      "title": "CeesJan Mol",
      "tags": [
        "person"
      ],
      "content": "#person"
    },
    {
      "id": "Philip Tetlock",
      "title": "Philip Tetlock",
      "tags": [
        "person"
      ],
      "content": "#person"
    },
    {
      "id": "Sayash Kapoor",
      "title": "Sayash Kapoor",
      "tags": [
        "person"
      ],
      "content": "#person"
    },
    {
      "id": "Plurality Mapping Project",
      "title": "Plurality Mapping Project",
      "tags": [
        "project"
      ],
      "content": "#project \n\nhttps://publish.obsidian.md/plurality-map"
    },
    {
      "id": "Juan-Pablo Rivera",
      "title": "Juan-Pablo Rivera",
      "tags": [
        "person"
      ],
      "content": "#person"
    },
    {
      "id": "Anita W. Woolley",
      "title": "Anita W. Woolley",
      "tags": [
        "person"
      ],
      "content": "#person"
    },
    {
      "id": "AI4Democracy - How AI Can Be Used to Inform Policymaking?",
      "title": "AI4Democracy - How AI Can Be Used to Inform Policymaking?",
      "tags": [
        "project"
      ],
      "content": "#project \n\nhttps://ai.objectives.institute/blog/ai4democracy-paper-how-ai-can-be-used-to-inform-policymaking\n\nReport on how LLMs can be used to process large volumes of public input and aggregate opinions for policymakers, specifically with \"[Talk to the City](https://ai.objectives.institute/talk-to-the-city).\" The paper examines three real-world case studies (including one on [DAO](https://en.wikipedia.org/wiki/Decentralized_autonomous_organization) governance)."
    },
    {
      "id": "Deep Research Bench - Evaluating AI Web Research Agents",
      "title": "Deep Research Bench - Evaluating AI Web Research Agents",
      "tags": [
        "project"
      ],
      "content": "#project \n\nhttps://arxiv.org/abs/2506.06287\nhttps://evals.futuresearch.ai/\n\nBenchmark for evaluating AI agents on complex, multi-step web research tasks that mirror real-world analytical work, using a frozen web environment to ensure consistent and repeatable evaluations. The benchmark tests research capabilities including strategic planning, source credibility assessment, systematic information gathering, and synthesis across contradictory sources through eight different task categories."
    },
    {
      "id": "Edith Elkind",
      "title": "Edith Elkind",
      "tags": [
        "person"
      ],
      "content": "#person"
    },
    {
      "id": "Wargames & Scenario Planning",
      "title": "Wargames & Scenario Planning",
      "tags": [
        "topic"
      ],
      "content": "#topic"
    },
    {
      "id": "Common Ground",
      "title": "Common Ground",
      "tags": [
        "project"
      ],
      "content": "#project \n\nhttps://www.dembrane.com/en-US/blog/report-openai-october-2023\n\nAI-powered platform for democratic deliberation that organizes people into small groups to discuss and vote on statements, with an AI moderator (powered by GPT-4) synthesizing new statements from their live video conversations with each other. Statements produced by each group are \"cross pollinated\" to each other and ultimately synthesized into final consensus statements.\n\nFunded by [OpenAI Democratic Inputs to AI](https://github.com/openai/democratic-inputs)."
    },
    {
      "id": "Echo",
      "title": "Echo",
      "tags": [
        "project"
      ],
      "content": "#project \n\nhttps://www.dembrane.com/en-US/products/echo\nhttps://github.com/dembrane/echo\n\nECHO is a transcription, analysis, and reporting platform designed for multilingual conversations and stakeholder engagement scenarios. It can capture multiple simultaneous conversations in real-time, transcribe them across multiple languages, and use AI to analyze the content and generate insights from the discussions."
    },
    {
      "id": "Dembrane",
      "title": "Dembrane",
      "tags": [
        "organization"
      ],
      "content": "#organization \n\nhttps://www.dembrane.com\n\nFunded (in part) by the European Union via the [AI4Deliberation](https://www.ai4dproject.eu/) project."
    },
    {
      "id": "John Aslanides",
      "title": "John Aslanides",
      "tags": [
        "person"
      ],
      "content": "#person"
    },
    {
      "id": "Supernotes",
      "title": "Supernotes",
      "tags": [
        "project"
      ],
      "content": "#project \n\nhttps://dl.acm.org/doi/pdf/10.1145/3696410.3714934\n\nAI system that synthesizes multiple community-written fact-checking notes into single, more effective notes that build consensus among diverse users. The system uses an LLM to generate many candidate notes from existing human-written notes, then employs a machine learning model trained on millions of historical ratings to predict which candidates would be most helpful to users with different viewpoints."
    },
    {
      "id": "Accelerated Preference Elicitation with LLM-Based Proxies",
      "title": "Accelerated Preference Elicitation with LLM-Based Proxies",
      "tags": [
        "project"
      ],
      "content": "#project \n\nhttps://arxiv.org/pdf/2501.14625\n\nAI system that help people communicate their preferences more efficiently in complex auctions by using natural language instead of technical queries. The system uses LLM-powered \"proxies\" that learn what people want through conversational interactions and can infer preferences for items not explicitly discussed."
    },
    {
      "id": "Policy",
      "title": "Policy",
      "tags": [
        "topic"
      ],
      "content": "#topic"
    },
    {
      "id": "LLM-Powered Preference Elicitation in Combinatorial Assignment",
      "title": "LLM-Powered Preference Elicitation in Combinatorial Assignment",
      "tags": [
        "project"
      ],
      "content": "#project \n\nhttps://arxiv.org/pdf/2502.10308\n\nAI system that allows people to express complex preferences through natural language rather than answering technical queries in allocation mechanisms. For example, in course registration, students could provide a single free-text description of their preferences (e.g., \"I prefer courses scheduled closely together\" or \"Course A and B complement each other\"), and an LLM proxy would answer comparison queries on their behalf. Experiments testing the system were run using LLM-simulated people."
    },
    {
      "id": "Jon Evans",
      "title": "Jon Evans",
      "tags": [
        "person"
      ],
      "content": "#person \n\nhttps://rezendi.com"
    },
    {
      "id": "Nick Hikita",
      "title": "Nick Hikita",
      "tags": [
        "person"
      ],
      "content": "#person"
    },
    {
      "id": "Iyad Rahwan",
      "title": "Iyad Rahwan",
      "tags": [
        "person"
      ],
      "content": "#person"
    },
    {
      "id": "Anna Strasser",
      "title": "Anna Strasser",
      "tags": [
        "person"
      ],
      "content": "#person"
    },
    {
      "id": "Romuald Elie",
      "title": "Romuald Elie",
      "tags": [
        "person"
      ],
      "content": "#person"
    },
    {
      "id": "Grace Kwak Danciu",
      "title": "Grace Kwak Danciu",
      "tags": [
        "person"
      ],
      "content": "#person"
    },
    {
      "id": "Prompt Engineering Large Language Models’ Forecasting Capabilities",
      "title": "Prompt Engineering Large Language Models’ Forecasting Capabilities",
      "tags": [
        "project"
      ],
      "content": "#project \n\nhttps://arxiv.org/pdf/2506.01578\n\nPaper presents a systematic study of prompt engineering techniques for improving large language models' forecasting capabilities. The researchers tested 38 different prompts across four major LLMs using 100 forecasting questions, followed by a second study with compound and professionally-designed prompts. Surprisingly, they found that most prompt modifications had negligible or even negative effects on forecasting accuracy, with some techniques like explicit Bayesian reasoning actually making predictions worse."
    },
    {
      "id": "Midjourney",
      "title": "Midjourney",
      "tags": [
        "organization"
      ],
      "content": "#organization \n\nhttps://www.midjourney.com\n\nThe Collective Intelligence team at Midjourney (currently stealth)."
    },
    {
      "id": "Luke Hewitt",
      "title": "Luke Hewitt",
      "tags": [
        "person"
      ],
      "content": "#person"
    },
    {
      "id": "Ted Suzman",
      "title": "Ted Suzman",
      "tags": [
        "person"
      ],
      "content": "#person"
    },
    {
      "id": "Avital Balwit",
      "title": "Avital Balwit",
      "tags": [
        "person"
      ],
      "content": "#person"
    },
    {
      "id": "Ozzie Gooen",
      "title": "Ozzie Gooen",
      "tags": [
        "person"
      ],
      "content": "#person"
    },
    {
      "id": "Georgina Evans",
      "title": "Georgina Evans",
      "tags": [
        "person"
      ],
      "content": "#person \n\nhttps://sites.google.com/view/georginaevans"
    },
    {
      "id": "Ryan Lowe",
      "title": "Ryan Lowe",
      "tags": [
        "person"
      ],
      "content": "#person"
    },
    {
      "id": "Nicky Case",
      "title": "Nicky Case",
      "tags": [
        "person"
      ],
      "content": "#person\n\nhttps://ncase.me/"
    },
    {
      "id": "Justin Stimatze",
      "title": "Justin Stimatze",
      "tags": [
        "person"
      ],
      "content": "#person"
    },
    {
      "id": "Brett Hennig",
      "title": "Brett Hennig",
      "tags": [
        "person"
      ],
      "content": "#person"
    },
    {
      "id": "Scaling Deliberation",
      "title": "Scaling Deliberation",
      "tags": [
        "topic"
      ],
      "content": "#topic"
    },
    {
      "id": "Nandika Donthi",
      "title": "Nandika Donthi",
      "tags": [
        "person"
      ],
      "content": "#person"
    },
    {
      "id": "Live Voice Interface",
      "title": "Live Voice Interface",
      "tags": [
        "topic"
      ],
      "content": "#topic"
    },
    {
      "id": "Jay Baxter",
      "title": "Jay Baxter",
      "tags": [
        "person"
      ],
      "content": "#person \n\nhttps://jaybaxter.net/"
    },
    {
      "id": "GitHub Next",
      "title": "GitHub Next",
      "tags": [
        "organization"
      ],
      "content": "#organization \n\nhttps://githubnext.com"
    },
    {
      "id": "Elicit Tool",
      "title": "Elicit Tool",
      "tags": [
        "project"
      ],
      "content": "#project \n\nhttps://elicit.com/\n\nAI research assistant that helps users conduct literature reviews by searching academic papers, extracting key findings, and synthesizing research evidence to answer specific questions."
    },
    {
      "id": "Xixin Wu",
      "title": "Xixin Wu",
      "tags": [
        "person"
      ],
      "content": "#person"
    },
    {
      "id": "Language models can reduce asymmetry in information markets",
      "title": "Language models can reduce asymmetry in information markets",
      "tags": [
        "project"
      ],
      "content": "#project \n\nhttps://arxiv.org/pdf/2403.14443\n\nPaper presents the \"Information Bazaar,\" a simulated digital marketplace where AI agents powered by LLMs buy and sell information on behalf of principals. The system addresses the buyer's inspection paradox in information markets, where buyers need access to information to assess its value, but sellers need to limit access to prevent theft. The key innovation is that AI agents have dual capabilities: they can evaluate the quality of privileged information and can \"forget\" unpurchased content, allowing temporary inspection without unauthorized retention."
    },
    {
      "id": "Plurality Institute",
      "title": "Plurality Institute",
      "tags": [
        "organization"
      ],
      "content": "#organization"
    },
    {
      "id": "AI Tools for Existential Security",
      "title": "AI Tools for Existential Security",
      "tags": [
        "project"
      ],
      "content": "#project \n\nhttps://www.forethought.org/research/ai-tools-for-existential-security\n\nPaper by Forethought that argues that rather than simply trying to slow down AI development, we should strategically accelerate the development of specific AI applications that help humanity navigate existential risks. \n\nThree key categories of beneficial AI tools: \n1. epistemic applications to help us anticipate and plan for emerging challenges \n2. coordination-enabling applications to help diverse groups work together towards shared goals\n3. risk-targeted applications to address specific challenges"
    },
    {
      "id": "Renée DiResta",
      "title": "Renée DiResta",
      "tags": [
        "person"
      ],
      "content": "#person"
    },
    {
      "id": "Forecasting Tools",
      "title": "Forecasting Tools",
      "tags": [
        "project"
      ],
      "content": "#project \n\nhttps://github.com/Metaculus/forecasting-tools\n\nPython framework for building AI-powered forecasting systems that help humans predict future events more accurately."
    },
    {
      "id": "Karim Benyekhlef",
      "title": "Karim Benyekhlef",
      "tags": [
        "person"
      ],
      "content": "#person"
    },
    {
      "id": "Yara Kyrychenko",
      "title": "Yara Kyrychenko",
      "tags": [
        "person"
      ],
      "content": "#person \n"
    },
    {
      "id": "How large language models can reshape collective intelligence",
      "title": "How large language models can reshape collective intelligence",
      "tags": [
        "project"
      ],
      "content": "#project \n\nhttps://www.nature.com/articles/s41562-024-01959-9\n\nPaper examines how LLMs are reshaping collective intelligence (the ability of groups to solve problems better than individuals alone). The authors examine both the opportunities and risks that LLMs present for human collective reasoning and decision-making processes."
    },
    {
      "id": "Mike Krieger",
      "title": "Mike Krieger",
      "tags": [
        "person"
      ],
      "content": "#person"
    },
    {
      "id": "Democratic Fine-Tuning",
      "title": "Democratic Fine-Tuning",
      "tags": [
        "project"
      ],
      "content": "#project \n\nhttps://www.meaningalignment.org/research/openai-dft-the-first-moral-graph\n\nLLM-based system creates \"moral graphs\" by using a two-stage process that aims to uncover shared values underlying political disagreement. A specialized chatbot engages participants in dialogue about contentious scenarios, asking for personal stories and role models to extract the underlying value behind responses rather than collecting ideological commitments like slogans or rules. The system then shows participants stories of people transitioning between different values and asks whether such transitions represent gains in wisdom, creating a graph structure where edges represent consensus about which values are more comprehensive than others\n\nFunded by [OpenAI Democratic Inputs to AI](https://github.com/openai/democratic-inputs)."
    },
    {
      "id": "Michael Curry",
      "title": "Michael Curry",
      "tags": [
        "person"
      ],
      "content": "#person \n"
    },
    {
      "id": "Carl Miller",
      "title": "Carl Miller",
      "tags": [
        "person"
      ],
      "content": "#person \n\nhttps://www.carlmiller.co/"
    },
    {
      "id": "Oded Adomi Leshem",
      "title": "Oded Adomi Leshem",
      "tags": [
        "person"
      ],
      "content": "#person"
    },
    {
      "id": "Evie Yu-Yen Cheng",
      "title": "Evie Yu-Yen Cheng",
      "tags": [
        "person"
      ],
      "content": "#person"
    },
    {
      "id": "Ermis Soumalias",
      "title": "Ermis Soumalias",
      "tags": [
        "person"
      ],
      "content": "#person "
    },
    {
      "id": "Generative AI Voting - Fair Collective Choice is Resilient to LLM Biases and Inconsistencies",
      "title": "Generative AI Voting - Fair Collective Choice is Resilient to LLM Biases and Inconsistencies",
      "tags": [
        "project"
      ],
      "content": "#project \n\nhttps://arxiv.org/pdf/2406.11871\n\nResearch studies how LLMs can serve as representatives for human voters in democratic processes like participatory budgeting and elections. The research demonstrates that AI systems can effectively represent abstaining voters."
    },
    {
      "id": "Rai Sur",
      "title": "Rai Sur",
      "tags": [
        "person"
      ],
      "content": "#person \n\nhttps://rai.dev"
    },
    {
      "id": "Naomi Esther",
      "title": "Naomi Esther",
      "tags": [
        "person"
      ],
      "content": "#person"
    },
    {
      "id": "Research",
      "title": "Research",
      "tags": [
        "topic"
      ],
      "content": "#topic"
    },
    {
      "id": "Tantum Collins",
      "title": "Tantum Collins",
      "tags": [
        "person"
      ],
      "content": "#person"
    },
    {
      "id": "Bench to the Future - A Pastcasting Benchmark for Forecasting Agents",
      "title": "Bench to the Future - A Pastcasting Benchmark for Forecasting Agents",
      "tags": [
        "project"
      ],
      "content": "#project \n\nhttps://arxiv.org/abs/2506.21558\nhttps://evals.futuresearch.ai/\n\nEstablishes a \"pastcasting\" benchmark that evaluates AI forecasting capabilities by having LLMs predict outcomes of events that have already happened, using carefully curated web snapshots from before those events occurred. The benchmark addresses a key challenge in forecasting research - the long wait times between making predictions and learning outcomes."
    },
    {
      "id": "Multi-Agent Consensus Seeking via Large Language Models",
      "title": "Multi-Agent Consensus Seeking via Large Language Models",
      "tags": [
        "project"
      ],
      "content": "#project \n\nhttps://arxiv.org/abs/2310.20151\n\nPaper studies how multiple AI agents powered by large language models can reach consensus through negotiation, where each agent starts with a numerical value and must collectively agree on a final shared value. The research reveals that LLM agents naturally tend to use averaging strategies for consensus-seeking and that factors like agent personality (stubborn vs. suggestible), network topology, and group size significantly influence both the speed and outcome of negotiations."
    },
    {
      "id": "Online Deliberation Platform",
      "title": "Online Deliberation Platform",
      "tags": [
        "project"
      ],
      "content": "#project\n\nhttps://stanforddeliberate.org/\n\nStanford Deliberate is an AI-assisted online video platform designed to facilitate structured small group discussions using deliberative polling methodology, with an automated moderator that manages speaking queues, timed agendas, and ensures equitable participation among participants. The platform has been used for large-scale national deliberative polling events across multiple countries and languages, allowing unlimited participants to deliberate simultaneously in small groups on policy issues."
    },
    {
      "id": "How AI Agents Will Improve the Consultation Process",
      "title": "How AI Agents Will Improve the Consultation Process",
      "tags": [
        "project"
      ],
      "content": "#project \n\nhttps://ai.objectives.institute/blog/how-ai-agents-will-improve-consultation-process\n\nBlog post describes how AI agents can transform consultation processes by making them more interactive and engaging than traditional surveys. The AI Objectives Institute proposes using different types of AI agents: \n1. \"domain experts\" to help participants understand complex questions and organize their thoughts, \n2. \"professional interviewers\" to help refine and improve responses, and \n3. \"cross-pollination\" agents to help participants exchange ideas and find consensus by synthesizing diverse viewpoints."
    },
    {
      "id": "Toby Shevlane",
      "title": "Toby Shevlane",
      "tags": [
        "person"
      ],
      "content": "#person"
    },
    {
      "id": "Jorim Theuns",
      "title": "Jorim Theuns",
      "tags": [
        "person"
      ],
      "content": "#person \n\nhttps://jtheuns.com/"
    },
    {
      "id": "vTaiwan & g0v",
      "title": "vTaiwan & g0v",
      "tags": [
        "organization"
      ],
      "content": "#organization \n\nhttps://info.vtaiwan.tw/\nhttps://g0v.tw/"
    },
    {
      "id": "Google Jigsaw",
      "title": "Google Jigsaw",
      "tags": [
        "organization"
      ],
      "content": "#organization \n\nhttps://jigsaw.google.com/"
    },
    {
      "id": "Abdullah Almaatouq",
      "title": "Abdullah Almaatouq",
      "tags": [
        "person"
      ],
      "content": "#person"
    },
    {
      "id": "Jungwon Byun",
      "title": "Jungwon Byun",
      "tags": [
        "person"
      ],
      "content": "#person"
    },
    {
      "id": "Manuel Wüthrich",
      "title": "Manuel Wüthrich",
      "tags": [
        "person"
      ],
      "content": "#person"
    },
    {
      "id": "Richard Everett",
      "title": "Richard Everett",
      "tags": [
        "person"
      ],
      "content": "#person"
    },
    {
      "id": "Irwin King",
      "title": "Irwin King",
      "tags": [
        "person"
      ],
      "content": "#person"
    },
    {
      "id": "Pattie Maes",
      "title": "Pattie Maes",
      "tags": [
        "person"
      ],
      "content": "#person"
    },
    {
      "id": "Miruna Pîslar",
      "title": "Miruna Pîslar",
      "tags": [
        "person"
      ],
      "content": "#person"
    },
    {
      "id": "Hannes Westermann",
      "title": "Hannes Westermann",
      "tags": [
        "person"
      ],
      "content": "#person"
    },
    {
      "id": "Maria Antoniak",
      "title": "Maria Antoniak",
      "tags": [
        "person"
      ],
      "content": "#person"
    },
    {
      "id": "Lucid Lens",
      "title": "Lucid Lens",
      "tags": [
        "project"
      ],
      "content": "#project \n\nhttps://ai.objectives.institute/blog/straightlines-surfaces-the-content-beneath-the-headline\nhttps://github.com/AIObjectives/lucidlens\n\nChrome extension that automatically rewrites sensational or misleading news headlines to more accurately reflect the actual content of articles, using AI to read and summarize the full text before generating clearer headlines."
    },
    {
      "id": "Ian Baker",
      "title": "Ian Baker",
      "tags": [
        "person"
      ],
      "content": "#person"
    },
    {
      "id": "Minda Hu",
      "title": "Minda Hu",
      "tags": [
        "person"
      ],
      "content": "#person"
    },
    {
      "id": "Ralph Hertwig",
      "title": "Ralph Hertwig",
      "tags": [
        "person"
      ],
      "content": "#person"
    },
    {
      "id": "Dan Schwarz",
      "title": "Dan Schwarz",
      "tags": [
        "person"
      ],
      "content": "#person \n\nSome writing about [Google's internal prediction markets](https://asteriskmag.com/issues/08/the-death-and-life-of-prediction-markets-at-google)."
    },
    {
      "id": "Ben Turtel",
      "title": "Ben Turtel",
      "tags": [
        "person"
      ],
      "content": "#person"
    },
    {
      "id": "Matthew Spaniol",
      "title": "Matthew Spaniol",
      "tags": [
        "person"
      ],
      "content": "#person"
    },
    {
      "id": "Lucy Campbell-Gillingham",
      "title": "Lucy Campbell-Gillingham",
      "tags": [
        "person"
      ],
      "content": "#person"
    },
    {
      "id": "Deliberative Democracy Lab",
      "title": "Deliberative Democracy Lab",
      "tags": [
        "organization"
      ],
      "content": "#organization \n\nhttps://deliberation.stanford.edu/"
    },
    {
      "id": "Robert Thomson",
      "title": "Robert Thomson",
      "tags": [
        "person"
      ],
      "content": "#person"
    },
    {
      "id": "Lufeng Xu",
      "title": "Lufeng Xu",
      "tags": [
        "person"
      ],
      "content": "#person"
    },
    {
      "id": "Houtan Bastani",
      "title": "Houtan Bastani",
      "tags": [
        "person"
      ],
      "content": "#person"
    },
    {
      "id": "Charlie George",
      "title": "Charlie George",
      "tags": [
        "person"
      ],
      "content": "#person"
    },
    {
      "id": "Daniel Hnyk",
      "title": "Daniel Hnyk",
      "tags": [
        "person"
      ],
      "content": "#person"
    },
    {
      "id": "Divya Siddarth",
      "title": "Divya Siddarth",
      "tags": [
        "person"
      ],
      "content": "#person \n\nhttps://divyasiddarth.com/"
    },
    {
      "id": "Edwin Lock",
      "title": "Edwin Lock",
      "tags": [
        "person"
      ],
      "content": "#person \n"
    },
    {
      "id": "Zhou Yu",
      "title": "Zhou Yu",
      "tags": [
        "person"
      ],
      "content": "#person"
    },
    {
      "id": "Andrea Baronchelli",
      "title": "Andrea Baronchelli",
      "tags": [
        "person"
      ],
      "content": "#person \n"
    },
    {
      "id": "Natasha Jensen",
      "title": "Natasha Jensen",
      "tags": [
        "person"
      ],
      "content": "#person"
    },
    {
      "id": "Don't Just Tell Me, Ask Me",
      "title": "Don't Just Tell Me, Ask Me",
      "tags": [
        "project"
      ],
      "content": "#project \n\nhttps://dl.acm.org/doi/pdf/10.1145/3544548.3580672\n\nResearch paper presents \"AI-framed Questioning\" - a method where AI systems ask users strategic questions instead of directly telling them answers, designed to improve human critical thinking and logical reasoning. The study found that when people were asked questions like \"If (premise), does it follow that (conclusion)?\" they performed significantly better at identifying logical fallacies in socially divisive statements compared to both receiving direct AI explanations or no assistance at all."
    },
    {
      "id": "Chandler Smith",
      "title": "Chandler Smith",
      "tags": [
        "person"
      ],
      "content": "#person"
    },
    {
      "id": "Rafael Valdece Sousa Bastos",
      "title": "Rafael Valdece Sousa Bastos",
      "tags": [
        "person"
      ],
      "content": "#person"
    },
    {
      "id": "Barbara Mellers",
      "title": "Barbara Mellers",
      "tags": [
        "person"
      ],
      "content": "#person"
    },
    {
      "id": "Topos Institute",
      "title": "Topos Institute",
      "tags": [
        "organization"
      ],
      "content": "#organization \n\nhttps://topos.institute"
    },
    {
      "id": "Yaoli Mao",
      "title": "Yaoli Mao",
      "tags": [
        "person"
      ],
      "content": "#person"
    },
    {
      "id": "Hélène Landemore",
      "title": "Hélène Landemore",
      "tags": [
        "person"
      ],
      "content": "#person\n\nhttps://www.helenelandemore.com/"
    },
    {
      "id": "Michael Henry Tessler",
      "title": "Michael Henry Tessler",
      "tags": [
        "person"
      ],
      "content": "#person\n\nhttps://www.mit.edu/~tessler/"
    },
    {
      "id": "Rob Gordon",
      "title": "Rob Gordon",
      "tags": [
        "person"
      ],
      "content": "#person"
    },
    {
      "id": "Ben Day",
      "title": "Ben Day",
      "tags": [
        "person"
      ],
      "content": "#person"
    },
    {
      "id": "Pantheon",
      "title": "Pantheon",
      "tags": [
        "project"
      ],
      "content": "#project \n\nhttps://pantheon.chat/\n\nExperimental AI tool that reverses the typical human-AI interaction pattern by having LLM-powered \"daemons\" proactively respond to and comment on a user's stream of consciousness writing, rather than waiting for human prompts. The system allows users to write their thoughts in a diary-like format while customizable AI advisors offer questions, insights, and challenges designed to stimulate deeper thinking and explore new directions."
    },
    {
      "id": "Ran Haase",
      "title": "Ran Haase",
      "tags": [
        "person"
      ],
      "content": "#person"
    },
    {
      "id": "Saffron Huang",
      "title": "Saffron Huang",
      "tags": [
        "person"
      ],
      "content": "#person"
    },
    {
      "id": "AI & Democracy Foundation",
      "title": "AI & Democracy Foundation",
      "tags": [
        "organization"
      ],
      "content": "#organization \n\nhttps://aidemocracyfoundation.org/"
    },
    {
      "id": "Chen Yueh-Han",
      "title": "Chen Yueh-Han",
      "tags": [
        "person"
      ],
      "content": "#person \n\nhttps://john-chen.cc/"
    },
    {
      "id": "Taha Yasseri",
      "title": "Taha Yasseri",
      "tags": [
        "person"
      ],
      "content": "#person"
    },
    {
      "id": "Soham De",
      "title": "Soham De",
      "tags": [
        "person"
      ],
      "content": "#person"
    },
    {
      "id": "Beth Goldberg",
      "title": "Beth Goldberg",
      "tags": [
        "person"
      ],
      "content": "#person"
    },
    {
      "id": "Peter Darche",
      "title": "Peter Darche",
      "tags": [
        "person"
      ],
      "content": "#person"
    },
    {
      "id": "Matt Brooks",
      "title": "Matt Brooks",
      "tags": [
        "person"
      ],
      "content": "#person \n\nhttps://mattbrooks.xyz/"
    },
    {
      "id": "Ming Yin",
      "title": "Ming Yin",
      "tags": [
        "person"
      ],
      "content": "#person"
    }
  ],
  "links": [
    {
      "source": "Mapping the Discourse on AI Safety & Ethics",
      "target": "Değer Turan"
    },
    {
      "source": "Mapping the Discourse on AI Safety & Ethics",
      "target": "Colleen McKenzie"
    },
    {
      "source": "Mapping the Discourse on AI Safety & Ethics",
      "target": "Oliver Klingefjord"
    },
    {
      "source": "ACE - A LLM-based Negotiation Coaching System",
      "target": "Ryan Shea"
    },
    {
      "source": "ACE - A LLM-based Negotiation Coaching System",
      "target": "Aymen Kallala"
    },
    {
      "source": "ACE - A LLM-based Negotiation Coaching System",
      "target": "Xin Lucy Liu"
    },
    {
      "source": "ACE - A LLM-based Negotiation Coaching System",
      "target": "Michael W. Morris"
    },
    {
      "source": "ACE - A LLM-based Negotiation Coaching System",
      "target": "Zhou Yu"
    },
    {
      "source": "Amina Green",
      "target": "Plurality Institute"
    },
    {
      "source": "Maggie Appleton",
      "target": "Elicit"
    },
    {
      "source": "Maggie Appleton",
      "target": "Ought"
    },
    {
      "source": "Wargames for Peace",
      "target": "Nicky Case"
    },
    {
      "source": "Wargames for Peace",
      "target": "Eli Lifland"
    },
    {
      "source": "Wargames for Peace",
      "target": "Daniel Kokotajlo"
    },
    {
      "source": "Evelien Nieuwenburg",
      "target": "Dembrane"
    },
    {
      "source": "Enhancing AI-Assisted Group Decision Making through LLM-Powered Devil’s Advocate",
      "target": "Chun-Wei Chiang"
    },
    {
      "source": "Enhancing AI-Assisted Group Decision Making through LLM-Powered Devil’s Advocate",
      "target": "Zhuoran Lu"
    },
    {
      "source": "Enhancing AI-Assisted Group Decision Making through LLM-Powered Devil’s Advocate",
      "target": "Zhuoyan Li"
    },
    {
      "source": "Enhancing AI-Assisted Group Decision Making through LLM-Powered Devil’s Advocate",
      "target": "Ming Yin"
    },
    {
      "source": "Squiggle AI",
      "target": "AI Forecasting Benchmark"
    },
    {
      "source": "Squiggle AI",
      "target": "Ozzie Gooen"
    },
    {
      "source": "Colleen McKenzie",
      "target": "AI Objectives Institute"
    },
    {
      "source": "Reimagining Democracy for AI",
      "target": "Aviv Ovadya"
    },
    {
      "source": "Tom Davidson",
      "target": "Forethought"
    },
    {
      "source": "AI‐assisted scenario generation for strategic planning",
      "target": "Matthew Spaniol"
    },
    {
      "source": "AI‐assisted scenario generation for strategic planning",
      "target": "Nicholas Rowland"
    },
    {
      "source": "Truthful AI",
      "target": "Owain Evans"
    },
    {
      "source": "Truthful AI",
      "target": "Owen Cotton-Barratt"
    },
    {
      "source": "Truthful AI",
      "target": "Lukas Finnveden"
    },
    {
      "source": "Truthful AI",
      "target": "Adam Bales"
    },
    {
      "source": "Truthful AI",
      "target": "Avital Balwit"
    },
    {
      "source": "Truthful AI",
      "target": "Peter Wills"
    },
    {
      "source": "Truthful AI",
      "target": "Luca Righetti"
    },
    {
      "source": "Truthful AI",
      "target": "William Saunders"
    },
    {
      "source": "AI and the Future of Digital Public Squares",
      "target": "Beth Goldberg"
    },
    {
      "source": "AI and the Future of Digital Public Squares",
      "target": "Diana Acosta-Navas"
    },
    {
      "source": "AI and the Future of Digital Public Squares",
      "target": "Michiel Bakker"
    },
    {
      "source": "AI and the Future of Digital Public Squares",
      "target": "Ian Beacock"
    },
    {
      "source": "AI and the Future of Digital Public Squares",
      "target": "Matthew Botvinick"
    },
    {
      "source": "AI and the Future of Digital Public Squares",
      "target": "Prateek Buch"
    },
    {
      "source": "AI and the Future of Digital Public Squares",
      "target": "Renée DiResta"
    },
    {
      "source": "AI and the Future of Digital Public Squares",
      "target": "Nandika Donthi"
    },
    {
      "source": "AI and the Future of Digital Public Squares",
      "target": "Nathanael Fast"
    },
    {
      "source": "AI and the Future of Digital Public Squares",
      "target": "Ravi Iyer"
    },
    {
      "source": "AI and the Future of Digital Public Squares",
      "target": "Zaria Jalan"
    },
    {
      "source": "AI and the Future of Digital Public Squares",
      "target": "Andrew Konya"
    },
    {
      "source": "AI and the Future of Digital Public Squares",
      "target": "Grace Kwak Danciu"
    },
    {
      "source": "AI and the Future of Digital Public Squares",
      "target": "Hélène Landemore"
    },
    {
      "source": "AI and the Future of Digital Public Squares",
      "target": "Alice Marwick"
    },
    {
      "source": "AI and the Future of Digital Public Squares",
      "target": "Carl Miller"
    },
    {
      "source": "AI and the Future of Digital Public Squares",
      "target": "Aviv Ovadya"
    },
    {
      "source": "AI and the Future of Digital Public Squares",
      "target": "Emily Saltz"
    },
    {
      "source": "AI and the Future of Digital Public Squares",
      "target": "Lisa Schirch"
    },
    {
      "source": "AI and the Future of Digital Public Squares",
      "target": "Dalit Shalom"
    },
    {
      "source": "AI and the Future of Digital Public Squares",
      "target": "Divya Siddarth"
    },
    {
      "source": "AI and the Future of Digital Public Squares",
      "target": "Felix Sieker"
    },
    {
      "source": "AI and the Future of Digital Public Squares",
      "target": "Christopher Small"
    },
    {
      "source": "AI and the Future of Digital Public Squares",
      "target": "Jonathan Stray"
    },
    {
      "source": "AI and the Future of Digital Public Squares",
      "target": "Audrey Tang"
    },
    {
      "source": "AI and the Future of Digital Public Squares",
      "target": "Michael Henry Tessler"
    },
    {
      "source": "AI and the Future of Digital Public Squares",
      "target": "Amy Zhang"
    },
    {
      "source": "Daniel P. Hogan",
      "target": "IQT Labs"
    },
    {
      "source": "Recursive Public",
      "target": "vTaiwan & g0v"
    },
    {
      "source": "Recursive Public",
      "target": "AI Objectives Institute"
    },
    {
      "source": "Recursive Public",
      "target": "Flynn Devine"
    },
    {
      "source": "Recursive Public",
      "target": "Alex Krasodomski-Jones"
    },
    {
      "source": "Recursive Public",
      "target": "Carl Miller"
    },
    {
      "source": "Recursive Public",
      "target": "Shu Yang Lin"
    },
    {
      "source": "Recursive Public",
      "target": "Jia-Wei Cui"
    },
    {
      "source": "Recursive Public",
      "target": "Bruno Marnette"
    },
    {
      "source": "Recursive Public",
      "target": "Rowan Wilkinson"
    },
    {
      "source": "Snow Globe",
      "target": "Daniel P. Hogan"
    },
    {
      "source": "Snow Globe",
      "target": "Andrea Brennen"
    },
    {
      "source": "Owain Evans",
      "target": "Ought"
    },
    {
      "source": "Forecasting",
      "target": "Forecasting Future World Events with Neural Networks"
    },
    {
      "source": "Forecasting",
      "target": "Approaching Human-Level Forecasting with Language Models"
    },
    {
      "source": "Forecasting",
      "target": "ForecastBench - A Dynamic Benchmark of AI Forecasting Capabilities"
    },
    {
      "source": "Forecasting",
      "target": "AI-Augmented Predictions - LLM Assistants Improve Human Forecasting Accuracy"
    },
    {
      "source": "Forecasting",
      "target": "Wisdom of the silicon crowd - LLM ensemble prediction capabilities rival human crowd accuracy"
    },
    {
      "source": "Forecasting",
      "target": "Prompt Engineering Large Language Models’ Forecasting Capabilities"
    },
    {
      "source": "Forecasting",
      "target": "LLMs Can Teach Themselves to Better Predict the Future"
    },
    {
      "source": "Forecasting",
      "target": "Outcome-based Reinforcement Learning to Predict the Future"
    },
    {
      "source": "Forecasting",
      "target": "Foresight AI"
    },
    {
      "source": "Forecasting",
      "target": "AI Forecasting Benchmark"
    },
    {
      "source": "Forecasting",
      "target": "Squiggle AI"
    },
    {
      "source": "Forecasting",
      "target": "My Current Claims and Cruxes on LLM Forecasting & Epistemics"
    },
    {
      "source": "Forecasting",
      "target": "Metaforecast"
    },
    {
      "source": "Forecasting",
      "target": "Q1 AI Benchmarking results"
    },
    {
      "source": "Forecasting",
      "target": "Forecasting Tools"
    },
    {
      "source": "Forecasting",
      "target": "Bench to the Future - A Pastcasting Benchmark for Forecasting Agents"
    },
    {
      "source": "Forecasting",
      "target": "Human v Bots Forecasting Tournament 2024"
    },
    {
      "source": "Forecasting",
      "target": "Contra papers claiming superhuman AI forecasting"
    },
    {
      "source": "Oliver Klingefjord",
      "target": "Meaning Alignment Institute"
    },
    {
      "source": "Scaffolding cooperation in human groups with deep reinforcement learning",
      "target": "Kevin McKee"
    },
    {
      "source": "Scaffolding cooperation in human groups with deep reinforcement learning",
      "target": "Andrea Tacchetti"
    },
    {
      "source": "Scaffolding cooperation in human groups with deep reinforcement learning",
      "target": "Michiel Bakker"
    },
    {
      "source": "Scaffolding cooperation in human groups with deep reinforcement learning",
      "target": "Jan Balaguer"
    },
    {
      "source": "Scaffolding cooperation in human groups with deep reinforcement learning",
      "target": "Lucy Campbell-Gillingham"
    },
    {
      "source": "Scaffolding cooperation in human groups with deep reinforcement learning",
      "target": "Richard Everett"
    },
    {
      "source": "Scaffolding cooperation in human groups with deep reinforcement learning",
      "target": "Matthew Botvinick"
    },
    {
      "source": "Language Agents as Digital Representatives in Collective Decision-Making",
      "target": "Daniel Jarrett"
    },
    {
      "source": "Language Agents as Digital Representatives in Collective Decision-Making",
      "target": "Michael Henry Tessler"
    },
    {
      "source": "Language Agents as Digital Representatives in Collective Decision-Making",
      "target": "Romuald Elie"
    },
    {
      "source": "Language Agents as Digital Representatives in Collective Decision-Making",
      "target": "Miruna Pîslar"
    },
    {
      "source": "Language Agents as Digital Representatives in Collective Decision-Making",
      "target": "Raphael Köster"
    },
    {
      "source": "Language Agents as Digital Representatives in Collective Decision-Making",
      "target": "Christopher Summerfield"
    },
    {
      "source": "Language Agents as Digital Representatives in Collective Decision-Making",
      "target": "Michiel Bakker"
    },
    {
      "source": "Language Agents as Digital Representatives in Collective Decision-Making",
      "target": "Jan Balaguer"
    },
    {
      "source": "Language Agents as Digital Representatives in Collective Decision-Making",
      "target": "Andrea Tacchetti"
    },
    {
      "source": "Luke Thorburn",
      "target": "AI & Democracy Foundation"
    },
    {
      "source": "Value Elicitation",
      "target": "Accelerated Preference Elicitation with LLM-Based Proxies"
    },
    {
      "source": "Value Elicitation",
      "target": "LLM-Powered Preference Elicitation in Combinatorial Assignment"
    },
    {
      "source": "Value Elicitation",
      "target": "Democratic Fine-Tuning"
    },
    {
      "source": "Value Elicitation",
      "target": "What are human values,and how do we align AI to them?"
    },
    {
      "source": "Value Elicitation",
      "target": "Israel-Palestine Collective Dialogues"
    },
    {
      "source": "Value Elicitation",
      "target": "Opportunities and Risks of LLMs for Scalable Deliberation with Polis"
    },
    {
      "source": "Foresight AI",
      "target": "AI Forecasting Benchmark"
    },
    {
      "source": "Foresight AI",
      "target": "Lightning Rod Labs"
    },
    {
      "source": "Double Crux Bot",
      "target": "Sofi Vanhanen"
    },
    {
      "source": "Double Crux Bot",
      "target": "Santeri Koivula"
    },
    {
      "source": "Double Crux Bot",
      "target": "Sarah Bluhm"
    },
    {
      "source": "Emily Saltz",
      "target": "Google Jigsaw"
    },
    {
      "source": "Julien Cornebise",
      "target": "The Computational Democracy Project"
    },
    {
      "source": "Danny Fanklin",
      "target": "Lightning Rod Labs"
    },
    {
      "source": "Language Models as Critical Thinking Tools - A Case Study of Philosophers",
      "target": "Andre Ye"
    },
    {
      "source": "Language Models as Critical Thinking Tools - A Case Study of Philosophers",
      "target": "Jared Moore"
    },
    {
      "source": "Language Models as Critical Thinking Tools - A Case Study of Philosophers",
      "target": "Rose Novick"
    },
    {
      "source": "Language Models as Critical Thinking Tools - A Case Study of Philosophers",
      "target": "Amy Zhang"
    },
    {
      "source": "Talk to the City",
      "target": "AI Objectives Institute"
    },
    {
      "source": "My Current Claims and Cruxes on LLM Forecasting & Epistemics",
      "target": "Ozzie Gooen"
    },
    {
      "source": "Niki Dupuis",
      "target": "Mosaic Labs"
    },
    {
      "source": "AI Enhanced Reasoning - Augmenting Human Critical Thinking With AI Systems",
      "target": "Valdemar Danry"
    },
    {
      "source": "Online Epistemics",
      "target": "Lucid Lens"
    },
    {
      "source": "Online Epistemics",
      "target": "Supernotes"
    },
    {
      "source": "Online Epistemics",
      "target": "Bridging Bot"
    },
    {
      "source": "Online Epistemics",
      "target": "$SL Muses"
    },
    {
      "source": "Online Epistemics",
      "target": "Artifact"
    },
    {
      "source": "Online Epistemics",
      "target": "Deceptive Explanations by Large Language Models Lead People to Change their Beliefs About Misinformation More Often than Honest Explanations"
    },
    {
      "source": "Online Epistemics",
      "target": "Making Artificial Intelligence Work for Investigative Journalism"
    },
    {
      "source": "Modelling Political Coalition Negotiations Using LLM-based Agents",
      "target": "Farhad Moghimifar"
    },
    {
      "source": "Modelling Political Coalition Negotiations Using LLM-based Agents",
      "target": "Yuan-Fang Li"
    },
    {
      "source": "Modelling Political Coalition Negotiations Using LLM-based Agents",
      "target": "Robert Thomson"
    },
    {
      "source": "Modelling Political Coalition Negotiations Using LLM-based Agents",
      "target": "Gholamreza Haffari"
    },
    {
      "source": "Matchmaking",
      "target": "Boardy"
    },
    {
      "source": "Matchmaking",
      "target": "Series"
    },
    {
      "source": "Approaching Human-Level Forecasting with Language Models",
      "target": "Danny Halawi"
    },
    {
      "source": "Approaching Human-Level Forecasting with Language Models",
      "target": "Fred Zhang"
    },
    {
      "source": "Approaching Human-Level Forecasting with Language Models",
      "target": "Chen Yueh-Han"
    },
    {
      "source": "Approaching Human-Level Forecasting with Language Models",
      "target": "Jacob Steinhardt"
    },
    {
      "source": "Alice Siu",
      "target": "Deliberative Democracy Lab"
    },
    {
      "source": "Kris Skotheim",
      "target": "Lightning Rod Labs"
    },
    {
      "source": "Audrey Tang",
      "target": "Collective Intelligence Project"
    },
    {
      "source": "Audrey Tang",
      "target": "Plurality Institute"
    },
    {
      "source": "Audrey Tang",
      "target": "vTaiwan & g0v"
    },
    {
      "source": "Bridging Bot",
      "target": "Google Jigsaw"
    },
    {
      "source": "Bridging Bot",
      "target": "Jeff Fossett"
    },
    {
      "source": "Bridging Bot",
      "target": "Amina Green"
    },
    {
      "source": "Bridging Bot",
      "target": "Ian Baker"
    },
    {
      "source": "Bridging Bot",
      "target": "Peter Darche"
    },
    {
      "source": "Philosophy & Morality",
      "target": "AI doing philosophy = AI generating hands?"
    },
    {
      "source": "Philosophy & Morality",
      "target": "Creating a large language model of a philosopher"
    },
    {
      "source": "Philosophy & Morality",
      "target": "Language Models as Critical Thinking Tools - A Case Study of Philosophers"
    },
    {
      "source": "Philosophy & Morality",
      "target": "What are human values,and how do we align AI to them?"
    },
    {
      "source": "Philosophy & Morality",
      "target": "Democratic Fine-Tuning"
    },
    {
      "source": "Philosophy & Morality",
      "target": "Rethinking Machine Ethics –Can LLMs Perform Moral Reasoning through the Lens of Moral Theories?"
    },
    {
      "source": "Jia-Wei Cui",
      "target": "vTaiwan & g0v"
    },
    {
      "source": "LLMs as Research Tools -  A Large Scale Survey of Researchers’ Usage and Perceptions",
      "target": "Zhehui Liao"
    },
    {
      "source": "LLMs as Research Tools -  A Large Scale Survey of Researchers’ Usage and Perceptions",
      "target": "Maria Antoniak"
    },
    {
      "source": "LLMs as Research Tools -  A Large Scale Survey of Researchers’ Usage and Perceptions",
      "target": "Inyoung Cheong"
    },
    {
      "source": "LLMs as Research Tools -  A Large Scale Survey of Researchers’ Usage and Perceptions",
      "target": "Evie Yu-Yen Cheng"
    },
    {
      "source": "LLMs as Research Tools -  A Large Scale Survey of Researchers’ Usage and Perceptions",
      "target": "Ai-Heng Lee"
    },
    {
      "source": "LLMs as Research Tools -  A Large Scale Survey of Researchers’ Usage and Perceptions",
      "target": "Kyle Lo"
    },
    {
      "source": "LLMs as Research Tools -  A Large Scale Survey of Researchers’ Usage and Perceptions",
      "target": "Joseph Chee Chang"
    },
    {
      "source": "LLMs as Research Tools -  A Large Scale Survey of Researchers’ Usage and Perceptions",
      "target": "Amy Zhang"
    },
    {
      "source": "Mediation & Negotiation",
      "target": "LLMediator"
    },
    {
      "source": "Mediation & Negotiation",
      "target": "Bot Mediation"
    },
    {
      "source": "Mediation & Negotiation",
      "target": "ACE - A LLM-based Negotiation Coaching System"
    },
    {
      "source": "Mediation & Negotiation",
      "target": "Multi-Agent Consensus Seeking via Large Language Models"
    },
    {
      "source": "Mediation & Negotiation",
      "target": "Modelling Political Coalition Negotiations Using LLM-based Agents"
    },
    {
      "source": "Mediation & Negotiation",
      "target": "Double Crux Bot"
    },
    {
      "source": "Applied Rationality",
      "target": "Linters for Thought"
    },
    {
      "source": "Applied Rationality",
      "target": "AI Enhanced Reasoning - Augmenting Human Critical Thinking With AI Systems"
    },
    {
      "source": "Applied Rationality",
      "target": "Enhancing AI-Assisted Group Decision Making through LLM-Powered Devil’s Advocate"
    },
    {
      "source": "Future Search",
      "target": "AI Forecasting Benchmark"
    },
    {
      "source": "Ben Wilson",
      "target": "Metaculus"
    },
    {
      "source": "Ben Wilson",
      "target": "The Society Library"
    },
    {
      "source": "Human v Bots Forecasting Tournament 2024",
      "target": "Future Search"
    },
    {
      "source": "Human v Bots Forecasting Tournament 2024",
      "target": "Manifold Markets"
    },
    {
      "source": "Democratic Policy Development using Collective Dialogues and AI",
      "target": "Andrew Konya"
    },
    {
      "source": "Democratic Policy Development using Collective Dialogues and AI",
      "target": "Lisa Schirch"
    },
    {
      "source": "Democratic Policy Development using Collective Dialogues and AI",
      "target": "Colin Irwin"
    },
    {
      "source": "Democratic Policy Development using Collective Dialogues and AI",
      "target": "Aviv Ovadya"
    },
    {
      "source": "Değer Turan",
      "target": "Metaculus"
    },
    {
      "source": "Değer Turan",
      "target": "AI Objectives Institute"
    },
    {
      "source": "Owen Cotton-Barratt",
      "target": "Ought"
    },
    {
      "source": "AI-Augmented Predictions - LLM Assistants Improve Human Forecasting Accuracy",
      "target": "Philipp Schoenegger"
    },
    {
      "source": "AI-Augmented Predictions - LLM Assistants Improve Human Forecasting Accuracy",
      "target": "Peter Park"
    },
    {
      "source": "AI-Augmented Predictions - LLM Assistants Improve Human Forecasting Accuracy",
      "target": "Ezra Karger"
    },
    {
      "source": "AI-Augmented Predictions - LLM Assistants Improve Human Forecasting Accuracy",
      "target": "Sean Trott"
    },
    {
      "source": "AI-Augmented Predictions - LLM Assistants Improve Human Forecasting Accuracy",
      "target": "Philip Tetlock"
    },
    {
      "source": "Jeff Fossett",
      "target": "Plurality Institute"
    },
    {
      "source": "AI Debate Maps",
      "target": "Debate Maps"
    },
    {
      "source": "Human vs. Machine -  Behavioral Differences between Expert Humans and Language Models in Wargame Simulations",
      "target": "Max Lamparth"
    },
    {
      "source": "Human vs. Machine -  Behavioral Differences between Expert Humans and Language Models in Wargame Simulations",
      "target": "Anthony Corso"
    },
    {
      "source": "Human vs. Machine -  Behavioral Differences between Expert Humans and Language Models in Wargame Simulations",
      "target": "Jacob Ganz"
    },
    {
      "source": "Human vs. Machine -  Behavioral Differences between Expert Humans and Language Models in Wargame Simulations",
      "target": "Oriana Skylar Mastro"
    },
    {
      "source": "Human vs. Machine -  Behavioral Differences between Expert Humans and Language Models in Wargame Simulations",
      "target": "Jacquelyn Schneider"
    },
    {
      "source": "Human vs. Machine -  Behavioral Differences between Expert Humans and Language Models in Wargame Simulations",
      "target": "Harold Trinkunas"
    },
    {
      "source": "Hadjar Homaei",
      "target": "The Computational Democracy Project"
    },
    {
      "source": "Information Market Mechanisms",
      "target": "Language models can reduce asymmetry in information markets"
    },
    {
      "source": "Information Market Mechanisms",
      "target": "Tell Me Why - Incentivizing Explanations"
    },
    {
      "source": "Philipp Schoenegger",
      "target": "Metaculus"
    },
    {
      "source": "Linting Adjacent",
      "target": "Linters for Thought"
    },
    {
      "source": "Linting Adjacent",
      "target": "Pantheon"
    },
    {
      "source": "Linting Adjacent",
      "target": "Roast My Post"
    },
    {
      "source": "Nuño Sempere",
      "target": "Sentinel"
    },
    {
      "source": "Sparse Autoencoders for Hypothesis Generation",
      "target": "Rajiv Movva"
    },
    {
      "source": "Sparse Autoencoders for Hypothesis Generation",
      "target": "Kenny Peng"
    },
    {
      "source": "Sparse Autoencoders for Hypothesis Generation",
      "target": "Nikhil Garg"
    },
    {
      "source": "Sparse Autoencoders for Hypothesis Generation",
      "target": "Jon Kleinberg"
    },
    {
      "source": "Sparse Autoencoders for Hypothesis Generation",
      "target": "Emma Pierson"
    },
    {
      "source": "Bruno Marnette",
      "target": "AI Objectives Institute"
    },
    {
      "source": "Ben Rachbach",
      "target": "Elicit"
    },
    {
      "source": "Ben Rachbach",
      "target": "Ought"
    },
    {
      "source": "Ian Beacock",
      "target": "Google Jigsaw"
    },
    {
      "source": "AI Safety Feed",
      "target": "Matt Brooks"
    },
    {
      "source": "Strategic Epistemic Defense",
      "target": "How Malicious AI Swarms Can Threaten Democracy"
    },
    {
      "source": "Strategic Epistemic Defense",
      "target": "Let’s use AI to harden human defenses against AI manipulation"
    },
    {
      "source": "Interface Design",
      "target": "Why Chatbots Are Not the Future"
    },
    {
      "source": "Interface Design",
      "target": "Amplifying transformative potential while designing augmented deliberative systems"
    },
    {
      "source": "Factored Cognition Primer",
      "target": "Ought"
    },
    {
      "source": "Generative Social Choice",
      "target": "Sara Fish"
    },
    {
      "source": "Generative Social Choice",
      "target": "Paul Gölz"
    },
    {
      "source": "Generative Social Choice",
      "target": "David Parkes"
    },
    {
      "source": "Generative Social Choice",
      "target": "Ariel Procaccia"
    },
    {
      "source": "Generative Social Choice",
      "target": "Gili Rusak"
    },
    {
      "source": "Generative Social Choice",
      "target": "Itai Shapira"
    },
    {
      "source": "Generative Social Choice",
      "target": "Manuel Wüthrich"
    },
    {
      "source": "Amplifying transformative potential while designing augmented deliberative systems",
      "target": "Shu Yang Lin"
    },
    {
      "source": "Creating a large language model of a philosopher",
      "target": "Eric Schwitzgebel"
    },
    {
      "source": "Creating a large language model of a philosopher",
      "target": "David Schwitzgebel"
    },
    {
      "source": "Creating a large language model of a philosopher",
      "target": "Anna Strasser"
    },
    {
      "source": "What's Important in \"AI for Epistemics\"?",
      "target": "Lukas Finnveden"
    },
    {
      "source": "Making Artificial Intelligence Work for Investigative Journalism",
      "target": "Jonathan Stray"
    },
    {
      "source": "Ezra Karger",
      "target": "Forecasting Research Institute"
    },
    {
      "source": "LLMediator",
      "target": "Hannes Westermann"
    },
    {
      "source": "LLMediator",
      "target": "Jaromir Savelka"
    },
    {
      "source": "LLMediator",
      "target": "Karim Benyekhlef"
    },
    {
      "source": "Daniel Kokotajlo",
      "target": "AI Future Project"
    },
    {
      "source": "Tell Me Why - Incentivizing Explanations",
      "target": "Siddarth Srinivasan"
    },
    {
      "source": "Tell Me Why - Incentivizing Explanations",
      "target": "Ezra Karger"
    },
    {
      "source": "Tell Me Why - Incentivizing Explanations",
      "target": "Michiel Bakker"
    },
    {
      "source": "Tell Me Why - Incentivizing Explanations",
      "target": "Yiling Chen"
    },
    {
      "source": "Amelia Wattenberger",
      "target": "GitHub Next"
    },
    {
      "source": "Metaforecast",
      "target": "Nuño Sempere"
    },
    {
      "source": "Metaforecast",
      "target": "Ozzie Gooen"
    },
    {
      "source": "What are human values,and how do we align AI to them?",
      "target": "Oliver Klingefjord"
    },
    {
      "source": "What are human values,and how do we align AI to them?",
      "target": "Ryan Lowe"
    },
    {
      "source": "What are human values,and how do we align AI to them?",
      "target": "Joe Edelman"
    },
    {
      "source": "Escalation Risks from Language Models in Military and Diplomatic Decision-Making",
      "target": "Juan-Pablo Rivera"
    },
    {
      "source": "Escalation Risks from Language Models in Military and Diplomatic Decision-Making",
      "target": "Gabriel Mukobi"
    },
    {
      "source": "Escalation Risks from Language Models in Military and Diplomatic Decision-Making",
      "target": "Anka Reuel"
    },
    {
      "source": "Escalation Risks from Language Models in Military and Diplomatic Decision-Making",
      "target": "Max Lamparth"
    },
    {
      "source": "Escalation Risks from Language Models in Military and Diplomatic Decision-Making",
      "target": "Chandler Smith"
    },
    {
      "source": "Escalation Risks from Language Models in Military and Diplomatic Decision-Making",
      "target": "Jacquelyn Schneider"
    },
    {
      "source": "Robert Gambee",
      "target": "Future Search"
    },
    {
      "source": "Valdemar Danry",
      "target": "MIT Media Lab"
    },
    {
      "source": "Contra papers claiming superhuman AI forecasting",
      "target": "Nikos Bosse"
    },
    {
      "source": "Contra papers claiming superhuman AI forecasting",
      "target": "Peter Mühlbacher"
    },
    {
      "source": "Contra papers claiming superhuman AI forecasting",
      "target": "Lawrence Phillips"
    },
    {
      "source": "Contra papers claiming superhuman AI forecasting",
      "target": "Dan Schwarz"
    },
    {
      "source": "Forecasting Future World Events with Neural Networks",
      "target": "Andy Zou"
    },
    {
      "source": "Forecasting Future World Events with Neural Networks",
      "target": "Tristan Xiao"
    },
    {
      "source": "Forecasting Future World Events with Neural Networks",
      "target": "Ryan Jia"
    },
    {
      "source": "Forecasting Future World Events with Neural Networks",
      "target": "Joe Kwon"
    },
    {
      "source": "Forecasting Future World Events with Neural Networks",
      "target": "Mantas Mazeika"
    },
    {
      "source": "Forecasting Future World Events with Neural Networks",
      "target": "Richard Li"
    },
    {
      "source": "Forecasting Future World Events with Neural Networks",
      "target": "Dawn Song"
    },
    {
      "source": "Forecasting Future World Events with Neural Networks",
      "target": "Jacob Steinhardt"
    },
    {
      "source": "Forecasting Future World Events with Neural Networks",
      "target": "Owain Evans"
    },
    {
      "source": "Forecasting Future World Events with Neural Networks",
      "target": "Dan Hendrycks"
    },
    {
      "source": "ForecastBench - A Dynamic Benchmark of AI Forecasting Capabilities",
      "target": "Ezra Karger"
    },
    {
      "source": "ForecastBench - A Dynamic Benchmark of AI Forecasting Capabilities",
      "target": "Houtan Bastani"
    },
    {
      "source": "ForecastBench - A Dynamic Benchmark of AI Forecasting Capabilities",
      "target": "Chen Yueh-Han"
    },
    {
      "source": "ForecastBench - A Dynamic Benchmark of AI Forecasting Capabilities",
      "target": "Zachary Jacobs"
    },
    {
      "source": "ForecastBench - A Dynamic Benchmark of AI Forecasting Capabilities",
      "target": "Danny Halawi"
    },
    {
      "source": "ForecastBench - A Dynamic Benchmark of AI Forecasting Capabilities",
      "target": "Fred Zhang"
    },
    {
      "source": "ForecastBench - A Dynamic Benchmark of AI Forecasting Capabilities",
      "target": "Philip Tetlock"
    },
    {
      "source": "Sofi Vanhanen",
      "target": "Mosaic Labs"
    },
    {
      "source": "AI Forecasting Benchmark",
      "target": "Metaculus"
    },
    {
      "source": "AI doing philosophy = AI generating hands?",
      "target": "Wei Dai"
    },
    {
      "source": "Why Chatbots Are Not the Future",
      "target": "Amelia Wattenberger"
    },
    {
      "source": "Andrea Brennen",
      "target": "IQT Labs"
    },
    {
      "source": "Habermas Machine",
      "target": "Michael Henry Tessler"
    },
    {
      "source": "Habermas Machine",
      "target": "Michiel Bakker"
    },
    {
      "source": "Habermas Machine",
      "target": "Daniel Jarrett"
    },
    {
      "source": "Habermas Machine",
      "target": "Hannah Sheahan"
    },
    {
      "source": "Habermas Machine",
      "target": "Martin Chadwick"
    },
    {
      "source": "Habermas Machine",
      "target": "Raphael Köster"
    },
    {
      "source": "Habermas Machine",
      "target": "Georgina Evans"
    },
    {
      "source": "Habermas Machine",
      "target": "Lucy Campbell-Gillingham"
    },
    {
      "source": "Habermas Machine",
      "target": "Tantum Collins"
    },
    {
      "source": "Habermas Machine",
      "target": "David Parkes"
    },
    {
      "source": "Habermas Machine",
      "target": "Matthew Botvinick"
    },
    {
      "source": "Habermas Machine",
      "target": "Christopher Summerfield"
    },
    {
      "source": "Justin Reppert",
      "target": "Elicit"
    },
    {
      "source": "Justin Reppert",
      "target": "Ought"
    },
    {
      "source": "Jack Wildman",
      "target": "Future Search"
    },
    {
      "source": "Voting",
      "target": "Generative Social Choice"
    },
    {
      "source": "Voting",
      "target": "Generative AI Voting - Fair Collective Choice is Resilient to LLM Biases and Inconsistencies"
    },
    {
      "source": "Debate Maps",
      "target": "The Society Library"
    },
    {
      "source": "Outcome-based Reinforcement Learning to Predict the Future",
      "target": "Ben Turtel"
    },
    {
      "source": "Outcome-based Reinforcement Learning to Predict the Future",
      "target": "Danny Fanklin"
    },
    {
      "source": "Outcome-based Reinforcement Learning to Predict the Future",
      "target": "Philipp Schoenegger"
    },
    {
      "source": "Outcome-based Reinforcement Learning to Predict the Future",
      "target": "Kris Skotheim"
    },
    {
      "source": "Outcome-based Reinforcement Learning to Predict the Future",
      "target": "Luke Hewitt"
    },
    {
      "source": "Molly Hickman",
      "target": "Forecasting Research Institute"
    },
    {
      "source": "Molly Hickman",
      "target": "Metaculus"
    },
    {
      "source": "Bot Mediation",
      "target": "Curtis Holdsworth"
    },
    {
      "source": "Bot Mediation",
      "target": "Darren Fancher"
    },
    {
      "source": "Bot Mediation",
      "target": "Nick Hikita"
    },
    {
      "source": "Linters for Thought",
      "target": "Joshua Levy"
    },
    {
      "source": "Lizka Vaintrob",
      "target": "Forethought"
    },
    {
      "source": "Fine-tuning language models to find agreement among humans with diverse preferences",
      "target": "Michiel Bakker"
    },
    {
      "source": "Fine-tuning language models to find agreement among humans with diverse preferences",
      "target": "Martin Chadwick"
    },
    {
      "source": "Fine-tuning language models to find agreement among humans with diverse preferences",
      "target": "Hannah Sheahan"
    },
    {
      "source": "Fine-tuning language models to find agreement among humans with diverse preferences",
      "target": "Michael Henry Tessler"
    },
    {
      "source": "Fine-tuning language models to find agreement among humans with diverse preferences",
      "target": "Lucy Campbell-Gillingham"
    },
    {
      "source": "Fine-tuning language models to find agreement among humans with diverse preferences",
      "target": "Jan Balaguer"
    },
    {
      "source": "Fine-tuning language models to find agreement among humans with diverse preferences",
      "target": "Nat McAleese"
    },
    {
      "source": "Fine-tuning language models to find agreement among humans with diverse preferences",
      "target": "Amelia Glaese"
    },
    {
      "source": "Fine-tuning language models to find agreement among humans with diverse preferences",
      "target": "John Aslanides"
    },
    {
      "source": "Fine-tuning language models to find agreement among humans with diverse preferences",
      "target": "Matthew Botvinick"
    },
    {
      "source": "Fine-tuning language models to find agreement among humans with diverse preferences",
      "target": "Christopher Summerfield"
    },
    {
      "source": "Nathan Young",
      "target": "Goodheart Labs"
    },
    {
      "source": "Ivan Vendrov",
      "target": "Midjourney"
    },
    {
      "source": "Can AI bring deliberative democracy to the masses?",
      "target": "Hélène Landemore"
    },
    {
      "source": "Colin Megill",
      "target": "The Computational Democracy Project"
    },
    {
      "source": "John Bash",
      "target": "Metaculus"
    },
    {
      "source": "Andreas Stuhlmüller",
      "target": "Elicit"
    },
    {
      "source": "Andreas Stuhlmüller",
      "target": "Ought"
    },
    {
      "source": "The AI Adoption Gap - Preparing the US Government for Advanced AI",
      "target": "Lizka Vaintrob"
    },
    {
      "source": "Zachary Jacobs",
      "target": "Forecasting Research Institute"
    },
    {
      "source": "Rethinking Machine Ethics –Can LLMs Perform Moral Reasoning through the Lens of Moral Theories?",
      "target": "Jingyan Zhou"
    },
    {
      "source": "Rethinking Machine Ethics –Can LLMs Perform Moral Reasoning through the Lens of Moral Theories?",
      "target": "Minda Hu"
    },
    {
      "source": "Rethinking Machine Ethics –Can LLMs Perform Moral Reasoning through the Lens of Moral Theories?",
      "target": "Junan Li"
    },
    {
      "source": "Rethinking Machine Ethics –Can LLMs Perform Moral Reasoning through the Lens of Moral Theories?",
      "target": "Xiaoying Zhang"
    },
    {
      "source": "Rethinking Machine Ethics –Can LLMs Perform Moral Reasoning through the Lens of Moral Theories?",
      "target": "Xixin Wu"
    },
    {
      "source": "Rethinking Machine Ethics –Can LLMs Perform Moral Reasoning through the Lens of Moral Theories?",
      "target": "Irwin King"
    },
    {
      "source": "Rethinking Machine Ethics –Can LLMs Perform Moral Reasoning through the Lens of Moral Theories?",
      "target": "Helen Meng"
    },
    {
      "source": "Goodheart Labs",
      "target": "AI Forecasting Benchmark"
    },
    {
      "source": "Artifact",
      "target": "Kevin Systrom"
    },
    {
      "source": "Artifact",
      "target": "Mike Krieger"
    },
    {
      "source": "Wearable Reasoner",
      "target": "Valdemar Danry"
    },
    {
      "source": "Wearable Reasoner",
      "target": "Pat Pataranutaporn"
    },
    {
      "source": "Wearable Reasoner",
      "target": "Yaoli Mao"
    },
    {
      "source": "Wearable Reasoner",
      "target": "Pattie Maes"
    },
    {
      "source": "Eli Lifland",
      "target": "AI Future Project"
    },
    {
      "source": "Eli Lifland",
      "target": "Ought"
    },
    {
      "source": "Brendan Fong",
      "target": "Topos Institute"
    },
    {
      "source": "Iterated Decomposition - Improving Science Q&A by Supervising Reasoning Processes",
      "target": "Justin Reppert"
    },
    {
      "source": "Iterated Decomposition - Improving Science Q&A by Supervising Reasoning Processes",
      "target": "Ben Rachbach"
    },
    {
      "source": "Iterated Decomposition - Improving Science Q&A by Supervising Reasoning Processes",
      "target": "Charlie George"
    },
    {
      "source": "Iterated Decomposition - Improving Science Q&A by Supervising Reasoning Processes",
      "target": "Luke Stebbing"
    },
    {
      "source": "Iterated Decomposition - Improving Science Q&A by Supervising Reasoning Processes",
      "target": "Jungwon Byun"
    },
    {
      "source": "Iterated Decomposition - Improving Science Q&A by Supervising Reasoning Processes",
      "target": "Maggie Appleton"
    },
    {
      "source": "Iterated Decomposition - Improving Science Q&A by Supervising Reasoning Processes",
      "target": "Andreas Stuhlmüller"
    },
    {
      "source": "Deceptive Explanations by Large Language Models Lead People to Change their Beliefs About Misinformation More Often than Honest Explanations",
      "target": "Valdemar Danry"
    },
    {
      "source": "Deceptive Explanations by Large Language Models Lead People to Change their Beliefs About Misinformation More Often than Honest Explanations",
      "target": "Pat Pataranutaporn"
    },
    {
      "source": "Deceptive Explanations by Large Language Models Lead People to Change their Beliefs About Misinformation More Often than Honest Explanations",
      "target": "Matthew Groh"
    },
    {
      "source": "Deceptive Explanations by Large Language Models Lead People to Change their Beliefs About Misinformation More Often than Honest Explanations",
      "target": "Ziv Epstein"
    },
    {
      "source": "Peter Mühlbacher",
      "target": "Future Search"
    },
    {
      "source": "Lawrence Phillips",
      "target": "Future Search"
    },
    {
      "source": "Shu Yang Lin",
      "target": "AI Objectives Institute"
    },
    {
      "source": "Shu Yang Lin",
      "target": "vTaiwan & g0v"
    },
    {
      "source": "Grim",
      "target": "Rai Sur"
    },
    {
      "source": "Grim",
      "target": "Nuño Sempere"
    },
    {
      "source": "Nikos Bosse",
      "target": "Future Search"
    },
    {
      "source": "Jacquelyn Schneider",
      "target": "Hoover Wargaming and Crisis Simulation Initiative"
    },
    {
      "source": "Jamie Joyce",
      "target": "The Society Library"
    },
    {
      "source": "$SL Muses",
      "target": "The Society Library"
    },
    {
      "source": "Lukas Finnveden",
      "target": "Forethought"
    },
    {
      "source": "Lukas Finnveden",
      "target": "Ought"
    },
    {
      "source": "Israel-Palestine Collective Dialogues",
      "target": "Andrew Konya"
    },
    {
      "source": "Israel-Palestine Collective Dialogues",
      "target": "Luke Thorburn"
    },
    {
      "source": "Israel-Palestine Collective Dialogues",
      "target": "Wasim Almasri"
    },
    {
      "source": "Israel-Palestine Collective Dialogues",
      "target": "Oded Adomi Leshem"
    },
    {
      "source": "Israel-Palestine Collective Dialogues",
      "target": "Ariel Procaccia"
    },
    {
      "source": "Israel-Palestine Collective Dialogues",
      "target": "Lisa Schirch"
    },
    {
      "source": "Israel-Palestine Collective Dialogues",
      "target": "Michiel Bakker"
    },
    {
      "source": "Opportunities and Risks of LLMs for Scalable Deliberation with Polis",
      "target": "Christopher Small"
    },
    {
      "source": "Opportunities and Risks of LLMs for Scalable Deliberation with Polis",
      "target": "Ivan Vendrov"
    },
    {
      "source": "Opportunities and Risks of LLMs for Scalable Deliberation with Polis",
      "target": "Esin Durmus"
    },
    {
      "source": "Opportunities and Risks of LLMs for Scalable Deliberation with Polis",
      "target": "Hadjar Homaei"
    },
    {
      "source": "Opportunities and Risks of LLMs for Scalable Deliberation with Polis",
      "target": "Elizabeth Barry"
    },
    {
      "source": "Opportunities and Risks of LLMs for Scalable Deliberation with Polis",
      "target": "Julien Cornebise"
    },
    {
      "source": "Opportunities and Risks of LLMs for Scalable Deliberation with Polis",
      "target": "Ted Suzman"
    },
    {
      "source": "Opportunities and Risks of LLMs for Scalable Deliberation with Polis",
      "target": "Deep Ganguli"
    },
    {
      "source": "Opportunities and Risks of LLMs for Scalable Deliberation with Polis",
      "target": "Colin Megill"
    },
    {
      "source": "Andrew Konya",
      "target": "Remesh"
    },
    {
      "source": "Andrew Konya",
      "target": "AI & Democracy Foundation"
    },
    {
      "source": "AI Epistemics",
      "target": "Truthful AI"
    },
    {
      "source": "Joe Edelman",
      "target": "Meaning Alignment Institute"
    },
    {
      "source": "Jacob Ganz",
      "target": "Hoover Wargaming and Crisis Simulation Initiative"
    },
    {
      "source": "Mantic",
      "target": "AI Forecasting Benchmark"
    },
    {
      "source": "Wisdom of the silicon crowd - LLM ensemble prediction capabilities rival human crowd accuracy",
      "target": "Philipp Schoenegger"
    },
    {
      "source": "Wisdom of the silicon crowd - LLM ensemble prediction capabilities rival human crowd accuracy",
      "target": "Indre Tuminauskaite"
    },
    {
      "source": "Wisdom of the silicon crowd - LLM ensemble prediction capabilities rival human crowd accuracy",
      "target": "Peter Park"
    },
    {
      "source": "Wisdom of the silicon crowd - LLM ensemble prediction capabilities rival human crowd accuracy",
      "target": "Rafael Valdece Sousa Bastos"
    },
    {
      "source": "Wisdom of the silicon crowd - LLM ensemble prediction capabilities rival human crowd accuracy",
      "target": "Philip Tetlock"
    },
    {
      "source": "Aviv Ovadya",
      "target": "AI & Democracy Foundation"
    },
    {
      "source": "Let’s use AI to harden human defenses against AI manipulation",
      "target": "Tom Davidson"
    },
    {
      "source": "Mapping AI Discourse",
      "target": "Recursive Public"
    },
    {
      "source": "Mapping AI Discourse",
      "target": "Global Dialogues"
    },
    {
      "source": "Mapping AI Discourse",
      "target": "Mapping the Discourse on AI Safety & Ethics"
    },
    {
      "source": "Mapping AI Discourse",
      "target": "AI Debate Maps"
    },
    {
      "source": "Finn Hambly",
      "target": "Future Search"
    },
    {
      "source": "Christopher Small",
      "target": "Google Jigsaw"
    },
    {
      "source": "Christopher Small",
      "target": "The Computational Democracy Project"
    },
    {
      "source": "Lisa Schirch",
      "target": "Plurality Institute"
    },
    {
      "source": "Sensemaker",
      "target": "Google Jigsaw"
    },
    {
      "source": "Elizabeth Barry",
      "target": "The Computational Democracy Project"
    },
    {
      "source": "Pat Pataranutaporn",
      "target": "MIT Media Lab"
    },
    {
      "source": "LLMs Can Teach Themselves to Better Predict the Future",
      "target": "Ben Turtel"
    },
    {
      "source": "LLMs Can Teach Themselves to Better Predict the Future",
      "target": "Danny Fanklin"
    },
    {
      "source": "LLMs Can Teach Themselves to Better Predict the Future",
      "target": "Philipp Schoenegger"
    },
    {
      "source": "Nexus",
      "target": "Sofi Vanhanen"
    },
    {
      "source": "Nexus",
      "target": "Niki Dupuis"
    },
    {
      "source": "Q1 AI Benchmarking results",
      "target": "Ben Wilson"
    },
    {
      "source": "Q1 AI Benchmarking results",
      "target": "John Bash"
    },
    {
      "source": "Q1 AI Benchmarking results",
      "target": "AI Forecasting Benchmark"
    },
    {
      "source": "Roast My Post",
      "target": "Quantified Uncertainty Research Institute"
    },
    {
      "source": "Global Dialogues",
      "target": "Collective Intelligence Project"
    },
    {
      "source": "Global Dialogues",
      "target": "Remesh"
    },
    {
      "source": "Luke Stebbing",
      "target": "Elicit"
    },
    {
      "source": "Luke Stebbing",
      "target": "Ought"
    },
    {
      "source": "How Malicious AI Swarms Can Threaten Democracy",
      "target": "Daniel Schroeder"
    },
    {
      "source": "How Malicious AI Swarms Can Threaten Democracy",
      "target": "Meeyoung Cha"
    },
    {
      "source": "How Malicious AI Swarms Can Threaten Democracy",
      "target": "Andrea Baronchelli"
    },
    {
      "source": "How Malicious AI Swarms Can Threaten Democracy",
      "target": "Nick Bostrom"
    },
    {
      "source": "How Malicious AI Swarms Can Threaten Democracy",
      "target": "Nicholas Christakis"
    },
    {
      "source": "How Malicious AI Swarms Can Threaten Democracy",
      "target": "David Garcia"
    },
    {
      "source": "How Malicious AI Swarms Can Threaten Democracy",
      "target": "Amit Goldenberg"
    },
    {
      "source": "How Malicious AI Swarms Can Threaten Democracy",
      "target": "Yara Kyrychenko"
    },
    {
      "source": "How Malicious AI Swarms Can Threaten Democracy",
      "target": "Kevin Leyton-Brown"
    },
    {
      "source": "How Malicious AI Swarms Can Threaten Democracy",
      "target": "Nina Lutz"
    },
    {
      "source": "How Malicious AI Swarms Can Threaten Democracy",
      "target": "Gary Marcus"
    },
    {
      "source": "How Malicious AI Swarms Can Threaten Democracy",
      "target": "Filippo Menczer"
    },
    {
      "source": "How Malicious AI Swarms Can Threaten Democracy",
      "target": "Gordon Pennycook"
    },
    {
      "source": "How Malicious AI Swarms Can Threaten Democracy",
      "target": "David Rand"
    },
    {
      "source": "How Malicious AI Swarms Can Threaten Democracy",
      "target": "Frank Schweitzer"
    },
    {
      "source": "How Malicious AI Swarms Can Threaten Democracy",
      "target": "Christopher Summerfield"
    },
    {
      "source": "How Malicious AI Swarms Can Threaten Democracy",
      "target": "Audrey Tang"
    },
    {
      "source": "How Malicious AI Swarms Can Threaten Democracy",
      "target": "Jay Van Bavel"
    },
    {
      "source": "How Malicious AI Swarms Can Threaten Democracy",
      "target": "Sander van der Linden"
    },
    {
      "source": "How Malicious AI Swarms Can Threaten Democracy",
      "target": "Dawn Song"
    },
    {
      "source": "How Malicious AI Swarms Can Threaten Democracy",
      "target": "Jonas Kunst"
    },
    {
      "source": "Philip Tetlock",
      "target": "Forecasting Research Institute"
    },
    {
      "source": "Plurality Mapping Project",
      "target": "Plurality Institute"
    },
    {
      "source": "AI4Democracy - How AI Can Be Used to Inform Policymaking?",
      "target": "Colleen McKenzie"
    },
    {
      "source": "AI4Democracy - How AI Can Be Used to Inform Policymaking?",
      "target": "Değer Turan"
    },
    {
      "source": "Deep Research Bench - Evaluating AI Web Research Agents",
      "target": "Nikos Bosse"
    },
    {
      "source": "Deep Research Bench - Evaluating AI Web Research Agents",
      "target": "Jon Evans"
    },
    {
      "source": "Deep Research Bench - Evaluating AI Web Research Agents",
      "target": "Robert Gambee"
    },
    {
      "source": "Deep Research Bench - Evaluating AI Web Research Agents",
      "target": "Daniel Hnyk"
    },
    {
      "source": "Deep Research Bench - Evaluating AI Web Research Agents",
      "target": "Peter Mühlbacher"
    },
    {
      "source": "Deep Research Bench - Evaluating AI Web Research Agents",
      "target": "Lawrence Phillips"
    },
    {
      "source": "Deep Research Bench - Evaluating AI Web Research Agents",
      "target": "Dan Schwarz"
    },
    {
      "source": "Deep Research Bench - Evaluating AI Web Research Agents",
      "target": "Jack Wildman"
    },
    {
      "source": "Wargames & Scenario Planning",
      "target": "Wargames for Peace"
    },
    {
      "source": "Wargames & Scenario Planning",
      "target": "Human vs. Machine -  Behavioral Differences between Expert Humans and Language Models in Wargame Simulations"
    },
    {
      "source": "Wargames & Scenario Planning",
      "target": "Snow Globe"
    },
    {
      "source": "Wargames & Scenario Planning",
      "target": "Escalation Risks from Language Models in Military and Diplomatic Decision-Making"
    },
    {
      "source": "Wargames & Scenario Planning",
      "target": "Grim"
    },
    {
      "source": "Wargames & Scenario Planning",
      "target": "AI‐assisted scenario generation for strategic planning"
    },
    {
      "source": "Common Ground",
      "target": "Jorim Theuns"
    },
    {
      "source": "Common Ground",
      "target": "Evelien Nieuwenburg"
    },
    {
      "source": "Common Ground",
      "target": "Pepijn Verburg"
    },
    {
      "source": "Common Ground",
      "target": "Lei Nelissen"
    },
    {
      "source": "Common Ground",
      "target": "Brett Hennig"
    },
    {
      "source": "Common Ground",
      "target": "Rich Rippin"
    },
    {
      "source": "Common Ground",
      "target": "Ran Haase"
    },
    {
      "source": "Common Ground",
      "target": "Aldo de Moor"
    },
    {
      "source": "Common Ground",
      "target": "CeesJan Mol"
    },
    {
      "source": "Common Ground",
      "target": "Naomi Esther"
    },
    {
      "source": "Common Ground",
      "target": "Rolf Kleef"
    },
    {
      "source": "Common Ground",
      "target": "Bram Delisse"
    },
    {
      "source": "Echo",
      "target": "Dembrane"
    },
    {
      "source": "Supernotes",
      "target": "Soham De"
    },
    {
      "source": "Supernotes",
      "target": "Michiel Bakker"
    },
    {
      "source": "Supernotes",
      "target": "Jay Baxter"
    },
    {
      "source": "Supernotes",
      "target": "Martin Saveski"
    },
    {
      "source": "Accelerated Preference Elicitation with LLM-Based Proxies",
      "target": "David Huang"
    },
    {
      "source": "Accelerated Preference Elicitation with LLM-Based Proxies",
      "target": "Francisco Marmolejo-Cossío"
    },
    {
      "source": "Accelerated Preference Elicitation with LLM-Based Proxies",
      "target": "Edwin Lock"
    },
    {
      "source": "Accelerated Preference Elicitation with LLM-Based Proxies",
      "target": "David Parkes"
    },
    {
      "source": "Policy",
      "target": "Democratic Policy Development using Collective Dialogues and AI"
    },
    {
      "source": "Policy",
      "target": "AI4Democracy - How AI Can Be Used to Inform Policymaking?"
    },
    {
      "source": "Policy",
      "target": "The AI Adoption Gap - Preparing the US Government for Advanced AI"
    },
    {
      "source": "LLM-Powered Preference Elicitation in Combinatorial Assignment",
      "target": "Ermis Soumalias"
    },
    {
      "source": "LLM-Powered Preference Elicitation in Combinatorial Assignment",
      "target": "Yanchen Jiang"
    },
    {
      "source": "LLM-Powered Preference Elicitation in Combinatorial Assignment",
      "target": "Kehang Zhu"
    },
    {
      "source": "LLM-Powered Preference Elicitation in Combinatorial Assignment",
      "target": "Michael Curry"
    },
    {
      "source": "LLM-Powered Preference Elicitation in Combinatorial Assignment",
      "target": "Sven Seuken"
    },
    {
      "source": "LLM-Powered Preference Elicitation in Combinatorial Assignment",
      "target": "David Parkes"
    },
    {
      "source": "Jon Evans",
      "target": "Future Search"
    },
    {
      "source": "Jon Evans",
      "target": "Metaculus"
    },
    {
      "source": "Prompt Engineering Large Language Models’ Forecasting Capabilities",
      "target": "Philipp Schoenegger"
    },
    {
      "source": "Prompt Engineering Large Language Models’ Forecasting Capabilities",
      "target": "Cameron Jones"
    },
    {
      "source": "Prompt Engineering Large Language Models’ Forecasting Capabilities",
      "target": "Philip Tetlock"
    },
    {
      "source": "Prompt Engineering Large Language Models’ Forecasting Capabilities",
      "target": "Barbara Mellers"
    },
    {
      "source": "Ted Suzman",
      "target": "The Computational Democracy Project"
    },
    {
      "source": "Ozzie Gooen",
      "target": "Quantified Uncertainty Research Institute"
    },
    {
      "source": "Ryan Lowe",
      "target": "Meaning Alignment Institute"
    },
    {
      "source": "Justin Stimatze",
      "target": "AI Objectives Institute"
    },
    {
      "source": "Scaling Deliberation",
      "target": "Habermas Machine"
    },
    {
      "source": "Scaling Deliberation",
      "target": "Nexus"
    },
    {
      "source": "Scaling Deliberation",
      "target": "Common Ground"
    },
    {
      "source": "Scaling Deliberation",
      "target": "Opportunities and Risks of LLMs for Scalable Deliberation with Polis"
    },
    {
      "source": "Scaling Deliberation",
      "target": "Talk to the City"
    },
    {
      "source": "Scaling Deliberation",
      "target": "Israel-Palestine Collective Dialogues"
    },
    {
      "source": "Scaling Deliberation",
      "target": "Amplifying transformative potential while designing augmented deliberative systems"
    },
    {
      "source": "Scaling Deliberation",
      "target": "Democratic Policy Development using Collective Dialogues and AI"
    },
    {
      "source": "Scaling Deliberation",
      "target": "Sensemaker"
    },
    {
      "source": "Scaling Deliberation",
      "target": "Can AI bring deliberative democracy to the masses?"
    },
    {
      "source": "Scaling Deliberation",
      "target": "Reimagining Democracy for AI"
    },
    {
      "source": "Nandika Donthi",
      "target": "Reddit"
    },
    {
      "source": "Live Voice Interface",
      "target": "Online Deliberation Platform"
    },
    {
      "source": "Live Voice Interface",
      "target": "Echo"
    },
    {
      "source": "Live Voice Interface",
      "target": "Common Ground"
    },
    {
      "source": "Live Voice Interface",
      "target": "Wearable Reasoner"
    },
    {
      "source": "Jay Baxter",
      "target": "X Community Notes"
    },
    {
      "source": "Elicit Tool",
      "target": "Elicit"
    },
    {
      "source": "Elicit Tool",
      "target": "Ought"
    },
    {
      "source": "Language models can reduce asymmetry in information markets",
      "target": "Martin Weiss"
    },
    {
      "source": "Language models can reduce asymmetry in information markets",
      "target": "Nasim Rahaman"
    },
    {
      "source": "Language models can reduce asymmetry in information markets",
      "target": "Manuel Wüthrich"
    },
    {
      "source": "Language models can reduce asymmetry in information markets",
      "target": "Yoshua Bengio"
    },
    {
      "source": "Language models can reduce asymmetry in information markets",
      "target": "Li Erran Li"
    },
    {
      "source": "Language models can reduce asymmetry in information markets",
      "target": "Bernhard Schölkopf"
    },
    {
      "source": "Language models can reduce asymmetry in information markets",
      "target": "Chris Pal"
    },
    {
      "source": "AI Tools for Existential Security",
      "target": "Lizka Vaintrob"
    },
    {
      "source": "AI Tools for Existential Security",
      "target": "Owen Cotton-Barratt"
    },
    {
      "source": "Forecasting Tools",
      "target": "Ben Wilson"
    },
    {
      "source": "How large language models can reshape collective intelligence",
      "target": "Jason W. Burton"
    },
    {
      "source": "How large language models can reshape collective intelligence",
      "target": "Ezequiel Lopez-Lopez"
    },
    {
      "source": "How large language models can reshape collective intelligence",
      "target": "Shahar Hechtlinger"
    },
    {
      "source": "How large language models can reshape collective intelligence",
      "target": "Zoe Rahwan"
    },
    {
      "source": "How large language models can reshape collective intelligence",
      "target": "Samuel Aeschbach"
    },
    {
      "source": "How large language models can reshape collective intelligence",
      "target": "Michiel Bakker"
    },
    {
      "source": "How large language models can reshape collective intelligence",
      "target": "Joshua Becker"
    },
    {
      "source": "How large language models can reshape collective intelligence",
      "target": "Aleks Berditchevskaia"
    },
    {
      "source": "How large language models can reshape collective intelligence",
      "target": "Julian Berger"
    },
    {
      "source": "How large language models can reshape collective intelligence",
      "target": "Levin Brinkmann"
    },
    {
      "source": "How large language models can reshape collective intelligence",
      "target": "Lucie Flek"
    },
    {
      "source": "How large language models can reshape collective intelligence",
      "target": "Stefan M. Herzog"
    },
    {
      "source": "How large language models can reshape collective intelligence",
      "target": "Saffron Huang"
    },
    {
      "source": "How large language models can reshape collective intelligence",
      "target": "Sayash Kapoor"
    },
    {
      "source": "How large language models can reshape collective intelligence",
      "target": "Arvind Narayanan"
    },
    {
      "source": "How large language models can reshape collective intelligence",
      "target": "Anne-Marie Nussberger"
    },
    {
      "source": "How large language models can reshape collective intelligence",
      "target": "Taha Yasseri"
    },
    {
      "source": "How large language models can reshape collective intelligence",
      "target": "Pietro Nickl"
    },
    {
      "source": "How large language models can reshape collective intelligence",
      "target": "Abdullah Almaatouq"
    },
    {
      "source": "How large language models can reshape collective intelligence",
      "target": "Ulrike Hahn"
    },
    {
      "source": "How large language models can reshape collective intelligence",
      "target": "Ralf H. J. M. Kurvers"
    },
    {
      "source": "How large language models can reshape collective intelligence",
      "target": "Susan Leavy"
    },
    {
      "source": "How large language models can reshape collective intelligence",
      "target": "Iyad Rahwan"
    },
    {
      "source": "How large language models can reshape collective intelligence",
      "target": "Divya Siddarth"
    },
    {
      "source": "How large language models can reshape collective intelligence",
      "target": "Alice Siu"
    },
    {
      "source": "How large language models can reshape collective intelligence",
      "target": "Anita W. Woolley"
    },
    {
      "source": "How large language models can reshape collective intelligence",
      "target": "Dirk U. Wulff"
    },
    {
      "source": "How large language models can reshape collective intelligence",
      "target": "Ralph Hertwig"
    },
    {
      "source": "Democratic Fine-Tuning",
      "target": "Joe Edelman"
    },
    {
      "source": "Democratic Fine-Tuning",
      "target": "Oliver Klingefjord"
    },
    {
      "source": "Democratic Fine-Tuning",
      "target": "Ivan Vendrov"
    },
    {
      "source": "Generative AI Voting - Fair Collective Choice is Resilient to LLM Biases and Inconsistencies",
      "target": "Srijoni Majumdar"
    },
    {
      "source": "Generative AI Voting - Fair Collective Choice is Resilient to LLM Biases and Inconsistencies",
      "target": "Edith Elkind"
    },
    {
      "source": "Generative AI Voting - Fair Collective Choice is Resilient to LLM Biases and Inconsistencies",
      "target": "Evangelos Pournaras"
    },
    {
      "source": "Rai Sur",
      "target": "Sentinel"
    },
    {
      "source": "Research",
      "target": "Elicit Tool"
    },
    {
      "source": "Research",
      "target": "LLMs as Research Tools -  A Large Scale Survey of Researchers’ Usage and Perceptions"
    },
    {
      "source": "Research",
      "target": "Deep Research Bench - Evaluating AI Web Research Agents"
    },
    {
      "source": "Research",
      "target": "Iterated Decomposition - Improving Science Q&A by Supervising Reasoning Processes"
    },
    {
      "source": "Research",
      "target": "AI Safety Feed"
    },
    {
      "source": "Bench to the Future - A Pastcasting Benchmark for Forecasting Agents",
      "target": "Jack Wildman"
    },
    {
      "source": "Bench to the Future - A Pastcasting Benchmark for Forecasting Agents",
      "target": "Nikos Bosse"
    },
    {
      "source": "Bench to the Future - A Pastcasting Benchmark for Forecasting Agents",
      "target": "Daniel Hnyk"
    },
    {
      "source": "Bench to the Future - A Pastcasting Benchmark for Forecasting Agents",
      "target": "Peter Mühlbacher"
    },
    {
      "source": "Bench to the Future - A Pastcasting Benchmark for Forecasting Agents",
      "target": "Finn Hambly"
    },
    {
      "source": "Bench to the Future - A Pastcasting Benchmark for Forecasting Agents",
      "target": "Jon Evans"
    },
    {
      "source": "Bench to the Future - A Pastcasting Benchmark for Forecasting Agents",
      "target": "Dan Schwarz"
    },
    {
      "source": "Bench to the Future - A Pastcasting Benchmark for Forecasting Agents",
      "target": "Lawrence Phillips"
    },
    {
      "source": "Multi-Agent Consensus Seeking via Large Language Models",
      "target": "Huaben Chen"
    },
    {
      "source": "Multi-Agent Consensus Seeking via Large Language Models",
      "target": "Wenkang Ji"
    },
    {
      "source": "Multi-Agent Consensus Seeking via Large Language Models",
      "target": "Lufeng Xu"
    },
    {
      "source": "Multi-Agent Consensus Seeking via Large Language Models",
      "target": "Shiyu Zhao"
    },
    {
      "source": "Online Deliberation Platform",
      "target": "Deliberative Democracy Lab"
    },
    {
      "source": "How AI Agents Will Improve the Consultation Process",
      "target": "Bruno Marnette"
    },
    {
      "source": "Toby Shevlane",
      "target": "Mantic"
    },
    {
      "source": "Jorim Theuns",
      "target": "Dembrane"
    },
    {
      "source": "Jungwon Byun",
      "target": "Elicit"
    },
    {
      "source": "Jungwon Byun",
      "target": "Ought"
    },
    {
      "source": "Manuel Wüthrich",
      "target": "Midjourney"
    },
    {
      "source": "Pattie Maes",
      "target": "MIT Media Lab"
    },
    {
      "source": "Lucid Lens",
      "target": "Alek Chakroff"
    },
    {
      "source": "Lucid Lens",
      "target": "Justin Stimatze"
    },
    {
      "source": "Lucid Lens",
      "target": "Natasha Jensen"
    },
    {
      "source": "Ian Baker",
      "target": "Plurality Institute"
    },
    {
      "source": "Dan Schwarz",
      "target": "Future Search"
    },
    {
      "source": "Ben Turtel",
      "target": "Lightning Rod Labs"
    },
    {
      "source": "Houtan Bastani",
      "target": "Forecasting Research Institute"
    },
    {
      "source": "Charlie George",
      "target": "Elicit"
    },
    {
      "source": "Charlie George",
      "target": "Ought"
    },
    {
      "source": "Daniel Hnyk",
      "target": "Future Search"
    },
    {
      "source": "Divya Siddarth",
      "target": "Collective Intelligence Project"
    },
    {
      "source": "Natasha Jensen",
      "target": "AI Objectives Institute"
    },
    {
      "source": "Don't Just Tell Me, Ask Me",
      "target": "Valdemar Danry"
    },
    {
      "source": "Don't Just Tell Me, Ask Me",
      "target": "Pat Pataranutaporn"
    },
    {
      "source": "Don't Just Tell Me, Ask Me",
      "target": "Yaoli Mao"
    },
    {
      "source": "Don't Just Tell Me, Ask Me",
      "target": "Pattie Maes"
    },
    {
      "source": "Rob Gordon",
      "target": "Goodheart Labs"
    },
    {
      "source": "Ben Day",
      "target": "Mantic"
    },
    {
      "source": "Pantheon",
      "target": "Sofi Vanhanen"
    },
    {
      "source": "Pantheon",
      "target": "Niki Dupuis"
    },
    {
      "source": "Saffron Huang",
      "target": "Collective Intelligence Project"
    },
    {
      "source": "Beth Goldberg",
      "target": "Google Jigsaw"
    },
    {
      "source": "Peter Darche",
      "target": "Plurality Institute"
    }
  ]
}